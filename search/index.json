[{"content":"1. 基础知识 摄影师看 资料\nhttps://pan.baidu.com/s/1Hz4TeF-yDjYz7DhmujULsQ?pwd=bqlw\n推荐书籍\n夜观星空：天文观测实践指南 1.0 天文知识 1. 星系, 星云, 星团 星系 定义：星系是由恒星、星云、行星、小行星、彗星、星际物质（包括气体和尘埃等）以及暗物质等组成的庞大天体系统。它们通过引力相互吸引和束缚在一起，共同围绕星系的中心进行运动。 结构特点：星系通常具有明显的结构。以银河系为例，它是一个旋涡星系。银河系有一个中心核球，核球内部恒星密集，其中包含大量的老年恒星。从核球向外延伸出旋臂，旋臂是恒星形成活跃的区域，有大量的年轻恒星、星云和星际物质。旋臂的形状像螺旋桨的叶片一样，围绕着星系中心旋转。星系的外围还有晕，晕中包含一些古老的恒星和球状星团。 分类：星系可以根据形态分为椭圆星系、旋涡星系和不规则星系。椭圆星系形状呈椭圆形，恒星分布比较均匀，没有明显的旋臂结构，其中的恒星大多是老年恒星。旋涡星系有明显的旋臂，恒星形成活跃，包含不同年龄的恒星。不规则星系的形状不规则，没有明显的核球和旋臂结构，可能是由于星系之间的相互作用等因素造成的。 运动特点：星系中的恒星等天体围绕星系中心旋转。这种旋转运动是由于星系内部的引力平衡所导致的。例如，在银河系中，太阳系所在的恒星大约每2.25亿年绕银河系中心转一圈。星系整体也会在宇宙中运动，包括星系之间的相互靠近或远离等运动方式。 星团 定义：星团是由数百到数十万颗恒星在引力作用下聚集在一起的恒星集团。它们是恒星诞生和演化的场所之一。 分类及特点 疏散星团：疏散星团中的恒星数量相对较少，一般有数百颗恒星。它们的形状比较松散，恒星之间的距离相对较远。疏散星团通常位于星系的旋臂中，因为旋臂是恒星形成活跃的区域。例如，昴星团（又称七姐妹星团）是一个著名的疏散星团，它位于金牛座，其中的恒星比较年轻，年龄大约在一亿年左右。疏散星团中的恒星大多是同时形成的，它们的化学成分相似。 球状星团：球状星团的恒星数量非常多，可以达到数十万颗。它们的形状呈球形，恒星在球形结构中紧密排列，中心区域恒星密度极高。球状星团主要分布在星系的晕中，例如银河系的晕中就有许多球状星团。球状星团中的恒星大多是老年恒星，年龄可以达到100亿年以上，它们是星系中最早形成的恒星集团之一。球状星团的恒星通过引力紧密束缚在一起，运动方式比较复杂，但总体上保持球形结构的稳定。 星云 定义：星云是由气体（主要是氢气和氦气）和尘埃组成的云雾状天体。它是恒星诞生的摇篮，也是恒星死亡后的产物之一。 分类及特点 发射星云：发射星云自身能够发光。这是因为星云中的气体受到附近恒星的紫外线辐射激发，使得气体原子的电子跃迁到高能级，当电子从高能级跃迁回低能级时，会发出特定波长的光。例如，猎户座大星云是一个典型的发射星云，它位于猎户座，其中心区域有年轻的恒星发出的紫外线辐射激发周围的气体，使星云呈现出红色等颜色。发射星云通常与恒星形成区相关联，是恒星诞生过程中气体被激发发光的现象。 反射星云：反射星云本身不发光，它通过反射附近恒星的光而被我们看到。星云中的尘埃颗粒会散射恒星的光线，使得星云呈现出蓝色等颜色。这是因为尘埃颗粒对短波长的光（如蓝光）散射能力较强。例如，位于金牛座的昴星团附近的反射星云，就是由于星团中的恒星光线被周围的尘埃反射而形成的。 暗星云：暗星云是不发光也不反射光的星云。它们主要由密集的尘埃和气体组成，在宇宙背景或明亮星云的衬托下呈现出暗黑色。暗星云是恒星形成的孕育地，在适当的条件下，暗星云中的气体和尘埃会在引力作用下坍缩，形成新的恒星。例如，位于天蝎座的煤袋星云就是一个著名的暗星云，它在明亮的银河背景中呈现出明显的暗黑色区域。 知道星云, 星团的类别,可以决定使用什么滤镜去拍摄\n1.1. 设备篇 0. 基础设备 三大件就是主镜头( 主镜头可以不是望远镜, 可以直接是使用相机镜头), 赤道仪, 摄像头, 一般来说先确定好三大件再去选购其他的设备, 怎么跟电脑类似, cpu + 主板 + 显卡也是三大件\n相机\n全画幅相机：感光元件尺寸大，对光线的捕捉能力较强，适合深空摄影。例如佳能的EOS 6D Mark II、尼康的D850等，它们的高感光度性能较好，能在暗光环境下拍摄到更多细节。 天文专用相机：如ZWO ASI系列，具有高灵敏度、低噪声等特点，部分型号还支持制冷功能，可有效降低热噪声，提高图像质量，更适合长时间曝光拍摄深空天体 专门的天文冷冻相机 普通相机改机, 入门的直接选择 ==nikon D5100 改机==, 设置可以改冷冻 赤道仪: 赤道仪用于补偿地球自转，保持拍摄对象在视野内稳定，避免星点拉长成星轨。选购的时候需要考虑到你主镜已经上面的设备的重量\n传统赤道仪: 需要挂重锤\n谐波赤道仪: 更轻便, 不需要重锤, 但是贵\n经纬仪: 没有完全抵消地球自转带来的影响, ==会有场旋==\n快门线/间隔计时器：控制长时间曝光\n导星镜和导星相机 : 提高跟踪精度，适合长时间曝光\n主镜: 信达小黑, 信达大黑, 80eq\n其他辅助设备\ngoto系统就是赤道仪的自动跟踪系统, 谐波赤道仪都是内置goto, 没有的话需要购买手动安装\n通常包含两个主控器和两个电机 控制终端\n天文盒子(更容易上手) 电脑 工控 导星设备, 实时监控赤道仪跟踪误差, 把跟踪数据反馈给赤道仪\n导星镜 导星相机 平场板, 可以使用画画用的拷贝板, 一定要买可以调节亮度的\n电源\n极轴镜, 极轴校准是深空摄影必不可少的步骤, 用天文盒子和电脑可以用软件来辅助极轴校准的\n除雾带: 两条, 主镜和导星镜各一条\n1. 一体式设备 振旺S50和Dwarf 3都是智能天文望远镜，以下是它们的详细介绍：\n==振旺S50==\n高度集成化设计：将望远镜、电调、天文相机、ASIAIR智能控制器、经纬仪、加热带、滤镜切换装置等高度集成到一个机器内，机身重量仅2.5KG，便于携带。 传感器与续航：搭载Sony高灵敏度传感器，内置6000mAh电池，正常使用下满电状态可持续6小时拍摄星图，且有C口充电接口，可使用大容量移动电源。 配件丰富：随机器配置抗震收纳盒、便携三脚架、C口充电数据线、快速入门指南、太阳滤镜等，一站式满足观天望远需求。 使用体验：自带双窄带光害滤镜，可在手机App里手动切换，有效降低城市光污染及月光影响；App内有丰富的天象信息和拍摄模式，操作简单，会用智能手机就能使用。 成像质量：通过算法升级优化了出图效果，大大降低了Walking Noise的影响，硬件上增添了光害滤镜后，拍摄的星云等深空天体图片质量不错，但焦距不适合行星观测。 ==Dwarf 3==\n光学性能提升：口径35mm，比Dwarf 2的24mm更大，光圈增加116%；焦距150mm，比Dwarf 2的100mm更长，视场稍窄；配备索尼IMX678 Starvis 2传感器，像素尺寸2μm，比Dwarf 2的1.45μm更大，信噪比更高，可进行更长时间曝光。 功能增强：具备赤道仪模式，避免场旋，可进行更长时间曝光；新增马赛克模式，可拼接更宽广的图像；存储容量增加且无需外接SD卡，电池续航提升但不可拆卸，WiFi范围扩大。 使用便捷性：可连接家中WiFi，扩大控制范围，方便在寒冷夜晚远程监控；App体验友好，有虚拟星图和自动设置，适合初学者，也适合有经验者使用；10000毫安时电池续航长，128GB内部存储卡空间充足。 成像效果：在城市光污染和月光干扰下，仍能拍摄到较为满意的仙女座星系、猎户座星云等深空天体图片，对于一款500美元左右的智能望远镜来说，成像质量令人满意。 两者对比\n价格：Dwarf 3售价499美元，振旺S50价格为2599元人民币，两者价格相近。 便携性：两者都较为轻便，便于携带外出观测。 功能丰富度：振旺S50的App功能较为丰富，有多种拍摄模式和详细的天象信息；Dwarf 3则在光学性能和一些专业功能上有所提升，如赤道仪模式和马赛克模式。 成像质量：Dwarf 3在传感器和光学系统上进行了升级，成像质量有一定优势，尤其在深空天体拍摄上表现更好；振旺S50的成像质量也不错，但可能在一些细节上稍逊于Dwarf 3。 2. 需要拍摄什么 亮场，暗场，偏置场，平场的作用及其拍摄要求\n亮场（Light Frame）： 亮场是指拍摄的实际天文目标图像，即你拍摄的星空、星系、星云等目标的图像。亮场图像记录了天文物体的光强，同时也包含了来自仪器、传感器的噪声和其他背景信号。\n暗场（Dark Frame）： 暗场是指在与亮场相同的曝光时间和温度条件下拍摄的图像，但这时相机镜头是完全遮光的（即没有光线进入）。暗场图像主要记录传感器产生的热噪声、固定噪声以及其他与光照无关的干扰。通过减去暗场图像，可以去除这些不希望存在的噪声。\n偏置场（Bias Frame）： 偏置场图像是在没有曝光的情况下拍摄的图像，即相机的快门完全关闭，曝光时间为零。它主要记录了传感器的偏置电压引起的固定噪声，通常是由于相机的电子设备产生的零曝光信号。通过减去偏置场图像，可以去除这些干扰信号。\n平场（Flat Field）： 平场图像是指拍摄一张均匀光照下的图像，通常是在一个白色的表面上进行拍摄，比如白纸或者平衡的光源。平场图像用于校正图像中的不均匀性，比如镜头的光学畸变（如暗角效应）或传感器上不同区域的灵敏度差异。通过减去平场图像，可以修正这些均匀的系统误差。\n1. 亮场（Light Frame） 作用：亮场是包含天体的曝光图，是我们拍摄的主要目标。通过多张亮场的叠加，可以提高信噪比 ( 信号功率与噪声功率的比值，是衡量信号质量的重要指标之一 )，获得更清晰的天体图像。 拍摄要求 曝光时间：单张曝光时间一般为3-10分钟，总曝光时间从半小时到几十个小时不等。 ISO值：根据拍摄目标和相机性能选择合适的ISO值。 对焦：确保望远镜和相机的对焦准确，拍摄前进行细调。 2. 暗场（Dark Frame） 作用：暗场用于消除热噪声。降噪除辉光, 通过在==拍摄天体后立即遮盖镜头获取，内含照片上固定的噪声点，这样就能在后期处理时扣掉天文照片上带入的相机自身的噪点==。 拍摄要求 曝光时间：与亮场相同。 ISO值：与亮场相同。 温度：环境温度要尽量恒定，避免因温度变化导致暗场挂掉。 拍摄时机：可以在拍摄亮场的前、中、后阶段分别拍摄一组暗场。 3. 偏置场（Bias Frame） 作用：抑制噪点, 除横条, 偏置场用于校准相机内部电路的干扰，扣除相机固有的电路干扰信号。 拍摄要求 曝光时间：使用相机支持的最小快门速度。 ISO值：与亮场相同。 拍摄时机：必须在拍摄亮场时当场拍摄，拍摄时盖上镜头盖。 拍摄数量：拍摄数量应大于亮场张数的1倍到数倍，尽可能多拍。 4. 平场（Flat Frame） 作用：平场用于校正镜头的不均匀性, ==消除灰尘和暗角==，记录望远镜-相机系统拍摄的照片从中心区到四周亮度的变化过程，这样就能在后期处理时将==天体照片中四周的暗角亮度适当提高==，获得一张亮度均匀的天文照片。 拍摄要求 光源：可以使用自然光（如黄昏、黎明或白天的均匀天空）或人造光（如均匀的白光板、iPad等）。 曝光时间：==通过试拍调整曝光时间，使得直方图上的均值落在30000-35000之间。这样才算是合格== ISO值：与亮场相同。 焦距和光圈：保持与拍摄亮场时的焦距和光圈不变。 拍摄数量：一般与亮场相同，也可以略多。 5. 暗平场（Dark Flat Frame） 入门阶段可以不拍这个\n作用：暗平场用于扣除平场中相机固有的噪声。 拍摄要求 曝光时间：与平场相同。 ISO值：与平场相同。 拍摄时机：在拍摄完平场后，马上盖镜头拍摄。 通过拍摄亮场、校准场: 暗场、偏置场和平场，可以有效减少噪声和校正不均匀性，提高天文照片的信噪比和均匀度。这些校准帧在后期处理软件中（如DeepSkyStacker）会自动进行校准，从而得到一张亮度均匀、噪声少的漂亮天文照片。\n==一般来说, 校准场的数量不能少于亮场数量的1/3, 拍了30张亮场, 那么三个校准场各自的数量就不能少于10张==\n冷冻相机可以保持温度一致, ==所以可以建库==, 但是不能建平场(不能保证每次拍摄的时候灰尘的位置都一样)\n暗场库 偏置场库 3. 其他问题 1. 热噪声 Thermal Noise 相机传感器在工作过程中产生的随机电子信号，这些信号与实际拍摄的图像信号无关，会干扰图像的质量。以下是关于相机热噪声的详细解释：\n==产生原因==\n温度影响：相机传感器在工作时会==产生热量==，==温度升高会导致电子在半导体材料中随机运动，产生额外的电子-空穴对==，这些额外的电子信号就是热噪声。 曝光时间：长时间曝光会增加传感器的热量积累，从而增加热噪声。因此，长时间曝光拍摄（如天文摄影）时，热噪声问题尤为突出。 ISO值：高ISO值会放大信号，同时也会放大噪声，包括热噪声。因此，高ISO值拍摄时，热噪声会更加明显。 2. ISO 感光度 感光度：ISO 值越高，感光元件对光线就越敏感；ISO 值越低，感光元件对光线就越不敏感。例如，在光线充足的环境下，可以使用较低的 ISO 值，如 ISO 100 或 ISO 200；而在光线较暗的环境中，为了获得足够的曝光，可能需要提高 ISO 值，如 ISO 1600、ISO 3200 甚至更高。\nISO 100 - 200：适合在光线充足的户外环境拍摄，如晴天的风景照、人像照等，能够获得清晰、细腻的图像。 ISO 400 - 800：适用于光线稍暗的室内环境，或者在户外拍摄快速移动的物体（如运动场景）时，需要较快的快门速度来定格动作，此时适当提高 ISO 可以保证照片的曝光和清晰度。 ISO 1600 - 3200：多用于光线较暗的室内、夜晚街景拍摄，或者在一些特殊情况下需要使用非常快的快门速度来捕捉瞬间画面，比如拍摄舞台表演、野生动物等。但此时需要注意噪点对画质的影响。 ISO 6400 及以上：一般是在极端暗光环境下才会使用，如在没有灯光辅助的室内拍摄，或者在夜晚拍摄星空等。不过，使用这么高的 ISO 值，噪点问题会比较严重，需要在拍摄后通过后期软件进行降噪处理，或者在拍摄时使用一些降噪技巧，如使用三脚架拍摄多张照片进行堆栈降噪等 3. 成像锐度 ​\t成像锐度（Sharpness）是衡量图像清晰度和细节表现的一个重要指标。它反映了图像中物体边缘的清晰度和对比度，是评价成像质量的关键因素之一。简单来说，锐度高意味着图像的细节更加清晰、边缘更加分明；锐度低则意味着图像看起来模糊、细节不够突出\n4. 画幅 ==画幅是摄影和光学成像中的一个重要概念，决定了相机捕捉图像的大小和比例。== ​\t它直接影响成像范围、视角、景深和成像质量。不同的画幅适用于不同的拍摄需求：\n全画幅：适合专业摄影，提供浅景深和高成像质量。 APS-C 画幅：适合中高端消费级摄影，性价比高。 M4/3 画幅：适合便携摄影，成像质量不错。 中画幅：适合高端商业摄影，提供极高的分辨率和成像质量。 手机画幅：适合日常拍摄和分享，便携性强 Stellarium 电脑版可以==模拟设备的成像==\n5. 光污染 满月的光污染等级相当于 6-7, 直接覆盖在天空中\n环境光污染: 选择远离路灯, 拍摄过程中让设备远离光源\n6. 云, 天气, 透明度, 视宁度 透明度: 空气湿度 (起雾)和空气质量(雾霾和扬沙)\n视宁度: 大气扰动, 视宁度是选择天文观测台址的重要参数之一。全球重要的天文台，如夏威夷的莫纳克亚山和加那利群岛的拉帕尔玛岛，都位于视宁度较好的地区。此外，自适应光学技术也被广泛应用于克服大气湍流对观测的影响。\n视宁度对深空摄影的影响要比行星摄影要小\n大气视宁度：由大气湍流引起，是衡量观测台址大气光学品质的重要指标。 圆顶视宁度：由观测圆顶和周围环境分离导致的成像质量变差。 镜面视宁度：由望远镜镜面附近的空气湍流引起，通常与镜面温度与环境温度的差异有关 7. 对极轴 “对极轴”是指将赤道仪的赤经轴（也称为极轴）调整到与地球自转轴平行的过程。这是为了确保赤道仪能够精确跟踪天体的运动，从而实现长时间曝光而不产生星点拖尾。\n初次校对, 可能需要耗费1到2小时才能完成\n视宁度的测量\n常见的视宁度测量方法包括：\n差分像运动测量法（DIMM）：通过测量星像的相对位置变化来计算视宁度，是目前最广泛使用的方法。 闪烁法：通过分析恒星闪烁图案来确定大气湍流的强度 8. 动态范围 人眼的动态范围非常宽，能够在同一场景中同时感知高亮和低暗区域的细节。例如：\n在室内环境中，人眼可以同时看到阳光透过窗户的明亮区域和室内阴影部分的细节，而相机通常很难做到这一点。 9. 天文改机 如果不拍发射星云的话, 不改机也是能拍的, 因为其他星云的光谱, 没有像发射星云一样集中于近红外部分\n10. 彩色相机,黑白相机 相机传感器本身只能识别光的强度，但通过拜耳滤镜阵列和去马赛克算法，它能够间接地记录下颜色信息\n拜耳滤镜最常见的排列方式是 RGGB, 因为人眼对绿色是最敏感的, 解拜耳转色算法\n11. 滤镜的种类与使用 4. 天文望远镜类型 0. 望远镜的主要参数 口径: ==望远镜主镜（或主透镜）的直径==, 最重要的光学参数, 直接决定了望远镜的分辨力极限\n焦距: ==焦距是指从望远镜的主镜（或主透镜）到成像焦点的距离== , 换句话说你能拍多大的目标和多小的目标, 影响放大倍率和视场大小，决定了望远镜的成像范围和放大能力 , 深空摄影 推荐500mm 到700mm 之间, 就可以\n视场大小: 焦距越短，视场越宽，适合观测较大的天体（如星系、星团）或进行广域巡天。 焦比(可以说是光圈): 就是在相同时间内的集光效率, 一般新手f4到f7\n1. 折射望远镜（Refracting Telescope） 原理：折射望远镜利用透镜（通常是凸透镜）来收集和聚焦光线。光线通过透镜折射后汇聚到焦点处，形成清晰的天体图像。 结构：它通常由一个长管组成，前端安装有物镜（主透镜），后端安装有目镜。物镜负责收集光线并聚焦，目镜则用于放大图像。 优点： 成像清晰，对比度高，适合观测明亮的天体（如行星、月球）。 结构简单，光学性能稳定，维护方便。 缺点： 由于透镜的色散效应，不同波长的光会聚焦在不同的位置，导致色差问题。虽然可以通过使用复合透镜（如消色差透镜）来改善，但无法完全消除。 制造大口径的透镜成本高昂且技术难度大，因此折射望远镜的口径通常较小。 应用场景：适合业余天文爱好者用于观测行星、月球表面细节，以及入门级的天文观测。 复消色差折射式望远镜 APO\n2. 反射望远镜（Reflecting Telescope） 比较便宜, 缺点是 ==需要调整光轴,还有存在慧差(就是越靠近画面边缘的地方, 成像越差),可以使用慧差修正镜进行修正==\n原理：反射望远镜利用凹面镜（通常是抛物面镜）来收集和聚焦光线。光线通过反射镜反射后汇聚到焦点处，形成图像。 结构：反射望远镜的核心部件是主镜（凹面镜），通常安装在望远镜的底部。光线通过主镜反射后汇聚到焦点处，为了避免遮挡，通常会使用次镜（如平面镜）将光线引出到侧面或顶部的目镜。 优点： 没有色差问题，因为光线是通过反射而非折射来聚焦。 可以制造大口径的反射镜，从而收集更多的光线，适合观测暗弱的天体（如星系、星云）。 相比折射望远镜，反射望远镜的制造成本较低，性价比更高。 缺点：会产生星芒 结构相对复杂，需要定期校准和维护反射镜的光学性能。 反射镜的表面需要高精度的抛光和镀膜，否则会影响成像质量。 应用场景：反射望远镜是专业天文台和高端业余天文爱好者的首选，适合观测深空天体（如星系、星云、星团）以及进行天文学研究。 ==关于光轴校准==\n牛顿式反射望远镜确实需要调整光轴，因为光轴的对准情况会直接影响成像质量和观测效果。以下是关于光轴的定义以及调整方法的详细说明：\n什么是光轴？\n光轴是光学系统中的基准线，通常是指光线通过光学元件（如透镜或反射镜）的中心轴线。在牛顿式反射望远镜中，光轴是指主镜和副镜的光学中心线，它们需要精确对准，以确保光线能够正确地反射和聚焦。\n为什么需要调整光轴？\n如果光轴没有正确对准，可能会导致成像模糊、星点无法聚焦成一个锐利的点，甚至出现彗差（星点带有尾巴）等问题。因此，调整光轴是确保望远镜成像质量的关键步骤。\n如何调整光轴？\n调整牛顿式反射望远镜的光轴通常需要以下步骤：\n准备工具： 带双十字线的窥管（用于辅助调整）。 手电筒（用于照亮窥管内的十字线）。 调整主镜和副镜的指向： 调节副镜指向：使目镜光轴经副镜反射后指向主镜中心。通过窥管观察，调整副镜的螺丝，直到主镜在副镜中所成的像与副镜的外圆轮廓同心。 调节主镜指向：使主镜的光轴与目镜光轴重合。用手电筒照亮窥管的双十字线，通过窥管观察，调整主镜背后的螺栓，使十字线、主镜中心点的像以及十字线的反射像三者同心。 检查调整效果： 在一个大气宁静度较好的夜晚，用望远镜的最高倍率观察一颗恒星（如果没有赤道仪，可以观察北极星）。将星点放在目镜视场中心，仔细调整焦距，从焦点外调到焦点，再调到焦点内。 如果光轴调整正确，星点在焦点上会非常锐利，散焦后形成的衍射环是同心圆。如果星点变成扇形，且发散方向不变，则说明光轴需要进一步调整。 注意事项\n调整光轴时，可能需要多次反复操作，因为副镜和主镜的调整会相互影响。 一旦光轴调整好，只要望远镜的结构稳固，后续使用时通常不需要频繁调整。 通过上述步骤，可以确保牛顿式反射望远镜的光轴正确对准，从而获得清晰、锐利的成像效果。\n3. 折反射望远镜（Catadioptric Telescope） 原理：折反射望远镜结合了折射和反射的原理，使用透镜和反射镜共同完成光线的聚焦。它通过特殊的光学设计（如施密特-卡塞格林系统或马克斯托夫-卡塞格林系统）来优化成像质量和光学性能。 结构：折反射望远镜通常包含一个主镜（凹面镜）、一个次镜（反射镜）和一个校正透镜（如施密特板或马克斯托夫透镜）。光线通过校正透镜进入望远镜，经过主镜和次镜的反射后汇聚到焦点处。 优点： 结合了折射和反射望远镜的优点，成像质量高，色差小。 光学系统紧凑，便于携带和使用，适合野外观测。 可以制造较大口径的望远镜，同时保持较短的焦距，适合多种观测场景。 缺点： 结构复杂，成本相对较高。 需要定期维护和校准光学系统，否则会影响成像质量。 应用场景：折反射望远镜适合中高端业余天文爱好者，既可用于观测行星、月球表面细节，也可用于深空天体的观测。它也是许多小型天文台和教育机构的首选设备。 总结 折射望远镜：适合入门级爱好者，主要用于观测明亮天体，如行星和月球。 反射望远镜：适合专业观测和深空天体研究，性价比高，适合大口径需求。 折反射望远镜：综合性能优秀，适合中高端爱好者，兼顾多种观测需求。 1.3 出摊 三角架 指向北端\n2. 后期处理 PixInsight 是一款==专业的天文图像处理软件平台==, ==收费2000rmb==，具有以下特点和功能： 模块化和开放架构：PixInsight 是一个模块化、开放架构的图像处理平台，支持多种操作系统，包括 FreeBSD、Linux、Mac OS X 和 Windows。 功能强大：它提供了丰富的图像处理功能，适用于天文摄影和其他技术成像领域。用户可以进行图像校准、对齐、堆叠、拉伸、色彩校正等多种操作。 用户友好：PixInsight 的界面设计直观，操作简便，适合初学者和专业用户。它提供了详细的教程和文档，帮助用户快速上手。 社区支持：拥有活跃的用户社区，用户可以分享经验和技巧，获取支持和帮助。 免费试用：提供 45 天的免费试用版本，用户可以在此期间体验所有功能，无需担心水印或功能限制。 Siril: 是一款==免费的==天文图像处理软件，适用于多种操作系统，包括 Linux 和 macOS，最新版本也支持 Windows。它提供了强大的功能，适用于天文摄影的预处理和后期处理。以下是 Siril 的主要功能和特点： 1. Siril 教程笔记 参考资料来源, B站up主: 摄影师看\nhttps://www.bilibili.com/video/BV17c411u7oL?spm_id_from=333.788.videopod.sections\u0026vd_source=6d85c4519c10d084ad4e184a7ec55add\nDeepSkyStacker 后期深空叠加\n专为天文摄影师设计，用于==简化深空图片的所有预处理步骤，包括注册、堆叠和简单的堆叠后处理，可快速查看最终结果==，并将生成的图像保存到TIFF或FITS文件（16或32位）。 第一步是要进行图片的筛选, 把拉线的图片给筛选出来 把亮场的素材先备份一下 https://downloads.topazlabs.com/deploy/TopazDeNoiseAI/3.7.2/TopazDeNoiseAI-3.7.2-windows-x64-Full-Installer.exe 1. 快捷键 2. 简易处理 DSS 筛选\nsiril 脚本自动叠加\n背景提取, 把星云的位置的红点给取消, 计算背景, 然后应用, 这一步的处理是为了减少天光污染的影响, 显示出更多的星云细节, 压暗背景, 平衡图像的整体亮度\n色彩校准, 彩色相机使用 反双曲正弦变换\n把预览模式改回线性\n星点移除, 可以吧图片中的星点和星云分成两张照片\n直方图变换\n转存图片\n使用 TD AI 进行降噪, 保存降噪后的图片\n使用ps 打开降噪图片, 点击滤镜, 使用camera raw 进行调色\n把调色完的图片拖到siril 中打开, 重新转存为TIFF格式, 还是保存为32位浮点格式\n如果不满意, 可以按照刚才的方式, ai 降噪, ps 调色, 同样处理一遍星点\n选择星点加工, 然后星点重绘\n最后将图片另存为TIF 格式, 16位无符号整数, 最后用PS 调整最后保存为 jpg, png\n2. DSS 使用 备份亮场文件夹\n用DSS 打开\n勾选全部\n识别勾选图像的星点\n按照分数进行排名, 删掉你所能接受最低的分数图片\n","date":"2025-02-05T19:22:11+08:00","permalink":"https://mikeLing-qx.github.io/p/%E5%A4%A9%E6%96%87%E6%91%84%E5%BD%B1/","title":"天文摄影"},{"content":"0. 前言 设计模式是一套被反复使用的、多数人知晓的、经过分类编目的、代码设计经验的总结\n1. 创建型模式 工厂模式（Factory Pattern） 抽象工厂模式（Abstract Factory Pattern） 单例模式（Singleton Pattern） 建造者模式（Builder Pattern） 原型模式（Prototype Pattern） 2. 结构型模式 适配器模式（Adapter Pattern） 桥接模式（Bridge Pattern） 过滤器模式（Filter、Criteria Pattern） 组合模式（Composite Pattern） 装饰器模式（Decorator Pattern） 外观模式（Facade Pattern） 享元模式（Flyweight Pattern） 代理模式（Proxy Pattern） 3. 行为型模式 责任链模式（Chain of Responsibility Pattern） 命令模式（Command Pattern） 解释器模式（Interpreter Pattern） 迭代器模式（Iterator Pattern） 中介者模式（Mediator Pattern） 备忘录模式（Memento Pattern） 观察者模式（Observer Pattern） 状态模式（State Pattern） 空对象模式（Null Object Pattern） 策略模式（Strategy Pattern） 模板模式（Template Pattern） 访问者模式（Visitor Pattern） 两个问题:\n使用场景, 好处 怎么用的 1. 工厂模式 Factory 1.0 概述 就是建立一个工厂类, 对实现了同一接口的一些类进行实例的创建\n普通工厂 多工厂方法 静态工厂方法 使用场景\n出现了大量的产品需要创建, 并且具有共同的接口时, 可以通过工厂方法模式进行创建, 建议使用静态工厂方法模式\n1.1 实现方式 1.1. 1 简单工厂 首先, 创建二者共同接口\npublic interface Sender (){ public void Send(); } 创建实现类\npublic class MailSender implements Sender { @Override public void Send() { System.out.println(\u0026quot;this is mailsender!\u0026quot;); } } public class SmsSender implements Sender { @Override public void Send() { System.out.println(\u0026quot;this is sms sender!\u0026quot;); } } 建工厂类\npublic class SendFactory { public Sender produce(String type) { if (\u0026quot;mail\u0026quot;.equals(type)) { return new MailSender(); } else if (\u0026quot;sms\u0026quot;.equals(type)) { return new SmsSender(); } else { System.out.println(\u0026quot;请输入正确的类型!\u0026quot;); return null; } } } 1.1.2 抽象工厂 ==概述==\n类的创建依赖于工厂, 如果想要拓展程序就必须要对工厂类进行修改, 这违背了闭包原则 (对拓展开放, 对修改关闭); ==当程序修改的时候不需要去修改原来的代码, 实现一个热插拔的效果 (使用接口和抽象类)==\n实现方式\n1. Cpu接口 2. 再有两个实现类 Kirin990 和 888 3. 屏幕Screen接口 4. 两个屏幕实现类 三星 和 京东方 5. phone工厂接口 6. 手机工厂实现类 小米工厂 (三星屏幕 + 888Cpu) 华为工厂 (Kirin990 + 京东方) 7. public class AbstractFactory { // 手机cpu public interface Cpu { void run(); class Cpu660 implements Cpu { @Override public void run() { System.out.println(\u0026quot;660\u0026quot;); } } class Cpu845 implements Cpu { @Override public void run() { System.out.println(\u0026quot;845\u0026quot;); } } class CpuKirin9000 implements Cpu { @Override public void run() { System.out.println(\u0026quot;Kirin9000\u0026quot;); } } } // 手机屏幕 public interface Screen { void Size(); class Screen5 implements Screen { @Override public void Size() { System.out.println(\u0026quot;5\u0026quot;); } } class Screen6 implements Screen { @Override public void Size() { System.out.println(\u0026quot;6\u0026quot;); } } } // 工厂接口 public interface PhoneFactory { Cpu getCpu(); Screen getScreen(); } public class XiaoMiFactory implements PhoneFactory { @Override public Cpu getCpu() { return new Cpu.Cpu845(); } @Override public Screen getScreen() { return new Screen.Screen6(); } } public class HongMiFactory implements PhoneFactory { @Override public Cpu getCpu() { return new Cpu.Cpu660(); } @Override public Screen getScreen() { return new Screen.Screen5(); } } public class HuaweiFactory implements PhoneFactory { @Override public Cpu getCpu() { return new Cpu.CpuKirin9000(); } @Override public Screen getScreen() { return new Screen.Screen6(); } } @Test public void test01() { HongMiFactory hongMiFactory = new HongMiFactory(); Cpu cpu = hongMiFactory.getCpu(); Screen screen = hongMiFactory.getScreen(); cpu.run(); screen.Size(); } } 2. 单例模式 SingleTon 2.1 概述 参考资料\nhttps://www.liaoxuefeng.com/wiki/1252599548343744/1281319214514210#0\n实现方式\n只用private 构造方法, 确保外部无法实例化\n通过 private static 变量持有唯一治理, 保证全局唯一性\n通过public static 方法返回唯一实例, 使外部调用方能获取\n==Singleton模式一般以“约定”为主 让框架（例如Spring）来实例化这些类，保证只有一个实例，不会刻意实现它==\n双重检查\n真正实现延迟加载，只能通过Java的ClassLoader机制完成。如果没有特殊的需求，使用Singleton模式的时候，最好不要延迟加载，这样会使代码更简单。\n2.2 实现方式 普通实现 public class Singleton { // 静态字段引用唯一实例: public static final Singleton INSTANCE = null; // private构造方法保证外部无法实例化: private Singleton() { } public synchronized static Singleton getInstance() { if (INSTANCE == null) { INSTANCE = new Singleton(); } return INSTANCE; } } enum 实现 public enum World { // 唯一枚举: INSTANCE; private String name = \u0026quot;world\u0026quot;; public String getName() { return this.name; } public void setName(String name) { this.name = name; } } 3. 构造模式 Builder 3.1 概述 使用Builder模式是, 适用于创建比较复杂的对象, 一步一步的创建出零件, 最后在装配起来\nString url = URLBuilder.builder() // 创建Builder .setDomain(\u0026quot;www.liaoxuefeng.com\u0026quot;) // 设置domain .setScheme(\u0026quot;https\u0026quot;) // 设置scheme .setPath(\u0026quot;/\u0026quot;) // 设置路径 .setQuery(Map.of(\u0026quot;a\u0026quot;, \u0026quot;123\u0026quot;, \u0026quot;q\u0026quot;, \u0026quot;K\u0026amp;R\u0026quot;)) // 设置query .build(); // 完成build 3.2 实现 4. 原型模式 Prototype 4.1 概述 创建新对象的时候, 根据现有的一个原型来创建,\n4.1.1 深拷贝和浅拷贝 浅: 将一个对象复制后, 基本数据类型的变量都会重新创建, 而 引用类型, 指向的还是 原对象所指向的\n深: 将一个对象复制后, 不论是基本数据类型还是引用类型, 都是重新创建的, , 就是深复制是彻底的复制\npublic class Prototype implements Cloneable, Serializable { private static final long serialVersionUID = 1L; private String string; private SerializableObject obj; /* 浅复制 */ public Object clone() throws CloneNotSupportedException { Prototype proto = (Prototype) super.clone(); return proto; } /* 深复制 */ public Object deepClone() throws IOException, ClassNotFoundException { /* 写入当前对象的二进制流 */ ByteArrayOutputStream bos = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(bos); oos.writeObject(this); /* 读出二进制流产生的新对象 */ ByteArrayInputStream bis = new ByteArrayInputStream(bos.toByteArray()); ObjectInputStream ois = new ObjectInputStream(bis); return ois.readObject(); } public String getString() { return string; } public void setString(String string) { this.string = string; } public SerializableObject getObj() { return obj; } public void setObj(SerializableObject obj) { this.obj = obj; } } class SerializableObject implements Serializable { private static final long serialVersionUID = 1L; } 4.2 实现 Q: 如果我们已经有了一个String[]数组，想再创建一个一模一样的String[]数组，怎么写？\n==这个创建过程封装一下，就成了原型模式==\n// 原型: String[] original = { \u0026quot;Apple\u0026quot;, \u0026quot;Pear\u0026quot;, \u0026quot;Banana\u0026quot; }; // 新对象: String[] copy = Arrays.copyOf(original, original.length); 5. 适配器模式 Adapter(Wrapper) 相当一个转换器, 输入为 接口A , 输出为 接口B\n5.1 概述 类的适配器模式：当希望将一个类转换成满足另一个新接口的类时，可以使用类的适配器模式，创建一个新类，继承原有的类，实现新的接口即可。 对象的适配器模式：当希望将一个对象转换成满足另一个新接口的对象时，可以创建一个Wrapper类，持有原类的一个实例，在Wrapper类的方法中，调用实例的方法就行。 接口的适配器模式：当不希望实现一个接口中所有的方法时，可以创建一个抽象类Wrapper，实现所有方法，我们写别的类的时候，继承抽象类即可。 类\n==使用场景==\n将一个类的接口转换成客户希望的另外一个接口, 是的原本由于接口不兼容而不能一起工作的那些类可以一起工作\n适配器模式在Java标准库中有广泛应用\nString[] exist = new String[] {\u0026quot;Good\u0026quot;, \u0026quot;morning\u0026quot;, \u0026quot;Bob\u0026quot;, \u0026quot;and\u0026quot;, \u0026quot;Alice\u0026quot;}; Set\u0026lt;String\u0026gt; set = new HashSet\u0026lt;\u0026gt;(Arrays.asList(exist)); 5.2 实现 5.2.0 实现步骤 实现目标接口 内部持有一个待转换接口的引用, 可通过字段 在目标接口的实现方法内部, 调用待转换接口的方法 5.2.1 类的适配器模式 ==sum==\n原本Source类只用method1 方法 public class Source { public void method1() { System.out.println(\u0026quot;this is original method!\u0026quot;); } } TargetTable 接口 public interface TargetTable { void method1(); void method2(); } ==Adapter类，将Source的功能扩展到Targetable里== public class Adapter extends Source implements TargetTable { @Override public void method2() { } public static void main(String[] args) { Adapter adapter = new Adapter(); adapter.method1(); adapter.method2(); } } test public static void main(String[] args) { Adapter adapter = new Adapter(); adapter.method1(); adapter.method2(); } 5.2.2 对象的适配器模式 不继承Source类，==而是持有Source类的实例==，以达到解决兼容性的问题\npublic class Wrapper implements Targetable { private Source source; public Wrapper(Source source){ super(); this.source = source; } @Override public void method2() { System.out.println(\u0026quot;this is the targetable method!\u0026quot;); } @Override public void method1() { source.method1(); } } 5.2.3 接口的适配器模式 ​\t接口定义了太多方法, 但是并不是都需要\n实现方式\n抽象类实现原接口 新的类继承抽象类, 重写我们需要的方法就可以了 6. 装饰模式 Decorator 6.1 概述 给对象动态的添加一些新功能, 要求装饰对象和 被装饰对象实现同一接口, 装饰对象持有被装饰对象的实例\n==应用场景==\n需要拓展一个类的功能 动态的为一个对象新增功能, 而且还能动态的撤销 (继承无法做到, 继承是静态的, 不能动态增删) ==缺点==: 产生过多相似的对象, 不易排错\n6.2 实现 接口\npublic interface Sourceable { public void method(); } 被装饰类\npublic class Source implents Sourceable{ @Override public void method(){ sout(\u0026quot;the original mehtod\u0026quot;); } } 装饰类\npublic class Decorator implement Sourceable{ private Sourceable source; public Decorator(Sourceable source){ super(); this.source = source; } @Override public void method(){ sout('before decorator'); source.method(); sout(\u0026quot;after decorator\u0026quot;); } } test\npsvm{ Sourceable source = new Source(); Sourceable obj = new Decoratoe(source); obj.method(); } 7 .代理模式 Proxy 7.1 概述 为其他对象提供一种代理以控制对这个对象的访问\n==应用场景==\n如果已有的方法在使用的时候需要对原有的方法进行改进, 就可以采用代理模式, 对产生的结果进行控制\n==装饰模式和代理模式的区别==\n对装饰器模式来说，装饰者（decorator）和被装饰者（decoratee）都实现同一个 接口。对代理模式来说，代理类（proxy class）和真实处理的类（real class）都实现同一个接口。他们之间的边界确实比较模糊，两者都是对类的方法进行扩展，具体区别如下：\n1、装饰器模式强调的是增强自身，在被装饰之后你能够在被增强的类上使用增强后的功能。增强后你还是你，只不过能力更强了而已；代理模式强调要让别人帮你去做一些本身与你业务没有太多关系的职责（记录日志、设置缓存）。代理模式是为了实现对象的控制，因为被代理的对象往往难以直接获得或者是其内部不想暴露出来。\n2、装饰模式是以对客户端透明的方式扩展对象的功能，是继承方案的一个替代方案；代理模式则是给一个对象提供一个代理对象，并由代理对象来控制对原有对象的引用；\n3、装饰模式是为装饰的对象增强功能；而代理模式对代理的对象施加控制，但不对对象本身的功能进行增强；\n7.2 实现 (静态代理) 代理模式通过封装一个已有的接口, 并向调用放返回相同的接口类型, 能在调用放不改变代码的前提下 增强功能(鉴权, 延迟加载, 连接池复用)\n==要求调用方持有接口, 作为 Proxy的类 也必须实现相同的接口==\n步骤\n代理类和目标类实现同一个接口 代理类中除了要调用目标类的方法实现业务逻辑, 还要实现额外的功能 接口\npublic interface Sourceable{ void method; } 被代理方法\npublic class Source implements Sourceable{ @Override public void method(){ sout(\u0026quot;111\u0026quot;)'; } } 代理\npublic class Proxy implemts Sourcealble { private Source source; public Proxy(){ super(); this.source = new source(); } @Override public void method(){ sout(\u0026quot;befor\u0026quot;); source.method(); } } 7.3 实现 (动态代理) 动态代理: 动态的为目标类创建代理类\n1. 引入依赖 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-context\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.2.16.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.aspectj\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;aspectjrt\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.9.5\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.aspectj\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;aspectjweaver\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.9.5\u0026lt;/version\u0026gt; \u0026lt;/dependency AspectJ AspectJ 是一个基于 Java 语言的 AOP 框架，提供了强大的 AOP 功能，其他很多 AOP 框架都借鉴或采纳其中的一些思想。\nAspectJ 是 Java 语言的一个 AOP 实现，其主要包括两个部分：第一个部分定义了如何表达、定义 AOP 编程中的语法规范，通过这套语言规范，我们可以方便地用 AOP 来解决 Java 语言中存在的交叉关注点问题；另一个部分是工具部分，包括编译器、调试工具等。\nAspectJ 是最早、功能比较强大的 AOP 实现之一，对整套 AOP 机制都有较好的实现，很多其他语言的 AOP 实现，也借鉴或采纳了 AspectJ 中很多设计。在 Java 领域，AspectJ 中的很多语法结构基本上已成为 AOP 领域的标准。\n2. 创建接口 和 实现类 接口\npublic interface CalculateService { // 加法 int add(int a,int b); // 减法 int sub(int a,int b); } 实现类(目标类)\n@Service public class CalculateServiceImpl implements CalculateService { @Override public int add(int a, int b) { int result = a + b; System.out.println(\u0026quot;加法操作。。。\u0026quot;); return result; } @Override public int sub(int a, int b) { int result = a - b; System.out.println(\u0026quot;减法操作。。。\u0026quot;); return result; } } 8. 外观模式 Facade (门面) 8.1 概述 为了解决类与类之间的依赖关系, 外观模式就是把他们的关系放在一个Facede类中, 降低类类之间的耦合度, 该模式没有涉及到接口\n8.2 实现 1. 数据库连接 Facade模式的一个典型应用就是进行数据库连接。一般我们在每一次对数据库进行访问，都要进行以下操作：先得到connect实例，然后打开connect获得连接，得到一个statement,.执行sql语句进行查询，得到查询结果集。\n我们可以将这些步骤提取出来，封装在一个类里面。这样，每次执行数据库访问只需要 将必要的参数传递到这个类中就可以了。\n2. 组装电脑 cpu 类 public void Cpu { public void startup() { sout(\u0026quot;\u0026quot;) }; public void shutdown() { sout(\u0026quot;\u0026quot;); } } memory 类\ndisk 类\ncomputer 进行统一管理\npublic void Computer { private Cpu cpu; pirvate Memory memory; private Disk disk; public Computer (){ cpu = new Cpu(); memory = new Memory(); disk = new Disk(); } public void startup (){ sout(); cpu.startup(); memory.startup(); disk.startup(); } public void shutdown() { sout(); cpu.shutdown(); memory.shutdown(); disk.shutdown(); } } 9. 桥接模式 9.1 概述 桥接模式就是把事物和其具体实现分开，使他们可以各自独立的变化。桥接的用意是：将抽象化与实现化解耦，使得二者可以独立变化，像我们常用的JDBC桥DriverManager一样，JDBC进行连接数据库的时候，在各个数据库之间进行切换，基本不需要动太多的代码，甚至丝毫不用动，原因就是JDBC提供统一接口，每个数据库提供各自的实现，用一个叫做数据库驱动的程序来桥接就行了\n将抽象部分和它的实现部分分离, 使它们都可以独立的变化;\n三个品牌的汽车, 都生产可以燃油, 混动, 纯电 汽车, 此时新增了第四种品牌之后, 都会导致子类的数量快速增长\n9.1.0 JDBC 连接数据库 9.1.1 使用场景 桥接模式实现比较复杂，实际应用也非常少，==但它提供的设计思想值得借鉴==，即不要过度使用继承，而是==优先拆分某些部件==，使用==组合的方式==来扩展功能。\n9.2 实现 Car 抽象类, 拥有基础的属性和方法; 引用引擎engine public abstract class Car { // 引用Engine: protected Engine engine; public Car(Engine engine) { this.engine = engine; } public abstract void drive(); } engine 是一个接口; public interface Engine { void start(); } 不同的引擎实现 engine public class HybridEngine implements Engine { public void start() { System.out.println(\u0026quot;Start Hybrid Engine...\u0026quot;); } } 一个修正的抽象类 RefinedCar , 定义额外的方法, 汽车品牌 public abstract class RefinedCar extends Car { public RefinedCar(Engine engine) { super(engine); } public void drive() { this.engine.start(); System.out.println(\u0026quot;Drive \u0026quot; + getBrand() + \u0026quot; car...\u0026quot;); } public abstract String getBrand(); } 不同的品牌汽车继承自 修正类 public class BossCar extends RefinedCar { public BossCar(Engine engine) { super(engine); } public String getBrand() { return \u0026quot;Boss\u0026quot;; } } 选择品牌, 和引擎 得到Car ==即一辆汽车的两个维度：品牌和引擎都可以独立地变化。== RefinedCar car = new BossCar(new HybridEngine()); car.drive(); 10. 组合模式 Composite ==使用Composite模式时，需要先统一单个节点以及“容器”节点的接口==\n==作为容器节点的ElementNode又可以添加任意个Node，这样就可以构成层级结构==\n11. 享元模式 Flyweight 11.1.1 概述 主要目的是实现对象的共享 , 减少内存的开销, ==通常和工厂模式一起使用, 工厂内部返回的是缓存的实例==;\n思想是: 如果一个对象实例一经创建就不可变, 那么反复创建相同的实例就是 非必要的, 直接向调用方返回一个共享的实例\n你要先理解创建实例和实例变化的区别：\n创建实例：var s = new StringBuilder(); 实例变化：s.append(\u0026quot;changed\u0026quot;); 实例不能变化：String s = \u0026quot;cannot-change\u0026quot;; 单例不是不变，是不允许创建新实例。 ==享元要求实例不变，所以才能把“应该创建一个新实例”的操作给优化成“直接返回一个缓存的实例”==\n11.1.2 包装类 包装类型如Byte、Integer都是不变类，因此，反复创建同一个值相同的包装类型是没有必要的。以Integer为例，如果我们通过Integer.valueOf()这个静态工厂方法创建Integer实例，当传入的int范围在-128~+127之间时，会直接返回缓存的Integer实例：\n11.2 实现 ==主要应用于缓存==\npublic class Student { // 持有缓存: private static final Map\u0026lt;String, Student\u0026gt; cache = new HashMap\u0026lt;\u0026gt;(); // 静态工厂方法: public static Student create(int id, String name) { String key = id + \u0026quot;\\n\u0026quot; + name; // 先查找缓存: Student std = cache.get(key); if (std == null) { // 未找到,创建新对象: System.out.println(String.format(\u0026quot;create new Student(%s, %s)\u0026quot;, id, name)); std = new Student(id, name); // 放入缓存: cache.put(key, std); } else { // 缓存中存在: System.out.println(String.format(\u0026quot;return cached Student(%s, %s)\u0026quot;, std.id, std.name)); } return std; } private final int id; private final String name; public Student(int id, String name) { this.id = id; this.name = name; } } 12. 策略模式 Category 核心思想: 在一个计算方法中把统一变化的算法抽出来 作为策略参数传进去, 从而使得新增策略不必修改原有逻辑;\n12.1 概述 在一个方法中，流程是确定的，但是，某些关键步骤的算法依赖调用方传入的策略，这样，传入不同的策略，即可获得不同的结果，大大增强了系统的灵活性。\n12.2 实现 一个折扣接口\npublic interface DiscountStrategy { // 计算折扣额度: BigDecimal getDiscount(BigDecimal total); } 两个实现类\n普通用户折扣\npublic class UserDiscountStrategy implements DiscountStrategy { public BigDecimal getDiscount(BigDecimal total) { // 普通会员打九折: return total.multiply(new BigDecimal(\u0026quot;0.1\u0026quot;)).setScale(2, RoundingMode.DOWN); } } 满减\npublic class OverDiscountStrategy implements DiscountStrategy { public BigDecimal getDiscount(BigDecimal total) { // 满100减20优惠: return total.compareTo(BigDecimal.valueOf(100)) \u0026gt;= 0 ? BigDecimal.valueOf(20) : BigDecimal.ZERO; } } 折扣类 (应用策略) 本身持有默认的普通用户折扣策略\npublic class DiscountContext { // 持有某个策略: private DiscountStrategy strategy = new UserDiscountStrategy(); // 允许客户端设置新策略: public void setStrategy(DiscountStrategy strategy) { this.strategy = strategy; } public BigDecimal calculatePrice(BigDecimal total) { return total.subtract(this.strategy.getDiscount(total)).setScale(2); } } 调用\nDiscountContext ctx = new DiscountContext(); // 默认使用普通会员折扣: BigDecimal pay1 = ctx.calculatePrice(BigDecimal.valueOf(105)); System.out.println(pay1); // 使用满减折扣: ctx.setStrategy(new OverDiscountStrategy()); BigDecimal pay2 = ctx.calculatePrice(BigDecimal.valueOf(105)); System.out.println(pay2); // 使用Prime会员折扣: ctx.setStrategy(new PrimeDiscountStrategy()); BigDecimal pay3 = ctx.calculatePrice(BigDecimal.valueOf(105)); System.out.println(pay3); 13. 模板方法 Template Method 定义一个操作中的算法的骨架, 而将一些步骤延迟到子类中去, 使得子类可以不改变一个算法的结构就可以重新定义改算法的某些特定步骤\n13.1 概述 为了防止子类重写父类的骨架方法，可以在父类中对骨架方法使用==final==。对于需要子类实现的抽象方法，一般==声明为protected==，使得这些方法对外部客户端不可见。\nJava标准库也有很多模板方法的应用。在集合类中，AbstractList和AbstractQueuedSynchronizer都定义了很多通用操作，子类只需要实现某些必要方法。\n13.2 实现 抽象父类, 用抽象方法让子类去实现\n读取设置的类\npublic abstract class AbstractSetting { public final String getSetting(String key) { String value = lookupCache(key); if (value == null) { value = readFromDatabase(key); putIntoCache(key, value); } return value; } protected abstract String lookupCache(String key); protected abstract void putIntoCache(String key, String value); } 可以选择使用不同的缓存\n用Map缓存\npublic class LocalSetting extends AbstractSetting { private Map\u0026lt;String, String\u0026gt; cache = new HashMap\u0026lt;\u0026gt;(); protected String lookupCache(String key) { return cache.get(key); } protected void putIntoCache(String key, String value) { cache.put(key, value); } } 用Redis缓存\npublic class RedisSetting extends AbstractSetting { private RedisClient client = RedisClient.create(\u0026quot;redis://localhost:6379\u0026quot;); protected String lookupCache(String key) { try (StatefulRedisConnection\u0026lt;String, String\u0026gt; connection = client.connect()) { RedisCommands\u0026lt;String, String\u0026gt; commands = connection.sync(); return commands.get(key); } } protected void putIntoCache(String key, String value) { try (StatefulRedisConnection\u0026lt;String, String\u0026gt; connection = client.connect()) { RedisCommands\u0026lt;String, String\u0026gt; commands = connection.sync(); commands.set(key, value); } } } 使用 (通过调用抽象类, 实现对子类的调用)\n本地缓存\nAbstractSetting setting1 = new LocalSetting(); System.out.println(\u0026quot;test = \u0026quot; + setting1.getSetting(\u0026quot;test\u0026quot;)); System.out.println(\u0026quot;test = \u0026quot; + setting1.getSetting(\u0026quot;test\u0026quot;)); redis 缓存\nAbstractSetting setting2 = new RedisSetting(); System.out.println(\u0026quot;autosave = \u0026quot; + setting2.getSetting(\u0026quot;autosave\u0026quot;)); System.out.println(\u0026quot;autosave = \u0026quot; + setting2.getSetting(\u0026quot;autosave\u0026quot;)); 14. 观察者模式 Observer 14.1 概述 当一个对象变化是, 其他依赖该对象的对象都会收到通知,并且随之变化, 这是一种==一对多的方式==; 类似于发布订阅\n==定义了一对多的依赖关系, 让多个对象能够同时监听一个主题对象, 如果这个主题对象发生了变化, 会通知所有的观察者对象==\n好处: 主题对象和观察者之间降低耦合\n14.2 实现 观察接口\npublic interface Observer { void update(); } 观察类\n两个观察类\npublic class Observer1 implements Observer { @Override public void update() { System.out.println(\u0026quot;Observer1 update\u0026quot;); } } public class Observer2 implements Observer { @Override public void update() { System.out.println(\u0026quot;Observer2 update\u0026quot;); } } 订阅管理接口\npublic interface Subject { void add(Observer observer); void remove(Observer observer); void notifyObservers(); void operation(); } 订阅抽象类 (==记录并管理订阅关系==, 当订阅对象放生变化时, 负责通知订阅类)\npublic abstract class AbstarctSubject implements Subject { // 用数组管理 private Vector\u0026lt;Observer\u0026gt; vector = new Vector\u0026lt;\u0026gt;(); @Override public void add(Observer observer) { vector.add(observer); } @Override public void remove(Observer observer) { vector.remove(observer); } // 通知 @Override public void notifyObservers() { Enumeration\u0026lt;Observer\u0026gt; elements = vector.elements(); while (elements.hasMoreElements()) { elements.nextElement().update(); } } } 实际订阅类\npublic class MySubject extends AbstarctSubject { @Override public void operation() { System.out.println(\u0026quot;observer update 11111\u0026quot;); notifyObservers(); } // 测试 public static void main(String[] args) { MySubject mySubject = new MySubject(); mySubject.add(new Observer1()); mySubject.add(new Observer2()); mySubject.operation(); } } 15. 迭代器模式 Iterator 15.1 概述 提供一个方法顺序访问一个聚合对象中的各个元素, 而又不需要暴露该对象的内部表示;\n在Java的集合类中广泛使用, 迭代模式 Java 允许任何支持Iterator 的集合对象用foreach 循环写出来;\n15.2 实现 实现 Iterable 接口 内部定义数据类型 内部类(可以隐含的持有它所在对象的this引用, 即可以通过ReverseArrayCollection.this拿到它所在的集合), 返回一个Iterator 对象,(==迭代器模式的关键==) public class ReverseArrayCollection\u0026lt;T\u0026gt; implements Iterable\u0026lt;T\u0026gt; { private T[] array; @Override public Iterator\u0026lt;T\u0026gt; iterator() { return new ReverserIterator(); } class ReverserIterator implements Iterator\u0026lt;T\u0026gt;{ int index; // 创建的时候, 索引在数组末尾 public ReverserIterator() { this.index = ReverseArrayCollection.this.array.length; } @Override public boolean hasNext() { return index \u0026gt; 0; } @Override public T next() { // 将索引移动到下一个元素, 并返回 index --; return array[index]; } } } 16. 责任链模式 Chain of Responsibility 16.1 概述 是一种处理请求的模式, 它会让多个处理器有机会处理该请求, 直到该请求被处理\n有多个对象, 每个对象都持有下一个对象的引用, 这样会形成一条链, 请求在这条链上传递, 直到某一个对象决定处理该请求, 所以可以实现在 在隐瞒客户端\n==链接上的请求可以是一条链，可以是一个树，还可以是一个环，模式本身不约束这个，需要我们自己去实现，同时，在一个时刻，命令只允许由一个对象传给另一个对象，而不允许传给多个对象。==\n==\nServlet规范定义的Filter就是一种责任链模式, 它不但允许每个Filter都有机会处理请求，还允许每个Filter决定是否将请求“放行”给下一个Filter：\npublic class AuditFilter implements Filter { public void doFilter(ServletRequest req, ServletResponse resp, FilterChain chain) throws IOException, ServletException { log(req); if (check(req)) { // 放行: chain.doFilter(req, resp); } else { // 拒绝: sendError(resp); } } } 16.2 实现 需求; 财务审批责任链 ==顺序需正确添加==\nManager：只能审核1000元以下的报销； Director：只能审核10000元以下的报销； CEO：可以审核任意额度 抽象出请求对象, 将它在责任链上传递\npublic class Request { private String name; private BigDecimal amount; public Request(String name, BigDecimal amount) { this.name = name; this.amount = amount; } public String getName() { return name; } public BigDecimal getAmount() { return amount; } } 抽象处理器\npublic interface Handler { // 返回Boolean.TRUE = 成功 // 返回Boolean.FALSE = 拒绝 // 返回null = 交下一个处理 Boolean process(Request request); } 依次编写处理类 ManagerHandler, DirectorHandler CEOHandler\npublic class ManagerHandler implements Handler { public Boolean process(Request request) { // 如果超过1000元，处理不了，交下一个处理: if (request.getAmount().compareTo(BigDecimal.valueOf(1000)) \u0026gt; 0) { return null; } // 对Bob有偏见: return !request.getName().equalsIgnoreCase(\u0026quot;bob\u0026quot;); } } 链管理器, 把处理器串联起来, 组成一条链, 并提供一个统一入口进行处理\npublic class HandlerChain { // 持有所有Handler: private List\u0026lt;Handler\u0026gt; handlers = new ArrayList\u0026lt;\u0026gt;(); public void addHandler(Handler handler) { this.handlers.add(handler); } public boolean process(Request request) { // 依次调用每个Handler: for (Handler handler : handlers) { Boolean r = handler.process(request); if (r != null) { // 如果返回TRUE或FALSE，处理结束: System.out.println(request + \u0026quot; \u0026quot; + (r ? \u0026quot;Approved by \u0026quot; : \u0026quot;Denied by \u0026quot;) + handler.getClass().getSimpleName()); return r; } } throw new RuntimeException(\u0026quot;Could not handle request: \u0026quot; + request); } } test\n// 构造责任链: HandlerChain chain = new HandlerChain(); chain.addHandler(new ManagerHandler()); chain.addHandler(new DirectorHandler()); chain.addHandler(new CEOHandler()); // 处理请求: chain.process(new Request(\u0026quot;Bob\u0026quot;, new BigDecimal(\u0026quot;123.45\u0026quot;))); chain.process(new Request(\u0026quot;Alice\u0026quot;, new BigDecimal(\u0026quot;1234.56\u0026quot;))); chain.process(new Request(\u0026quot;Bill\u0026quot;, new BigDecimal(\u0026quot;12345.67\u0026quot;))); chain.process(new Request(\u0026quot;John\u0026quot;, new BigDecimal(\u0026quot;123456.78\u0026quot;))); 18. 命令模式 Command 18.1 概述 将一个请求封装为一个对象,从而使你可用不同的请求对客户进行参数化;\n目的就是达到命令的发出者和执行者之间解耦, 实现请求和执行分开\n18.2 实现 命令接口\npublic void Command { void exe(); } 命令类\npublic class MyCommand implements Command { private Receiver receiver; public MyCommand(Receiver receiver) { this.receiver = receiver; } @Override public void exe() { receiver.action(); } } 被调用者\npublicclass Receiver { public void action(){ sout(\u0026quot;command received\u0026quot;); } } 调用方\npublic class Invoker { private Command command; public Invoker (Command command) { this.command = command; } public void action (){ command.exe(); } } 使用过程\npublic static void main(String[] args) { Receiver receiver = new Receiver(); MyCommand cmd = new MyCommand(receiver); Invoker invoker = new Invoker(cmd); invoker.action(); } 19. 备忘录模式 Memento 19.1 概述 目的是: 保存某个对象的某种状态, 以便在适当的时候恢复对象, 备份模式\n19.2 实现 原始类, 设置构造方法和 存储字段的get,set 方法, 以及设置存储备忘录和恢复备忘录的方法\npublic class Original { private String value; public String getValue() { return value; } public void setValue(String value) { this.value = value; } public Original(String value) { this.value = value; } // 创建备忘录 public Memento createMemento() { return new Memento(value); } // 存储备忘录 public void storageMemento(Memento memento) { this.value = memento.getValue(); } } 备忘录类\npublic class Memento { private String value; public Memento(String value) { this.value = value; } public String getValue() { return value; } public void setValue(String value) { this.value = value; } } 备忘录存储类\npublic class Storage { private Memento memento; public Storage(Memento memento) { this.memento = memento; } public Memento getMemento (){ return memento; } public void setMemento(Memento memento) { this.memento = memento; } } 使用测试\npublic static void main(String[] args) { // 原始类 Original apple = new Original(\u0026quot;apple\u0026quot;); // 创建备忘录 Storage storage = new Storage(apple.createMemento()); // 修改原始类的状态 System.out.println(\u0026quot;初始类的原始状态: \u0026quot; + apple.getValue()); apple.setValue(\u0026quot;watermelon\u0026quot;); System.out.println(\u0026quot;初始类的原始状态被修改成了: \u0026quot; + apple.getValue()); // 恢复原始类的状态 apple.storageMemento(storage.getMemento()); System.out.println(\u0026quot;状态回复后: \u0026quot; + apple.getValue()); } 20. 状态模式 State 20.1 概述 核心思想: 当对象的状态改变, 同时改变它的行为\n状态模式在日常开发中用的挺多的，尤其是做网站的时候，我们有时希望根据对象的某一属性，区别开他们的一些功能，比如说简单的权限控制等。\n20.2 实现 状态类的核心类 (内包含了所有的方法)\npublic class State { private String value; public String getValue() { return value; } public void setValue(String value) { this.value = value; } public void method1 (){ System.out.println(\u0026quot;method1() 执行了\u0026quot;); } public void method2 (){ System.out.println(\u0026quot;method2() 执行了\u0026quot;); } } 状态模式的切换类, 持有状态字段, 构造方法依赖状态, 同时提供状态核心类的\npublic class Context { private State state; public Context(State state) { this.state = state; } public State getState() { return state; } public void setState(State state) { this.state = state; } public void method() { if (\u0026quot;method1\u0026quot;.equals(state.getValue())) { state.method1(); } else if (\u0026quot;method2\u0026quot;.equals(state.getValue())){ state.method2(); } } } 测试: 使用的时候只需要用切换类就可以了\npublic static void main(String[] args) { State state = new State(); Context context = new Context(state); state.setValue(\u0026quot;method1\u0026quot;); context.method(); state.setValue(\u0026quot;method2\u0026quot;); context.method(); } 21. 访问者模式 Visitor 21.1 概述 访问者模式: 就是一种分离对象数据接口与行为的方法, 通过这种分离, 可达到为一个被访问这动态添加新的操作而无需做其他的修改; ==访问者模式的核心思想是为了访问比较复杂的数据结构==，==不去改变数据结构==，而是把==对数据的操作抽象==出来，在“访问”的过程中==以回调形式在访问者中处理操作逻辑==。如果要新增一组操作，那么只需要增加一个新的访问者。\n如果想为现有的类添加新功能\n21.2 实现 访问者接口\n设置访问对象\npublic interface Vistor { public void visit(Subject sub); } 访问者实现类\n```java public class MyVistor implements Vistor { @Override public void visit(Subject sub) { System.out.println(\u0026quot;visit the subject：\u0026quot;+ sub.getSubjet()); } } ``` 访问接受者接口\n接受访问者\n获取被访问属性\n```java public interface Subject { void accept(Vistor vistor); String getSubjet(); } ``` 访问接受者实现类\n```java public class MySubject implements Subject { @Override public void accept(Vistor vistor) { vistor.visit(this); } @Override public String getSubjet() { return \u0026quot;love\u0026quot;; } } ``` test\n```java public class Test { public static void main(String[] args) { Visitor visitor = new MyVisitor(); Subject sub = new MySubject(); sub.accept(visitor); } } ``` 21.3 实现 (递归改造) 访问者模式先把==数据结构（这里是文件夹和文件构成的树型结构）==和对其的==操作（查找文件）分离开==，以后==如果要新增操作==（例如清理.class文件），只需要==新增访问者==，不需要改变现有逻辑。\n场景描述, 假设我们要遍历某个文件夹的所有子文件夹 和文件, 然后找出java 文件\n写递归\n// 递归的问题是, 把数据扫描 和 对 数据的操作混在了一起 public void scan(File dir, List\u0026lt;File\u0026gt; fileList){ for (File file : dir.listFiles()) { if (file.isFile() \u0026amp;\u0026amp; file.getName().endsWith(\u0026quot;.java\u0026quot;)) { fileList.add(file); } else if (file.isDirectory()){ scan(file, fileList); } } } 访问者接口定义的访问者的行为 public interface Vistor { // 访问文件夹 void visitDir(File dir); // 访问文件 void visitFile(File file); } 定义能持有文件和文件夹的数据结构 FileStructure public class FileStructure { // 根目录 private File path; public FileStructure(File path) { this.path = path; } // handle 方法, 传入访问者; 这里把访问者的行为抽象出来了 public void handle(Vistor vistor) { scan(this.path, vistor); } private void scan(File file, Vistor vistor) { if (file.isDirectory()) { // 让访问者处理文件夹: vistor.visitDir(file); for (File sub : file.listFiles()) { // 递归处理子文件夹: scan(sub, vistor); } } else if (file.isFile()) { // 让访问者处理文件: vistor.visitFile(file); } } } 访问javal类的访问者 public class JavaFileVistor implements Vistor { @Override public void visitDir(File dir) { System.out.println(\u0026quot;Visit dir: \u0026quot; + dir); } @Override public void visitFile(File file) { System.out.println(\u0026quot;Found java file: \u0026quot; + file); } } 清除java 文件的 public class ClassFileCleanerVistor implements Vistor { @Override public void visitDir(File dir) { } @Override public void visitFile(File file) { if (file.getName().endsWith(\u0026quot;.class\u0026quot;)) { System.out.println(\u0026quot;Will clean class file: \u0026quot; + file); } } } 22. 中介模式 Mediator 22.1 概述 用一个中介对象来封装一系列的对象交互。中介者使各个对象不需要显式地相互引用，从而使其耦合松散，而且可以独立地改变它们之间的交互。\n","date":"2025-01-03T15:25:43+08:00","permalink":"https://mikeLing-qx.github.io/p/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","title":"设计模式"},{"content":"MongoDB 目标 了解什么是MongoDB 掌握MongoDB的安装 掌握MongoDB的常用命令 掌握mongo的索引 了解mongo集群原理，能够搭建集群 掌握mongodb-driver的基本使用 掌握SpringDataMongoDB的使用 了解如何在原有的spring-data框架中封装新的功能 第一章-MongoDB 知识点-MongoDB简介 1.目标 了解什么是MongoDB 2.路径 为什么要使用MongoDB 什么是MongoDB MongoDB特点 MongoDB体系结构 MongoDB数据类型 3.讲解 3.1 为什么要使用MongoDB 文章评论功能存在以下特点：\n数据量大 写入操作频繁 价值较低 对于这样的数据，我们更适合使用MongoDB来实现数据的存储\n3.2 什么是MongoDB ​\tMongoDB是一个基于分布式文件存储的数据库。由C++语言编写。旨在为WEB应用提供==可扩展的高性能数据存储解决方案==。 ​\tMongoDB是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。它支持的数据结构非常松散，是类似json的bson(数据类型)格式，因此可以存储比较复杂的数据类型。\n3.3 MongoDB特点 ​\tMongo最大的特点是它支持的查询语言非常强大，其语法有点类似于面向对象的查询语言，几乎可以实现类似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。(正则表达式支持索引)\n它的特点是高性能、易部署、易使用，存储数据非常方便。主要功能特性有：\n面向集合存储，易存储对象类型的数据。(集合相当于表) 模式自由。 支持动态查询。 支持完全索引，包含内部对象。 支持查询。 支持复制和故障恢复。(高可用) 使用高效的二进制数据存储，包括大型对象（如视频等）。 自动处理碎片，以支持云计算层次的扩展性。（mapreduce） 支持RUBY，PYTHON，JAVA，C++，PHP，C#等多种语言。 文件存储格式为BSON（一种JSON的扩展）。 支持将mongodb bson文件转换成json 文件\n命令\nbsondump collection.bson \u0026gt; collection.json 3.4 MongoDB体系结构 ​\tMongoDB 的逻辑结构是一种层次结构。主要由：文档(document)、集合(collection)、数据库(database)这三部分组成的。逻辑结构是面向用户的，用户使用 MongoDB 开发应用程序使用的就是逻辑结构。\nMongoDB 的文档（document），相当于关系数据库中的一行记录。 多个文档组成一个集合（collection），相当于关系数据库的表。 多个集合（collection），逻辑上组织在一起，就是数据库（database）。 一个 MongoDB 实例支持多个数据库（database）。 文档(document)、集合(collection)、数据库(database)的层次结构如下图:\nMongoDb 关系型数据库Mysql 数据库(databases) 数据库(databases) 集合(collections) 表(table) 文档(document) 行(row) 3.5 MongoDB数据类型 数据类型 描述 String 字符串。存储数据常用的数据类型。在 MongoDB 中，UTF-8 编码的字符串才是合法的。 Integer 整型数值。用于存储数值。根据你所采用的服务器，可分为 32 位或 64 位。 Boolean 布尔值。用于存储布尔值（真/假）。 Double 双精度浮点值。用于存储浮点值。 Array 用于将数组或列表或多个值存储为一个键。 Timestamp 时间戳。记录文档修改或添加的具体时间。 Object 用于内嵌文档。 Null 用于创建空值。 Date 日期时间。用 UNIX 时间格式来存储当前日期或时间。你可以指定自己的日期时间：创建 Date 对象，传入年月日信息。 Object ID 对象 ID。用于创建文档的 ID。 Binary Data 二进制数据。用于存储二进制数据。 Code 代码类型。用于在文档中存储 JavaScript 代码。 Regular expression 正则表达式类型。用于存储正则表达式。 特殊说明：\nObjectId\nObjectId 类似唯一主键，可以很快的去生成和排序，包含 12 bytes，含义是：\n前 4 个字节表示创建 unix 时间戳，格林尼治时间 UTC 时间，比北京时间晚了 8 个小时 接下来的 3 个字节是机器标识码 紧接的两个字节由进程 id 组成 PID 最后三个字节是随机数 MongoDB 中存储的文档必须有一个 _id 键。这个键的值可以是任何类型的，默认是个 ObjectId 对象\n时间戳\nBSON 有一个特殊的时间戳类型，与普通的日期类型不相关。时间戳值是一个 64 位的值。其中：\n前32位是一个 time_t 值【与Unix新纪元（1970年1月1日）相差的秒数】 后32位是在某秒中操作的一个递增的序数 在单个 mongod 实例中，时间戳值通常是唯一的。\n日期\n表示当前距离 Unix新纪元（1970年1月1日）的毫秒数。日期类型是有符号的, 负数表示 1970 年之前的日期。\n3.6 MongoDB和Redis比较【面试】 比较指标 MongoDB(海量数据) Redis（热点数据） 比较说明 实现语言 c++ c/c++ - 协议 ==BSON,自定义二进制== ==类telnet(TCP/IP)== - 性能 ==依赖内存 (内存映射文件技术)== ==依赖内存==（纯内存） Redis优于MongoDB 可操作性 丰富的数据表达,索引;最类似于关系型数据库,支持丰富的查询语句 数据类型丰富,较少的IO - 内存及存储 适合大数据量存储,依赖系统虚拟内存,采用镜像文件存储;内存占用率比较高,官方建议独立部署在64位系统 Redis2.0后支持虚拟内存特性(VM) 突破物理内存限制;数据可以设置时效性,类似于memcache 不同的应用场景,各有千秋 可用性 支持master-slave,replicatset(内部采用paxos选举算法,自动故障恢复),auto sharding机制,对客户端屏蔽了故障转移和切片机制 依赖客户端来实现分布式读写;主从复制时,每次从节点重新连接主节点都要依赖整个快照,无增量复制;不支持auto sharding,需要依赖程序设定一致性hash机制 MongoDB优于Redis；单点问题上,MongoDB应用简单,相对用户透明,Redis比较复杂,需要客户端主动解决.(MongoDB一般使用replicasets和sharding相结合,replicasets侧重高可用性以及高可靠,sharding侧重性能,水平扩展) 可靠性 从1.8版本后,采用binlog方式(类似Mysql) 支持持久化 依赖快照进行持久化;AOF增强可靠性;增强性的同时,影响访问性能 mongodb在启动时，专门初始化一个线程不断循环（除非应用crash掉），用于在一定时间周期内来从defer队列中获取要持久化的数据并写入到磁盘的journal(日志)和mongofile(数据)处，当然它不是在用户添加记录时就写到磁盘上 一致性 ==以前都版本不支持事务,靠客户端保证== ==最新4.x的支持事务== ==支持事务,比较脆,仅能保证事务中的操作按顺序执行== - 数据分析 内置数据分析功能(mapreduce) 不支持 MongoDB优于Redis 应用场景 ==海量数据存储和访问效率提升== ==热点数据的存储== - 知识点-MongoDB安装 1.目标 掌握MongoDB的安装 2.路径 Docker 环境下MongoDB安装 客户端的安装使用 3.讲解 3.1 Docker 环境下MongoDB安装 在Linux虚拟机中创建mongo容器，命令如下：\ndocker run -di --name=tensquare_mongo -p 27017:27017 mongo 在Window命令行窗口出入登录命令：\nmongo 192.168.200.128 3.2客户端的安装和使用 Mongodb有很多可视化工具，这里我们使用robomongo，可以访问官网：https://robomongo.org/\n我们可以看到有两个版本Studio 3T和Robo 3T\nStudio 3T是一个功能很强大的收费版。。。\nRobo 3T前身就是Robomongo，是一个免费的可视化工具，我们使用他可以很轻松的进行Mongodb的管理。\n在资料中找到robo3t-1.3.1-windows-x86_64-7419c406.exe并双击安装。打开后看到以下界面：\n点击Create创建连接，进行如下配置即可：\n4.小结 docker环境 下载镜像(已经下载了) 启动容器 客户端 默认端口: 27017\n知识点-常用命令 1.目标 掌握MongoDB的常用命令 2.路径 数据库 集合 文档 3.讲解 3.1 选择和创建数据库 选择和创建数据库的语法格式：\nuse 数据库名称 如果数据库存在则选择该数据库，如果数据库不存在则自动创建。以下语句创建commentdb数据库：\nuse commentdb 查看数据库：\nshow dbs 查看集合,需要先选择数据库之后，才能查看该数据库的集合：\nshow collections 3.2 插入与查询文档 选择数据库后，使用集合来对文档进行操作，插入文档语法格式：\ndb.集合名称.insert(数据); 插入以下测试数据：\ndb.comment.insert({content:\u0026quot;十次方课程\u0026quot;,userid:\u0026quot;1011\u0026quot;}) 查询集合的语法格式：\ndb.集合名称.find() db.collection.find(query, projection); projection 也为可选项，表示使用投影操作符指定返回的字段，如果忽略此选项则返回所有字段。 查询spit集合的所有文档，输入以下命令：\ndb.comment.find() ​\t发现文档会有一个叫_id的字段，==这个相当于我们原来关系数据库中表的主键，当你在插入文档记录时没有指定该字段，MongoDB会自动创建，其类型是ObjectID类型==。如果我们在插入文档记录时指定该字段也可以，其类型可以是ObjectID类型，也可以是MongoDB支持的任意类型。\n输入以下测试语句:\ndb.comment.insert({_id:\u0026quot;1\u0026quot;,content:\u0026quot;到底为啥 出错\u0026quot;,username:\u0026quot;张三\u0026quot;,userid:\u0026quot;1012\u0026quot;,thumbup:2020,tags:[\u0026quot;很好\u0026quot;,\u0026quot;十分认同\u0026quot;],tupuser:[{name:\u0026quot;李大\u0026quot;,sex:\u0026quot;女\u0026quot;,age:10},{name:\u0026quot;李二\u0026quot;,sex:\u0026quot;男\u0026quot;,age:42}],lastModifiedDate:new Date()}); db.comment.insert({_id:\u0026quot;2\u0026quot;,content:\u0026quot;加班到半夜\u0026quot;,username:\u0026quot;李 四\u0026quot;,userid:\u0026quot;1013\u0026quot;,thumbup:1023,tags:[\u0026quot;一般\u0026quot;,\u0026quot;不给力\u0026quot;],tupuser:[{name:\u0026quot;李二\u0026quot;,sex:\u0026quot;男\u0026quot;,age:42},{name:\u0026quot;李三\u0026quot;,sex:\u0026quot;女\u0026quot;,age:12}],lastModifiedDate:new Date()}); db.comment.insert({_id:\u0026quot;3\u0026quot;,content:\u0026quot;手机流量超了咋办\u0026quot;,username:\u0026quot;王五\u0026quot;,userid:\u0026quot;1013\u0026quot;,thumbup:111,tags:[\u0026quot;很好\u0026quot;,\u0026quot;给力\u0026quot;],tupuser:[{name:\u0026quot;李三\u0026quot;,sex:\u0026quot;女\u0026quot;,age:12},{name:\u0026quot;李四\u0026quot;,sex:\u0026quot;男\u0026quot;,age:17}],lastModifiedDate:new Date()}); db.comment.insert({_id:\u0026quot;4\u0026quot;,content:\u0026quot;坚持就是胜利\u0026quot;,username:\u0026quot;赵六\u0026quot;,userid:\u0026quot;1014\u0026quot;,thumbup:1223,tags:[\u0026quot;不好\u0026quot;,\u0026quot;说的不对\u0026quot;],tupuser:[{name:\u0026quot;李四\u0026quot;,sex:\u0026quot;男\u0026quot;,age:17},{name:\u0026quot;李五\u0026quot;,sex:\u0026quot;女\u0026quot;,age:26}],lastModifiedDate:new Date()}); db.comment.insert({_id:\u0026quot;5\u0026quot;,content:\u0026quot;手机没电了啊\u0026quot;,username:\u0026quot;李云龙\u0026quot;,userid:\u0026quot;1014\u0026quot;,thumbup:923,tags:[\u0026quot;很好\u0026quot;,\u0026quot;十分认同\u0026quot;],tupuser:[{name:\u0026quot;李五\u0026quot;,sex:\u0026quot;女\u0026quot;,age:26},{name:\u0026quot;李六\u0026quot;,sex:\u0026quot;男\u0026quot;,age:39}],lastModifiedDate:new Date()}); db.comment.insert({_id:\u0026quot;6\u0026quot;,content:\u0026quot;这个手机好\u0026quot;,username:\u0026quot;风清扬\u0026quot;,userid:\u0026quot;1014\u0026quot;,thumbup:123,tags:[\u0026quot;很好\u0026quot;,\u0026quot;十分认同\u0026quot;],tupuser:[{name:\u0026quot;李六\u0026quot;,sex:\u0026quot;男\u0026quot;,age:39},{name:\u0026quot;李七\u0026quot;,sex:\u0026quot;男\u0026quot;,age:62}],lastModifiedDate:new Date()}); 按一定条件来查询，比如查询userid为1013的记录，只要在find()中添加参数即可，参数也是json格式，如下：\ndb.comment.find({userid:'1013'}) 只需要返回符合条件的第一条数据，我们可以使用findOne命令来实现：\ndb.comment.findOne({userid:'1013'}) 返回指定条数的记录，可以在find方法后调用limit来返回结果，例如：\ndb.comment.find().limit(2) 3.3 修改与删除文档 修改文档的语法结构：\ndb.集合名称.update(条件,修改后的数据) 修改_id为1的记录，点赞数为1000，输入以下语句：\ndb.comment.update({_id:\u0026quot;1\u0026quot;},{thumbup:1000}) 执行后发现，这条文档除了thumbup字段其它字段都不见了。\n为了解决这个问题，我们需要使用修改器$set来实现，命令如下：\ndb.comment.update({_id:\u0026quot;2\u0026quot;},{$set:{thumbup:2000}}) 删除文档的语法结构：\ndb.集合名称.remove(条件) 以下语句可以将数据全部删除，慎用~\ndb.comment.remove({}) 删除条件可以放到大括号中，例如删除thumbup为1000的数据，输入以下语句：\ndb.comment.remove({thumbup:1000}) update 更新函数\n$push : 增加一个对象到数组底部 $pushAll: 增加多个对象到数组底部 $pop: 从数组底部删除一个对象 $pull: 如果匹配指定的值, 从数组中删除对应的对象 $pullAll: 如果匹配任意的值, 从数据中删除相应的对象 $addToSet: 如果不存在则增加一个值到数组 3.4 统计条数 统计记录条件使用count()方法。以下语句统计spit集合的记录数：\ndb.comment.count() 按条件统计 ，例如统计userid为1013的记录条数：\ndb.comment.count({userid:\u0026quot;1013\u0026quot;}) 3.5 模糊查询 MongoDB的模糊查询是通过正则表达式的方式实现的。格式为：\n/模糊查询字符串/ 查询评论内容包含“流量”的所有文档，代码如下：\ndb.comment.find({content:/流量/}) 查询评论内容中以“加班”开头的，代码如下：\ndb.comment.find({content:/^加班/}) 3.6 大于 小于 不等于 \u0026lt;, \u0026lt;=, \u0026gt;, \u0026gt;= 这个操作符也是很常用的，格式如下:\ndb.集合名称.find({ \u0026quot;field\u0026quot; : { $gt: value }}) // 大于: field \u0026gt; value db.集合名称.find({ \u0026quot;field\u0026quot; : { $lt: value }}) // 小于: field \u0026lt; value db.集合名称.find({ \u0026quot;field\u0026quot; : { $gte: value }}) // 大于等于: field \u0026gt;= value db.集合名称.find({ \u0026quot;field\u0026quot; : { $lte: value }}) // 小于等于: field \u0026lt;= value db.集合名称.find({ \u0026quot;field\u0026quot; : { $ne: value }}) // 不等于: field != value 查询评论点赞数大于1000的记录：\ndb.comment.find({thumbup:{$gt:1000}}) 3.7 包含与不包含 包含使用$in操作符\n查询评论集合中userid字段包含1013和1014的文档：(select * from comment where userid in(1013,1014))\ndb.comment.find({userid:{$in:[\u0026quot;1013\u0026quot;,\u0026quot;1014\u0026quot;]}}) 不包含使用$nin操作符\n查询评论集合中userid字段不包含1013和1014的文档：\ndb.comment.find({userid:{$nin:[\u0026quot;1013\u0026quot;,\u0026quot;1014\u0026quot;]}}) 3.8 条件连接 我们如果需要查询同时满足两个以上条件，需要使用$and操作符将条件进行关联（相当于SQL的and）。格式为：\n$and:[ {条件},{条件},{条件} ] 查询评论集合中thumbup大于等于1000 并且小于2000的文档：\ndb.comment.find({$and:[ {thumbup:{$gte:1000}} ,{thumbup:{$lt:2000} }]}) 如果两个以上条件之间是或者的关系，我们使用操作符进行关联，与前面and的使用方式相同，格式为：\n$or:[ {条件},{条件},{条件} ] 查询评论集合中userid为1013，或者点赞数小于2000的文档记录：\ndb.comment.find({$or:[ {userid:\u0026quot;1013\u0026quot;} ,{thumbup:{$lt:2000} }]}) 3.9 列值增长 对某列值在原有值的基础上进行增加或减少，可以使用$inc运算符：\ndb.comment.update({_id:\u0026quot;2\u0026quot;},{$inc:{thumbup:1}}) 4.小结 语法下去过一遍就行 知识点-索引的使用 1.目标 掌握索引的使用 2.路径 索引简介 索引基本操作 索引类型 索引属性 索引分析 3.讲解 3.1 索引简介 ​\t索引支持在MongoDB中高效执行查询。没有索引，MongoDB必须执行集合扫描，即扫描集合中的每个文档，以选择与查询语句匹配的那些文档。如果查询存在适当的索引，则MongoDB可以使用该索引来限制它必须检查的文档数。索引是特殊的数据结构，它以易于遍历的形式存储集合数据集的一小部分。索引存储一个特定字段或一组字段的值，按该字段的值排序。索引条目的排序支持有效的相等匹配和基于范围的查询操作。另外，MongoDB可以使用索引中的顺序返回排序的结果。\n下图说明了使用==索引选择和排序匹配文档的查询==：\n​\t从根本上讲，MongoDB中的索引类似于其他数据库系统中的索引。MongoDB在集合 级别定义索引，并支持MongoDB集合中文档的任何字段或子字段的索引。\n3.2 索引基本操作 3.2.1 创建索引 基本语法\ndb.collection.createIndex(keys, options) ​\tkeys可写为要配置索引的字段如{\u0026ldquo;articleid\u0026rdquo;:1},其中，1 为指定按升序创建索引，如果你想按降序来创建索引指定为 -1 即可。但是字段不能指定为_id，因为该字段默认会创建索引。\n​\t注意: 注意在 3.0.0 版本前创建索引方法为 db.collection.ensureIndex()，之后的版本使用了 db.collection.createIndex() 方法，ensureIndex() 还能用，但只是 createIndex() 的别名。\n​\toptions表示创建索引时的设置，这个在索引属性中单独说明：\nex:\n//为articleid字段创建升序索引 db.comment.createIndex({\u0026quot;articleid\u0026quot;:1}) 也可以创建多个字段的联合索引，比如：\n//创建article升序字段和userid降序字段的联合索引 db.comment.createIndex({\u0026quot;articleid\u0026quot;:1,\u0026quot;userid\u0026quot;:-1}) 这种索引只支持：\n​\tdb.comment.find().sort({\u0026ldquo;articleid\u0026rdquo;:1,\u0026ldquo;userid\u0026rdquo;:-1})或者b.comment.find().sort({\u0026ldquo;articleid\u0026rdquo;:-1,\u0026ldquo;userid\u0026rdquo;:1})的查询使用该索引，其他情况如db.comment.find().sort({\u0026ldquo;articleid\u0026rdquo;:1,\u0026ldquo;userid\u0026rdquo;:1})则不能使用，这种情景和关系型数据库中的联合索引表现是一致的。 有关排序顺序和复合索引的更多信息，请参见 使用索引对查询结果进行排序。\n3.2.2 查看索引 查看当前数据库中该集合的所有索引\ndb.comment.getIndexes() 或者\ndb.comment.aggregate( [ { $indexStats: { } } ] ) 3.2.3 删除索引 删除所有索引\ndb.comment.dropIndexes() 删除指定索引\ndb.comment.dropIndex(索引名称) 3.2.4 修改索引 要修改现有索引，您需要删除并重新创建索引。TTL索引是此规则的例外 ，可以通过collMod命令与index收集标志一起 修改TTL索引。\n3.3 索引类型 普通索引, 联合索引 除了上面讲到的普通索引以及联合索引外还有以下类型：\n多键索引（数组索引） ​\tMongoDB使用多键索引来索引存储在数组中的内容。如果您对保存数组值的字段建立索引，则MongoDB将为数组的每个元素创建单独的索引key键。这些多键索引允许查询通过匹配数组的一个或多个元素来选择包含数组的文档。如果索引字段包含数组值，MongoDB会自动确定是否创建多键索引。您无需显式指定多键类型。\nex：可以看到tags字段是一个数组，可以建立数组索引\n//所有文档中的所有元素字段都作为该索引的key db.comment.createIndex({\u0026quot;tags\u0026quot;:1}) ​\t如果该字段是一个对象数组，可以建立\n//对文档中的tupuser字段的age属性建立索引 db.comment.createIndex({\u0026quot;tupuser.age\u0026quot;:1}) ​\t查询点赞用户年龄超过30岁的评论\ndb.comment.find({\u0026quot;tupuser.age\u0026quot;:{$gt:30}}).explain() ​\t发现该索引已经生效了\n文本索引 ​\tMongoDB提供了一种text索引类型，该类型支持在集合中搜索字符串内容。这些文本索引不存储特定于语言的停用词（例如“ the”，“ a”，“ or”），并且在集合中词干仅存储根词。有关文本索引和搜索的更多信息，请参见文本索引。 一个集合最多可以有一个文本索引\n//为username和content字段创建文本索引 db.comment.createIndex( { username: \u0026quot;text\u0026quot;, content: \u0026quot;text\u0026quot; } ) //文本查询单个词条 db.comment.find({$text:{$search:\u0026quot;出错\u0026quot;}}) //文本查询多个词条 db.comment.find({$text:{$search:\u0026quot;出错 李\u0026quot;}}) ​\t可以看到mongo的全文索引建立方式与英文的方式基本一样，是根据词（英文单词）的方式建立的， 如果一个文本里面有多个词条， 则需要按空格方式隔开，所以索引效率比较低，想要实现中文模糊搜素，可以用elasticsearch或者Sphinx，或者lucene 。\n哈希索引 为了支持基于散列的分片，MongoDB提供了一种散列索引类型，该索引类型对字段值的散列进行索引。这些索引在其范围内具有更随机的值分布，但仅 支持相等匹配，而不能支持基于范围的查询。\n//为articleid字段创建hash索引 db.comment.createIndex({\u0026quot;userid\u0026quot;:\u0026quot;hashed\u0026quot;}) 3.4 索引属性 ​\t索引属性参数列表：\nParameter Type Description background Boolean 建索引过程会阻塞其它数据库操作，background可指定以后台方式创建索引，即增加 \u0026ldquo;background\u0026rdquo; 可选参数。 \u0026ldquo;background\u0026rdquo; 默认值为false。 unique Boolean 建立的索引是否唯一。指定为true创建唯一索引。默认值为false. name string 索引的名称。如果未指定，MongoDB的通过连接索引的字段名和排序顺序生成一个索引名称。 dropDups Boolean **3.0+版本已废弃。**在建立唯一索引时是否删除重复记录,指定 true 创建唯一索引。默认值为 false. sparse Boolean 对文档中不存在的字段数据不启用索引；这个参数需要特别注意，如果设置为true的话，在索引字段中不会查询出不包含对应字段的文档.。默认值为 false. expireAfterSeconds integer 指定一个以秒为单位的数值，完成 TTL设定，设定集合的生存时间。 v index version 索引的版本号。默认的索引版本取决于mongod创建索引时运行的版本。 weights document 索引权重值，数值在 1 到 99,999 之间，表示该索引相对于其他索引字段的得分权重。 default_language string 对于文本索引，该参数决定了停用词及词干和词器的规则的列表。 默认为英语 language_override string 对于文本索引，该参数指定了包含在文档中的字段名，语言覆盖默认的language，默认值为 language. 唯一索引 唯一索引可确保索引字段不会存储重复值；即对索引字段实施唯一性。默认情况下， 在创建集合期间，MongoDB在_id字段上创建唯一索引。\n//创建articleid字段的唯一索引 db.comment.createIndex({\u0026quot;articleid\u0026quot;:1},{ unique: true }) 和mysql一样，复合索引也可创建为唯一索引\n部分索引 3.2版中的新功能。\n部分索引仅索引集合中符合指定过滤器表达式的文档。通过索引集合中文档的子集，部分索引具有较低的存储需求，并降低了索引创建和维护的性能成本。\n要创建部分索引，请将该 db.collection.createIndex()方法与 partialFilterExpression选项一起使用。该partialFilterExpression 选项接受使用以下命令指定过滤条件的文档\n等式表达式（即或使用 运算符），field: value$eq $exists: true 表达， $gt，$gte，$lt，$lte表情， $type 表达式， $and 仅限顶级运营商 ex:只给该集合点赞数大于900的评论的thumbup字段创建索引\ndb.comment.createIndex( { thumbup: 1}, { partialFilterExpression: { thumbup: { $gt: 900 } } } ) 部分索引提供了稀疏索引功能的超集，应优先于稀疏索引。\n稀疏索引 稀疏索引仅包含具有索引字段的文档条目，即使索引字段包含空值也是如此。索引会跳过缺少索引字段的所有文档。索引是“稀疏的”，因为它不包括集合的所有文档。相比之下，非稀疏索引包含集合中的所有文档，为那些不包含索引字段的文档存储空值。 将sparse属性设置为true即为稀疏索引\n//插入一条没有tags字段的索引 db.comment.insert({_id:\u0026quot;6\u0026quot;,content:\u0026quot;这个手机好\u0026quot;,username:\u0026quot;风清扬\u0026quot;,userid:\u0026quot;1014\u0026quot;,thumbup:123,tupuser:[{name:\u0026quot;李六\u0026quot;,sex:\u0026quot;男\u0026quot;,age:39},{name:\u0026quot;李七\u0026quot;,sex:\u0026quot;男\u0026quot;,age:62}],lastModifiedDate:new Date()}); //删除原tags索引 db.comment.dropIndex(\u0026quot;tags_1\u0026quot;) //创建tags字段的稀疏索引 db.comment.createIndex({tags: 1},{ sparse: true }) TTL指数 TTL索引是特殊的单字段索引，MongoDB可以使用它们在一定时间后或在特定时钟时间自动从集合中删除文档。数据到期对于某些类型的信息很有用，例如机器生成的事件数据，日志和会话信息，它们仅需要在数据库中保留有限的时间。\n要创建TTL索引，请将该db.collection.createIndex() 方法与expireAfterSeconds选项结合使用，该方法的值（索引字段类型）是日期或包含日期值的数组。ex:\n//当前时间如果在lastModifiedDate的10s之后，则标记该文档过期 db.comment.createIndex({\u0026quot;lastModifiedDate\u0026quot;:1},{expireAfterSeconds:10}) mongo后台的TTL线程会每隔60s删除一次被ttl索引标记为过期的文档\n3.5 索引分析 //对查询进行分析 db.comment.find().explain() 作用和mysql的explain关键字一样都是用来分析执行计划的\nqueryPlanner：执行计划\nwinningPlan：竞争成功的计划 如果该计划使用了索引，会有以下参数 indexName：索引名称 isMultiKey：是否为多键索引 isUnique：是否唯一索引 isSparse：是否是稀疏索引 isPartial：是否是部分索引 如果没有以上参数，表示没有使用索引 rejectedPlans：竞争失败的计划 4.总结 知识点-mongo集群搭建\t1.目标 了解集群原理，能够搭建出分片集群 查看集群状态\ndb.stats() 2.路径 3.讲解 3.1 集群简介 ​\tmongodb集群搭建方式有三种，1、主从(官方已经不推荐)，2、副本集，3、分片。这里介绍如何通过分片sharding方式搭建mongodb集群。sharding集群方式也基于副本集，在搭建过程中，需要对分片和配置节点做副本集。最后将做好的副本集的分片加入到路由节点，构成集群。\n3.2 副本集 3.2.1副本集角色 主节点（Primary） 接收所有的写请求，然后把修改同步到所有Secondary。一个Replica Set只能有一个Primary节点，当Primary挂掉后，其他Secondary或者Arbiter节点会重新选举出来一个主节点。 默认读请求也是发到Primary节点处理的，可以通过修改客户端连接配置以支持读取Secondary节点。 副本节点（Secondary） 与主节点保持同样的数据集。当主节点挂掉的时候，参与选主。 仲裁者（Arbiter） 不保有数据，不参与选主，只进行选主投票。使用Arbiter可以减轻数据存储的硬件需求，Arbiter几乎没什么大的硬件资源需求，但重要的一点是，在生产环境下它和其他数据节点不要部署在同一台机器上。 3.2.2 两种架构模式 PSS Primary + Secondary + Secondary模式，通过Primary和Secondary搭建的Replica Set Diagram of a 3 member replica set that consists of a primary and two secondaries. 该模式下 Replica Set节点数必须为奇数，目的是选主投票的时候要出现大多数才能进行选主决策。\nPSA Primary + Secondary + Arbiter模式，使用Arbiter搭建Replica Set 偶数个数据节点，加一个Arbiter构成的Replica Set\n3.3 分片 sharding方式的集群中，有三类角色，分别是shard， config servers，router（mongos）。如下图所示。\n​\tshard： 分片节点，存储数据。每个分片都可以部署为副本集。 router： 路由节点，是mongo集群与外部客户端连接的入口，他提供mongos客户端，对客户端透明，让客户端感觉使用单节点数据库。 config servers ： 配置节点，不会存储数据，会存储元数据信息，比如片键的范围。 从MongoDB 3.4开始，必须将配置服务器部署为副本集。 3.2 搭建集群 3.2.1 创建配置服务复本集 docker run --name configsvr0 -d mongo --configsvr --replSet \u0026quot;rs_configsvr\u0026quot; --bind_ip_all docker run --name configsvr1 -d mongo --configsvr --replSet \u0026quot;rs_configsvr\u0026quot; --bind_ip_all docker run --name configsvr2 -d mongo --configsvr --replSet \u0026quot;rs_configsvr\u0026quot; --bind_ip_all 3.2.2 创建分片复本集 创建分片0复制集\ndocker run --name shardsvr00 -d mongo --shardsvr --replSet \u0026quot;rs_shardsvr0\u0026quot; --bind_ip_all docker run --name shardsvr01 -d mongo --shardsvr --replSet \u0026quot;rs_shardsvr0\u0026quot; --bind_ip_all docker run --name shardsvr02 -d mongo --shardsvr --replSet \u0026quot;rs_shardsvr0\u0026quot; --bind_ip_all 创建分片1复制集\ndocker run --name shardsvr10 -d mongo --shardsvr --replSet \u0026quot;rs_shardsvr1\u0026quot; --bind_ip_all docker run --name shardsvr11 -d mongo --shardsvr --replSet \u0026quot;rs_shardsvr1\u0026quot; --bind_ip_all docker run --name shardsvr12 -d mongo --shardsvr --replSet \u0026quot;rs_shardsvr1\u0026quot; --bind_ip_all 执行 docker inspect -f \u0026lsquo;{{.Name}} - {{.NetworkSettings.IPAddress }}\u0026rsquo; $(docker ps -aq) 得到我们刚刚启动的容器的名称和ip列表\n/shardsvr12 - 172.17.0.11 /shardsvr11 - 172.17.0.10 /shardsvr10 - 172.17.0.9 /shardsvr02 - 172.17.0.8 /shardsvr01 - 172.17.0.7 /shardsvr00 - 172.17.0.6 /configsvr2 - 172.17.0.5 /configsvr1 - 172.17.0.4 /configsvr0 - 172.17.0.3 /tensquare_mongo - 172.17.0.2 3.2.3 初始化配置副本集和分片副本集 ps:使用--configsvr构建的配置容器默认的端口是27019\n#进入容器 docker exec -it configsvr0 bash #连接configsvr0 mongo --host 172.17.0.3 --port 27019 #启动配置副本集 rs.initiate( { _id: \u0026quot;rs_configsvr\u0026quot;, configsvr: true, members: [ { _id : 0, host : \u0026quot;172.17.0.3:27019\u0026quot; }, { _id : 1, host : \u0026quot;172.17.0.4:27019\u0026quot; }, { _id : 2, host : \u0026quot;172.17.0.5:27019\u0026quot; } ] } ) #exit退出 同理，以相同的方式初始化分片副本集\nps:使用--shardsvr构建的分片容器默认的端口是27018\n初始化分片副本集0：\n#连接shardsvr00 mongo --host 172.17.0.6 --port 27018 #初始化分片副本集0 rs.initiate( { _id : \u0026quot;rs_shardsvr0\u0026quot;, members: [ { _id : 0, host : \u0026quot;172.17.0.6:27018\u0026quot; }, { _id : 1, host : \u0026quot;172.17.0.7:27018\u0026quot; }, { _id : 2, host : \u0026quot;172.17.0.8:27018\u0026quot; } ] } ) #exit退出 初始化分片副本集1:\n#连接shardsvr10 mongo --host 172.17.0.9 --port 27018 #初始化分片副本集1 rs.initiate( { _id : \u0026quot;rs_shardsvr1\u0026quot;, members: [ { _id : 0, host : \u0026quot;172.17.0.9:27018\u0026quot; }, { _id : 1, host : \u0026quot;172.17.0.10:27018\u0026quot; }, { _id : 2, host : \u0026quot;172.17.0.11:27018\u0026quot; } ] } ) #exit退出 3.2.4 创建集群入口并关联配置集 ps:默认是mongod(分片处理模式),我们需要将起修改为mongos(路由模式),负责路由和协调操作，使得集群像一个整体的系统\n#exit退出到root目录 #创建路由实例 docker run --name mongos0 -p 27027:27017 -d --entrypoint \u0026quot;mongos\u0026quot; mongo --configdb rs_configsvr/172.17.0.3:27019,172.17.0.4:27019,172.17.0.5:27019 --bind_ip_all 同样执行 docker inspect -f \u0026lsquo;{{.Name}} - {{.NetworkSettings.IPAddress }}\u0026rsquo; $(docker ps -aq) 得到容器的名称和ip的对应列表，并获得mongo0容器的ip为172.17.0.12\n3.2.5 在集群入口(路由)上挂载分片集 ps:容器默认的端口是27017\n#进入容器 docker exec -it mongos0 bash #连接mongos0 mongo --host 172.17.0.12 --port 27017 sh.addShard(\u0026quot;rs_shardsvr0/172.17.0.6:27018,172.17.0.7:27018,172.17.0.8:27018\u0026quot;) sh.addShard(\u0026quot;rs_shardsvr1/172.17.0.9:27018,172.17.0.10:27018,172.17.0.11:27018\u0026quot;) 3.2.6 测试集群 进入路由容器创建commentdb数据库并启用分片\nsh.enableSharding(\u0026quot;commentdb\u0026quot;) 分片方式有两种远程分片和hash分片，我们采用hash分片\n#对 commentdb.comment 的 _id 字段进行哈希分片 sh.shardCollection(\u0026quot;commentdb.comment\u0026quot;, {\u0026quot;_id\u0026quot;: \u0026quot;hashed\u0026quot; }) 插入数据测试下\n#使用数据库 use commentdb #插入数据 db.comment.insert({_id:\u0026quot;1\u0026quot;,content:\u0026quot;到底为啥 出错\u0026quot;,username:\u0026quot;张三\u0026quot;,userid:\u0026quot;1012\u0026quot;,thumbup:2020,tags:[\u0026quot;很好\u0026quot;,\u0026quot;十分认同\u0026quot;],tupuser:[{name:\u0026quot;李大\u0026quot;,sex:\u0026quot;女\u0026quot;,age:10},{name:\u0026quot;李二\u0026quot;,sex:\u0026quot;男\u0026quot;,age:42}],lastModifiedDate:new Date()}); db.comment.insert({_id:\u0026quot;2\u0026quot;,content:\u0026quot;加班到半夜\u0026quot;,username:\u0026quot;李 四\u0026quot;,userid:\u0026quot;1013\u0026quot;,thumbup:1023,tags:[\u0026quot;一般\u0026quot;,\u0026quot;不给力\u0026quot;],tupuser:[{name:\u0026quot;李二\u0026quot;,sex:\u0026quot;男\u0026quot;,age:42},{name:\u0026quot;李三\u0026quot;,sex:\u0026quot;女\u0026quot;,age:12}],lastModifiedDate:new Date()}); db.comment.insert({_id:\u0026quot;3\u0026quot;,content:\u0026quot;手机流量超了咋办\u0026quot;,username:\u0026quot;王五\u0026quot;,userid:\u0026quot;1013\u0026quot;,thumbup:111,tags:[\u0026quot;很好\u0026quot;,\u0026quot;给力\u0026quot;],tupuser:[{name:\u0026quot;李三\u0026quot;,sex:\u0026quot;女\u0026quot;,age:12},{name:\u0026quot;李四\u0026quot;,sex:\u0026quot;男\u0026quot;,age:17}],lastModifiedDate:new Date()}); db.comment.insert({_id:\u0026quot;4\u0026quot;,content:\u0026quot;坚持就是胜利\u0026quot;,username:\u0026quot;赵六\u0026quot;,userid:\u0026quot;1014\u0026quot;,thumbup:1223,tags:[\u0026quot;不好\u0026quot;,\u0026quot;说的不对\u0026quot;],tupuser:[{name:\u0026quot;李四\u0026quot;,sex:\u0026quot;男\u0026quot;,age:17},{name:\u0026quot;李五\u0026quot;,sex:\u0026quot;女\u0026quot;,age:26}],lastModifiedDate:new Date()}); db.comment.insert({_id:\u0026quot;5\u0026quot;,content:\u0026quot;手机没电了啊\u0026quot;,username:\u0026quot;李云龙\u0026quot;,userid:\u0026quot;1014\u0026quot;,thumbup:923,tags:[\u0026quot;很好\u0026quot;,\u0026quot;十分认同\u0026quot;],tupuser:[{name:\u0026quot;李五\u0026quot;,sex:\u0026quot;女\u0026quot;,age:26},{name:\u0026quot;李六\u0026quot;,sex:\u0026quot;男\u0026quot;,age:39}],lastModifiedDate:new Date()}); db.comment.insert({_id:\u0026quot;6\u0026quot;,content:\u0026quot;这个手机好\u0026quot;,username:\u0026quot;风清扬\u0026quot;,userid:\u0026quot;1014\u0026quot;,thumbup:123,tags:[\u0026quot;很好\u0026quot;,\u0026quot;十分认同\u0026quot;],tupuser:[{name:\u0026quot;李六\u0026quot;,sex:\u0026quot;男\u0026quot;,age:39},{name:\u0026quot;李七\u0026quot;,sex:\u0026quot;男\u0026quot;,age:62}],lastModifiedDate:new Date()}); 到分片数据库看下数据情况\n#连接mongo mongo --host 172.17.0.9 --port 27018 #选择数据库 use commentdb #查看数量 db.comment.count() 可以看到已经得到预期效果 那么备份呢?(备份默认情况下不允许读取,需要执行db.getMongo().setSlaveOk()来允许读取)\nmongo --host 172.17.0.7 --port 27018 db.getMongo().setSlaveOk() db.comment.count() 4.总结 知识点-mongo聚合查询 1.目标 管道查询 map-reduce 2.路径 3.讲解 3.1 管道查询 MongoDB的聚合框架以数据处理管道的概念为模型。文档进入多阶段流水线，该流水线将文档转换成汇总结果。例如：\n在这个例子中\ndb.orders.aggregate([ { $match: { status: \u0026quot;A\u0026quot; } }, { $group: { _id: \u0026quot;$cust_id\u0026quot;, total: { $sum: \u0026quot;$amount\u0026quot; } } } ]) 第一阶段：$match阶段按status字段过滤文档，并将status等于的文档传递到下一阶段\u0026quot;A\u0026quot;。\n第二阶段：该$group阶段按cust_id字段将文档分组，以计算每个唯一值的总和cust_id。\n最基本的管道阶段提供过滤器，其操作类似于查询和修改输出文档形式的文档转换。\n其他管道操作提供了用于按特定字段对文档进行分组和排序的工具，以及用于聚合包括文档数组在内的数组内容的工具。另外，管道阶段可以将运算符用于诸如计算平均值或连接字符串之类的任务。\n管道使用MongoDB中的本机操作提供有效的数据聚合，并且是MongoDB中数据聚合的首选方法。\n聚合管道可以操作 分片集合。\n聚合管道可以在某些阶段使用索引来提高其性能。另外，聚合管道具有内部优化阶段。有关详细信息，请参见管道运算符和索引以及 聚合管道优化。\nex:获取当前点赞数大于200并统计用户的总点赞数\ndb.comment.aggregate([ { $match: { thumbup: {$gt:200} } }, { $group: { _id: \u0026quot;$userid\u0026quot;, total: { $sum: \u0026quot;$thumbup\u0026quot; } } } ]) 3.2 map-reduce MongoDB还提供了map-reduce操作来执行聚合。通常，map-reduce操作有两个阶段：\nmap阶段: 处理每个文档并为每个输入文档发出一个或多个对象的， reduce阶段: 以及将map操作的输出组合在一起的。 可选地，map-reduce可以具有一个 finalize阶段来对结果进行最终修改。与其他聚合操作一样，map-reduce可以指定查询条件以选择输入文档以及对结果进行排序和限制。\nMap-reduce使用自定义JavaScript函数来执行映射和归约操作以及可选的finalize操作。尽管自定义JavaScript与聚合管道相比提供了极大的灵活性，但总的来说，map-reduce比聚合管道效率低下，并且复杂性更高。\nMap-reduce可以对分片 集合进行操作。映射减少操作也可以输出到分片集合。有关详细信息，请参见 Map-Reduce和分片集合。\nex:获取当前点赞数大于200并统计用户的总点赞数\nvar mapFunction1 = function() { emit(this.userid, this.thumbup); }; var reduceFunction1 = function(keyUid, thumbupArr) { return Array.sum(thumbupArr); }; db.comment.mapReduce( mapFunction1, reduceFunction1, { query: {thumbup:{$gt:200}}, out: \u0026quot;thumbup_sum\u0026quot; } ) db.thumbup_sum.find().sort({value:-1}); 修改成我们想要的样子\n4.总结 知识点-mongodb-driver使用 mongodb-driver ​\tmongodb-driver是mongo官方推出的java连接mongoDB的驱动包，相当于JDBC驱动。我们现在来使用mongodb-driver完成对Mongodb的操作。\n添加以下依赖：\n\u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.mongodb\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mongodb-driver\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.10.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;junit\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.12\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 使用步骤:\n创建客户端连接 mongoclient 获取数据库 获取集合 操作集合 3.2 查询所有 @Test public void test1() { //创建连接 MongoClient client = new MongoClient(\u0026quot;192.168.184.136\u0026quot;,27027); //打开数据库 MongoDatabase commentdb = client.getDatabase(\u0026quot;commentdb\u0026quot;); //获取集合 MongoCollection\u0026lt;Document\u0026gt; comment = commentdb.getCollection(\u0026quot;comment\u0026quot;); //查询 FindIterable\u0026lt;Document\u0026gt; documents = comment.find(); //查询记录获取文档集合 for (Document document : documents) { System.out.println(\u0026quot;_id：\u0026quot; + document.get(\u0026quot;_id\u0026quot;)); System.out.println(\u0026quot;内容：\u0026quot; + document.get(\u0026quot;content\u0026quot;)); System.out.println(\u0026quot;用户ID:\u0026quot; + document.get(\u0026quot;userid\u0026quot;)); System.out.println(\u0026quot;点赞数：\u0026quot; + document.get(\u0026quot;thumbup\u0026quot;)); } //关闭连接 client.close(); } 3.3根据_id查询 每次使用都要用到MongoCollection，进行抽取：\nprivate MongoClient client; private MongoCollection\u0026lt;Document\u0026gt; comment; @Before public void init() { //创建连接 client =new MongoClient(\u0026quot;192.168.184.136\u0026quot;,27027); //打开数据库 MongoDatabase commentdb = client.getDatabase(\u0026quot;commentdb\u0026quot;); //获取集合 comment = commentdb.getCollection(\u0026quot;comment\u0026quot;); } @After public void after() { client.close(); } 测试根据_id查询：\n@Test public void test2() { //查询 FindIterable\u0026lt;Document\u0026gt; documents = comment.find(new BasicDBObject(\u0026quot;_id\u0026quot;, \u0026quot;1\u0026quot;)); //查询记录获取文档集合 for (Document document : documents) { System.out.println(\u0026quot;_id：\u0026quot; + document.get(\u0026quot;_id\u0026quot;)); System.out.println(\u0026quot;内容：\u0026quot; + document.get(\u0026quot;content\u0026quot;)); System.out.println(\u0026quot;用户ID:\u0026quot; + document.get(\u0026quot;userid\u0026quot;)); System.out.println(\u0026quot;点赞数：\u0026quot; + document.get(\u0026quot;thumbup\u0026quot;)); } } 3.4新增 @Test public void test3() { Map\u0026lt;String, Object\u0026gt; map = new HashMap(); map.put(\u0026quot;_id\u0026quot;, \u0026quot;6\u0026quot;); map.put(\u0026quot;content\u0026quot;, \u0026quot;很棒！\u0026quot;); map.put(\u0026quot;userid\u0026quot;, \u0026quot;9999\u0026quot;); map.put(\u0026quot;thumbup\u0026quot;, 123); Document document = new Document(map); comment.insertOne(document); } 3.5修改 @Test public void test4() { //修改的条件 Bson filter = new BasicDBObject(\u0026quot;_id\u0026quot;, \u0026quot;6\u0026quot;); //修改的数据 Bson update = new BasicDBObject(\u0026quot;$set\u0026quot;, new Document(\u0026quot;userid\u0026quot;, \u0026quot;8888\u0026quot;)); comment.updateOne(filter, update); } 3.6 删除 @Test public void test5() { //删除的条件 Bson filter = new BasicDBObject(\u0026quot;_id\u0026quot;, \u0026quot;6\u0026quot;); comment.deleteOne(filter); } 4.小结 mongodb-driver: mongo官方提供的一个Java操作Mongo客户端 类似jdbc驱动 如果有条件, 我们使用 new BasicDbObject() 一个BasicDbObject就相当于语法里面的一组{} 操作过程 创建连接 获取数据库对象 获取集合对象 操作集合 关闭连接 我们使用mongodb-driver有些繁琐, 项目里面使用Spring-Data-mongo, 内部就是封装了mongodb-driver 第二章-SpringDataMongoDB 知识点-基本crud实现 1.目标 掌握文章评论环境准备 2.路径 开发准备环境准备 基本CRUD 3.讲解 3.1 开发准备 ​\tSpringDataMongoDB是SpringData家族成员之一，用于操作MongoDb的持久层框架，封装了底层的==mongodb-driver==。本功能使用SpringDataMongoDB进行开发\n步骤:\n创建maven项目\n添加SpringDataMongoDB起步依赖\n在application.yml里面配置MongoDB\n创建pojo(和集合对应)\n@Document(collection=\u0026ldquo;集合名称\u0026rdquo;) @Id标记主键 @CompoundIndexes({@CompoundIndex(),@CompoundIndex()})添加索引 创建一个Dao接口继承MongoRepository\n添加依赖： \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.1.4.RELEASE\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-mongodb\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 添加配置文件： spring: data: mongodb: host: 192.168.184.136 port: 27027 database: commentdb 创建实体类 @Document(collection = \u0026quot;comment\u0026quot;) @CompoundIndexes({ @CompoundIndex(name = \u0026quot;idx_userid\u0026quot;, def = \u0026quot;{'userid': 'hashed'}\u0026quot;, background = true) }) public class Comment implements Serializable { @Id private String _id; private String articleid; private String content; private String userid; private String parentid; private Date publishdate; private Integer thumbup; } 3.2基本CRUD 3.2.1新增 CommentService @Service public class CommentService { @Autowired private IdWorker idWorker; @Autowired private CommentRepository commentRepository; public void add(Comment comment) { String id = idWorker.nextId() + \u0026quot;\u0026quot;; comment.set_id(id); //初始化数据 comment.setPublishdate(new Date()); comment.setThumbup(0); commentRepository.save(comment); } } CommentRepository public interface CommentRepository extends MongoRepository\u0026lt;Comment,String\u0026gt;{ } 在com.tensquare.article.repository包下创建\n3.2.2删除 业务:\n把所有的子评论也删除 不删子评论, 只删自己 CommentService public void deleteById(String id) { commentRepository.deleteById(id); } 递归删除 //1.查询出父id为commentid的数据做为集合A List\u0026lt;Comment\u0026gt; childList = commentDao.findByParentid(commentid); //2.删除id为commentid的数据 commentDao.deleteById(commentid); //3.遍历集合A，递归调用deleteById(A[i]._id) for (Comment comment : childList) { deleteById(comment.get_id()); } 3.2.3修改 CommentService public void update(Comment comment) { commentRepository.save(comment); } 这种方式的缺陷就是会覆盖原本的数据，应该只修改要修改的数据\n//1.查询条件 Query query = new Query(); query.addCriteria(new Criteria(\u0026quot;_id\u0026quot;).is(comment.get_id())); //2.修改内容 /*Update update = new Update(); update.inc(\u0026quot;thumbup\u0026quot;,1).set(\u0026quot;content\u0026quot;,\u0026quot;aaaaaa\u0026quot;);*/ Update update = MongoUpdateUtils.attrUpdateByBean(comment); //3.集合名称 mongoTemplate.updateFirst(query,update,\u0026quot;comment\u0026quot;); 创建一个工具类通过反射封装Update\npublic class MongoUpdateUtils { public static Update attrUpdateByBean(Object object) { Update update = new Update(); //1.通过反射拿到属性 Field[] declaredFields = object.getClass().getDeclaredFields(); for (Field declaredField : declaredFields) { try { declaredField.setAccessible(true);//允许不通过get方法直接获取私有属性值 Object value = declaredField.get(object); //2.通过get获取值如果不为空的话作为Update要修改属性 if(value!=null){ update = update.set(declaredField.getName(),value); } } catch (IllegalAccessException e) { e.printStackTrace(); } } return update; } } 3.2.4查询所有 CommentService public List\u0026lt;Comment\u0026gt; findAll() { return commentRepository.findAll(); } 3.2.5根据id查询 CommentService public Comment findById(String id) { return commentRepository.findById(id).get(); } 3.2.6 获取当前点赞数大于200并统计用户的总点赞数 public List\u0026lt;Map\u0026gt; findTotalThumbupByUserId(){ TypedAggregation\u0026lt;Comment\u0026gt; commentTypedAggregation = Aggregation.newAggregation(Comment.class, Aggregation.match(Criteria.where(\u0026quot;thumbup\u0026quot;).gt(1000)), Aggregation.group(\u0026quot;userid\u0026quot;).sum(\u0026quot;thumbup\u0026quot;).as(\u0026quot;sum\u0026quot;) ); AggregationResults\u0026lt;Map\u0026gt; aggregate = mongoTemplate.aggregate(commentTypedAggregation, mongoTemplate.getCollectionName(Comment.class), Map.class); return aggregate.getMappedResults(); } 3.2.7 按照点赞数排序，查询前3条评论 public PageResult\u0026lt;Comment\u0026gt; findByPage(int current, int size){ Sort sort = Sort.by(Sort.Direction.DESC,\u0026quot;thumbup\u0026quot;); Query query = new Query(); long total = mongoTemplate.count(query, Comment.class); int offset = (current- 1)*size; List\u0026lt;Comment\u0026gt; list = mongoTemplate.find(query.with(sort).limit(size).skip(offset), Comment.class, mongoTemplate.getCollectionName(Comment.class)); PageResult\u0026lt;Comment\u0026gt; pageResult = new PageResult(); pageResult.setCurrent(current); pageResult.setData(list); pageResult.setPages((int) ((total/size)+1)); return pageResult; } 4.小结 知识点-框架封装 1.目标 了解如何在原有的springdata框架上封装自己的功能 将我们自己实现的修改功能封装进框架 封装分页查询功能 封装聚合查询功能 2.路径 讲解如何在原有的springdata框架上封装自己的功能 讲解如何将我们自己实现的修改功能封装进框架 讲解如何封装分页查询功能 讲解如何封装聚合查询功能 3.讲解 3.1 在原有的springdata框架上封装自己的功能 ​\tMongoRepository虽然已经封装了一些基本功能，但是在我们日常开发的过程中会发现仍然不够用，比如分页，聚合查询都需要使用mongoTepmlate，如果你是架构师，为了保证开发人员在开发过程中能够更加规范的使用框架，我们需要对框架进一步进行封装，所以我们需要对SpringDataMongoDB框架进行研究，了解MongoRepository是如何实现接口的功能，我们又该如何合理的封装一些新的功能进来。这个过程和其他的springdata框架都是类似的。\n​\t首先找到这个类\n​\t随便找个方法，找到实现类\n​\t正是这个SimpleMongoRepository\n​\t那么这个实现类又是如何注册到ioc容器的呢\n​\t选择第一个，是个工厂类\n​\t那么这个工厂类又是如何工作的呢，我们ctrl+左键点击类查看\n​\t发现配置到了配置文件中，我们当然可以仿照它的方式也创建一个配置文件，保存自定义的工厂类，或者使用@EnableMongoRepositories注解更加方便一些，经过我们的查看，接下来要做的事很清楚了：\n创建接口封装自定义的方法，最好将接口分开，比如分页的单独放一个接口，聚合查询单独放一个接口，基本的增删改也单独放一个接口\n创建一个接口继承所有创建的接口包括MongoRepository\n创建该接口的实现类\n在启动类上添加@EnableMongoRepositories注解，并将我们的实现类配置进来\n​\t开始动手吧：\n1.创建HMMongoRepository,以后这个接口就是我们要实现的接口\npublic interface HMMongoRepository\u0026lt;T, ID extends Serializable\u0026gt;{ } 2.创建该接口的实现类\npublic class HMMongoRespositoryImpl\u0026lt;T, ID extends Serializable\u0026gt; extends SimpleMongoRepository\u0026lt;T, ID\u0026gt; implements HMMongoRepository\u0026lt;T,ID\u0026gt; { private final MongoOperations mongoOperations; private final MongoEntityInformation\u0026lt;T, ID\u0026gt; entityInformation; /** * Creates a new {@link SimpleMongoRepository} for the given {@link MongoEntityInformation} and {@link MongoTemplate}. * * @param metadata must not be {@literal null}. * @param mongoOperations must not be {@literal null}. */ public HMMongoRespositoryImpl(MongoEntityInformation\u0026lt;T, ID\u0026gt; metadata, MongoOperations mongoOperations) { super(metadata, mongoOperations); this.entityInformation = metadata; this.mongoOperations = mongoOperations; } } 3.2 将我们自己实现的修改功能封装进框架 创建接口\npublic interface BaseMongoRepository\u0026lt;T,TD\u0026gt; extends MongoRepository\u0026lt;T,TD\u0026gt; { public int upsert(Query query, Update update); public int update(T t); } 将接口添加到HMMongoRepository继承列表中\npublic interface HMMongoRepository\u0026lt;T, ID extends Serializable\u0026gt; extends MongoRepository\u0026lt;T, ID\u0026gt;,BaseMongoRepository\u0026lt;T,ID\u0026gt; { } 添加实现\n@Override public int upsert(Query query, Update update) { return (int) mongoTemplate.upsert(query, update, entityClass).getModifiedCount(); } @Override public int update(T t) { Query query = new Query(); query.addCriteria(new Criteria(\u0026quot;_id\u0026quot;).is(entityInformation.getId(t))); Update update = MongoUpdateUtils.attrUpdateByBean(t); return (int)mongoOperations.updateFirst(query,update,entityInformation.getCollectionName()).getModifiedCount(); } 3.3 封装分页查询功能 封装一个Page接口\npublic interface Page\u0026lt;T\u0026gt; { Page getInstance(int total, int current, int size, List\u0026lt;T\u0026gt; list); } 封装一个分页接口\npublic interface PageMongoRespository\u0026lt;T,TD\u0026gt; { public long count(@Nullable Query query); public Page\u0026lt;T\u0026gt; findPage(Query query, int current, int size, @Nullable Sort sort, Page\u0026lt;T\u0026gt; page); } 添加到HMMongoRepository继承列表中,实现该方法\n@Override public long count(@Nullable Query query) { if (query == null) query = new Query(); return mongoOperations.count(query, entityInformation.getJavaType()); } @Override public Page\u0026lt;T\u0026gt; findPage(Query query, int current, int size, @Nullable Sort sort, Page\u0026lt;T\u0026gt; page) { long total = count(query); int offset = (current- 1)*size; List\u0026lt;T\u0026gt; list = mongoOperations.find(query.with(sort).limit(size).skip(offset), entityInformation.getJavaType(), entityInformation.getCollectionName()); return page.getInstance((int) total, current, size, list); } 3.4 封装聚合查询功能 3.4.1 管道查询 接口\npublic interface AggregationOperationMongoDao\u0026lt;T,TD\u0026gt; { public List\u0026lt;T\u0026gt; findAggregationOperation(List\u0026lt;AggregationOperation\u0026gt; aggregationOperationList); } 实现\n@Override public List\u0026lt;T\u0026gt; findAggregationOperation(List\u0026lt;AggregationOperation\u0026gt; aggregationOperationList) { Aggregation aggregation = Aggregation.newAggregation(aggregationOperationList); AggregationResults\u0026lt;T\u0026gt; outputType = mongoOperations.aggregate(aggregation,entityInformation.getCollectionName(), entityInformation.getJavaType()); List\u0026lt;T\u0026gt; resultList = new ArrayList\u0026lt;\u0026gt;(); Iterator\u0026lt;T\u0026gt; iterator = outputType.iterator(); while (iterator.hasNext()) { resultList.add(iterator.next()); } return resultList; } 4.小结 第三章 聚合查询 0. mysql类比 1. 聚合管道操作符 pipeline \u0026ndash; stage种类\n$count , $group, $match, $project, $unwind, $limit, $skip, $sort, $sortByCount, $lookup, $out, $addFields\n1.1 $count 语法\n{ $count: \u0026lt;string\u0026gt; } 示例\ndb.collection.aggregate( [ { $group: { _id: null, myCount: { $sum: 1 } } }, #这里myCount自定义，相当于mysql的select count(*) as myCount { $project: { _id: 0 } } # 返回不显示_id字段 ] ) 1.2 $group 指定的表达式对文档进行分组，并将每个不同分组的文档输出到下一个阶段,$group不会输出具体的文档而只是统计信息。\naccumulator操作符\n语法\n{ $group: { _id: \u0026lt;expression\u0026gt;, \u0026lt;field1\u0026gt;: { \u0026lt;accumulator1\u0026gt; : \u0026lt;expression1\u0026gt; }, ... } } _id字段是必填的;但是，可以指定_id值为null来为整个输入文档计算累计值。 剩余的计算字段是可选的，并使用运算符进行计算。 _id和表达式可以接受任何有效的表达式。 查询distinct values\n1.3 $match 语法\n{ $match: { \u0026lt;query\u0026gt; } } 在实际应用中尽可能将$match放在管道的前面位置。这样有两个好处：\n一是可以快速将不需要的文档过滤掉，以减少管道的工作量；\n二是如果再投射和分组之前执行$match，查询可以使用索引。\n不能在$ match查询中使用$作为聚合管道的一部分。 要在$match阶段使用$text，$match阶段必须是管道的第一阶段。 视图不支持文本搜索。 1.4 $unwind 简单说就是 可以将数组拆分为单独的文档。\n使用语法\n{ $unwind: { path: \u0026lt;field path\u0026gt;, includeArrayIndex: \u0026lt;string\u0026gt;, #可选,一个新字段的名称用于存放元素的数组索引。该名称不能以$开头。 preserveNullAndEmptyArrays: \u0026lt;boolean\u0026gt; #可选，default :false，若为true,如果路径为空，缺少或为空数组，则$unwind输出文档 } } 1.5 $project ==可以利用，$project 对输入文档进行再次投影==\n1. 语法 { $project: { \u0026lt;specification(s)\u0026gt; } } specifications有以下形式：\n: \u0026lt;1 or true\u0026gt; 是否包含该字段，field:1/0，表示选择/不选择 field\n_id: \u0026lt;0 or false\u0026gt; 是否指定_id字段\n: 添加新字段或重置现有字段的值。 在版本3.6中更改：MongoDB 3.6添加变量REMOVE。如果表达式的计算结果为$$REMOVE，则该字段将排除在输出中\n2. 例子 db.person.insert([ {name:{firstName:'Jonathan', lastName:'Lee'}, age:18, book:{name:'玩转HTML', price: 88}}, {name:{firstName:'Amelie', lastName:'BNTang'}, age:17, book:{name:'玩转JavaScript', price: 99}} ]); db.person.find(); db.person.aggregate([ { $project:{ _id:0, clientName: '$name.firstName', clientAge: '$age' } } ]); 1.6 $lookup 相当于 sql 的 left outer join\n2. find详解 操作符 格式 实例 与 RDBMS where 语句比较 等于（=） { : {}} db.test.find( {price : 24} ) where price = 24 大于（\u0026gt;） { : {$gt : }} db.test.find( {price : {$gt : 24}} ) where price \u0026gt; 24 小于（\u0026lt;） { : {$lt : }} db.test.find( {price : {$lt : 24}} ) where price \u0026lt; 24 大于等于（\u0026gt;=） { : {$gte : }} db.test.find( {price : {$gte : 24}} ) where price \u0026gt;= 24 小于等于（\u0026lt;=） { : {$lte : }} db.test.find( {price : {$lte : 24}} ) where price \u0026lt;= 24 不等于（!=） { : {$ne : }} db.test.find( {price : {$ne : 24}} ) where price != 24 与（and） {key01 : value01, key02 : value02, \u0026hellip;} db.test.find( {name : \u0026ldquo;《MongoDB 入门教程》\u0026rdquo;, price : 24} ) where name = \u0026ldquo;《MongoDB 入门教程》\u0026rdquo; and price = 24 或（or） {$or : [{key01 : value01}, {key02 : value02}, \u0026hellip;]} db.test.find( {$or:[{name : \u0026ldquo;《MongoDB 入门教程》\u0026rdquo;},{price : 24}]} ) where name = \u0026ldquo;《MongoDB 入门教程》\u0026rdquo; or price = 24 查询逻辑对照表\nsql mql a = 1 AND b = 1 {a: 1, b: 1} 或 {$and : [{a:1},{b:1}]} a = 1OR b = 1 {$or: [{a: 1},{b: 1}]} a is NULL {a: {$exists: false}} a IN (1, 2, 3) {a: {$in: [1, 2, 3] }} db.test.find({age:null}) 此语句不仅匹配出 age 为 null 的文档，其他不同类型的文档也会被查出。这是因为 null 不仅会匹配某个键值为 null 的文档，而且还会匹配不包含这个键的文档\n查询有三个元素的数组\ndb.test.find( { tags:{$size:3} } ) limiit()函数与SQL中的作用相同, 用于限制查询结果的个数\ndb.test.find().limit(3) Skip() 函数用于略过指定个数的文档，如下语句略过第一个文档，返回后两个：\ndb.test.find( { tags:{$size:3} } ) sort() 函数用于对查询结果进行排序，1 是升序，-1 是降序，如下语句可将查询结果升序显示\ndb.test.find().sort({\u0026quot;price\u0026quot; : 1}) 3. 游标 4. 数组操作 $all, $size, $slice, $elemMatch\n4.1 $all 查找数组中包含指定值的文档\n语法\n{filed: { $all: [ \u0026lt;value1\u0026gt;, \u0026lt;value2\u0026gt; ] }} db.orders.find( { \u0026quot;books\u0026quot;:{$all:[\u0026quot;java\u0026quot;, \u0026quot;mongodb\u0026quot;]} } ) 4.2 $size 查找数组代销等于指定值的文档\n语法\n{filed: { $size: number }} 4.3 $slice 查询数组中指定返回元素的个数\n语法\ndb.collect.find( {},filed:{$slice:number} ) db.collect.find( {}, {field:{ $slice:[ number1, number2 ] }}) number说明\n为正数表示返回前面指定的值的个数, 例如为1, 返回数组第一个\n为负数, 例如为-1, 返回数组倒数第一个\n4.4 $elemMatch 数组元素操作符\n作用：数组值中至少一个元素满足所有指定的匹配条件 语法： { \u0026lt;field\u0026gt;: { $elemMatch: { \u0026lt;query1\u0026gt;, \u0026lt;query2\u0026gt;, ... } } } 说明： 如果查询为单值查询条件，即只有\u0026lt;query1\u0026gt;，则无需指定$elemMatch 搜索子对象的多个字段时, 如果使用$elemMatch, 它表示必须是同一个子对象满足多个条件\n5.0 mongodb compass https://blog.csdn.net/jamin_liu_90/article/details/88017013\n6.0 聚合性能分析 db.report.explain().aggregate() /* 1 */ { \u0026quot;stages\u0026quot; : [ { \u0026quot;$cursor\u0026quot; : { \u0026quot;query\u0026quot; : { \u0026quot;formId\u0026quot; : 5.0, \u0026quot;userId\u0026quot; : 1.0 }, \u0026quot;fields\u0026quot; : { \u0026quot;answers\u0026quot; : 1, \u0026quot;formId\u0026quot; : 1, \u0026quot;userId\u0026quot; : 1, \u0026quot;_id\u0026quot; : 1 }, \u0026quot;queryPlanner\u0026quot; : { \u0026quot;plannerVersion\u0026quot; : 1, \u0026quot;namespace\u0026quot; : \u0026quot;form.report\u0026quot;, \u0026quot;indexFilterSet\u0026quot; : false, \u0026quot;parsedQuery\u0026quot; : { \u0026quot;$and\u0026quot; : [ { \u0026quot;formId\u0026quot; : { \u0026quot;$eq\u0026quot; : 5.0 } }, { \u0026quot;userId\u0026quot; : { \u0026quot;$eq\u0026quot; : 1.0 } } ] }, \u0026quot;queryHash\u0026quot; : \u0026quot;8046B318\u0026quot;, \u0026quot;planCacheKey\u0026quot; : \u0026quot;8046B318\u0026quot;, \u0026quot;winningPlan\u0026quot; : { \u0026quot;stage\u0026quot; : \u0026quot;COLLSCAN\u0026quot;, // 查询方式，常见的有COLLSCAN/全表扫描、IXSCAN/索引扫描、 FETCH/根据索引去检索文档、SHARD_MERGE/合并分片结果、IDHACK/针对_id进行查询 \u0026quot;filter\u0026quot; : { \u0026quot;$and\u0026quot; : [ { \u0026quot;formId\u0026quot; : { \u0026quot;$eq\u0026quot; : 5.0 } }, { \u0026quot;userId\u0026quot; : { \u0026quot;$eq\u0026quot; : 1.0 } } ] }, \u0026quot;direction\u0026quot; : \u0026quot;forward\u0026quot; }, \u0026quot;rejectedPlans\u0026quot; : [] } } }, { \u0026quot;$unwind\u0026quot; : { \u0026quot;path\u0026quot; : \u0026quot;$answers\u0026quot; } }, { \u0026quot;$match\u0026quot; : { \u0026quot;answers.id\u0026quot; : { \u0026quot;$eq\u0026quot; : 64.0 } } }, { \u0026quot;$unwind\u0026quot; : { \u0026quot;path\u0026quot; : \u0026quot;$answers.contents\u0026quot; } }, { \u0026quot;$sort\u0026quot; : { \u0026quot;sortKey\u0026quot; : { \u0026quot;answers.contents.reportTime\u0026quot; : -1 }, \u0026quot;limit\u0026quot; : NumberLong(1) } }, { \u0026quot;$project\u0026quot; : { \u0026quot;_id\u0026quot; : true, \u0026quot;formId\u0026quot; : true, \u0026quot;userId\u0026quot; : true, \u0026quot;answers\u0026quot; : { \u0026quot;id\u0026quot; : true, \u0026quot;contents\u0026quot; : { \u0026quot;reportTime\u0026quot; : true } } } } ], \u0026quot;ok\u0026quot; : 1.0 } 参数解释\n实践 db.report.aggregate([ { $match:{ formId:5, userId:1 } }, { $unwind:{ path:\u0026quot;$answers\u0026quot; } }, { $match: { \u0026quot;answers.id\u0026quot;: 1 } }, { $project: { answers: 1 } }, { $unwind: { path: \u0026quot;$answers.contents\u0026quot; } } ]) 第四章 文档设计 注意: mongodb 聚合管道中的数据不能超过 16mb\n深拷贝 public Object deepClone() throws Exception { Object cloneObj = null; ByteArrayOutputStream out = new ByteArrayOutputStream(); ObjectOutputStream obs = new ObjectOutputStream(out); obs.writeObject(this); obs.close(); ByteArrayInputStream ios = new ByteArrayInputStream(out.toByteArray()); ObjectInputStream ois = new ObjectInputStream(ios); cloneObj = ois.readObject(); return cloneObj; } ","date":"2025-01-03T14:32:15+08:00","permalink":"https://mikeLing-qx.github.io/p/mongodb/","title":"MongoDb"},{"content":"0. mapping 1. 概述 2. 数据类型 3. dynamic Mapping 类型自动识别的方式\n4. mapping 的更改 只能新增, 不能修改\n5. 配置 1. 默认配置 2. null_value 需要对null 值实现搜索 只有 keyword 类型支持设定 null_value 3. 数组类型 ElasticSearch 不提供专门的数组类型, 但是任何字段, 都可以包含多个相同类类型的数值\n4. copy to 在mapping 上进行配置, 搜索的时候 可以把两个字段的进行一个联合的搜索\n6. 多字段特性 It is often useful to index the same field in different ways for different purposes. This is the purpose of multi-fields. For instance, a string field could be mapped as a text field for full-text search, and as a keyword field for sorting or aggregations: 7. 自定义分词 character filter 在Tokenizer 之前对文本进行处理, 例如 增删替换字符串, 会影响tokenizer 的 position 和offset 信息\n自带的\nHtml strip \u0026ndash; 去除html 标签 Mapping \u0026ndash; 字符串替换 Pattern -filter \u0026ndash; 正则匹配替换 tokenizer 将原始的文本按照一定的规则, 切分为词 (term or token)\n有内置的, 也可以用 Java 开发插件, 实现自己的 Tokenizer\ntoken filter 将Tokenizer 输出的单词, 进行增加, 修改, 删除\n自带的: lowercase /stop /synonym (添加近义词)\n1. 版本对照 spring boot data elasticsearch 版本: 7.6.2\n项目spring boot version: 2.5.4\n| Spring Data Release Train | Spring Data Elasticsearch | Elasticsearch | Spring Framework | Spring Boot | | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- | \u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; | | 2022.0 (Turing) | 5.0.x | 8.4.2 | 6.0.x | 3.0.x? | | 2021.2 (Raj) | 4.4.x | 7.17.3 | 5.3.x | 2.7.x | | 2021.1 (Q) | 4.3.x | 7.15.2 | 5.3.x | 2.6.x | | 2021.0 (Pascal) | 4.2.x[1] | 7.12.0 | 5.3.x | 2.5.x | | 2020.0 (Ockham)[1] | 4.1.x[1] | 7.9.3 | 5.3.2 | 2.4.x | | Neumann[1] | 4.0.x[1] | 7.6.2 | 5.2.12 | 2.3.x | | Moore[1] | 3.2.x[1] | 6.8.12 | 5.2.12 | 2.2.x | | Lovelace[1] | 3.1.x[1] | 6.2.2 | 5.1.19 | 2.1.x | | Kay[1] | 3.0.x[1] | 5.5.0 | 5.0.13 | 2.0.x | | Ingalls[1] | 2.1.x[1] | 2.4.0 | 4.3.25 | 1.5.x | 提供操作es-api 的框架\nspring-data sparkStreaming Flink https://blog.csdn.net/u011863024/article/details/115721328 2. 常用分词器 自带\nStandard Analyzer - 默认分词器，按英文空格切分\nSimple Analyzer - 按照非字母切分(符号被过滤)\nStop Analyzer - 小写处理，停用词过滤(the,a,is)\nWhitespace Analyzer - 按照空格切分，不转小写\nKeyword Analyzer - 不分词，直接将输入当作输出\nPatter Analyzer - 正则表达式，默认\\W+(非字符分割)\n中文\n分词器 优势 劣势 地址 IK Analyzer 简单易用，支持自定义词典和远程词典 词库需要自行维护，不支持词性识别 https://github.com/medcl/elasticsearch-analysis-ik Ansj 分词精准度不错，支持词性识别 对标hanlp词库略少，学习成本高 https://github.com/NLPchina/elasticsearch-analysis-ansj jieba 新词发现功能 不支持词性识别 https://github.com/sing1ee/elasticsearch-jieba-plugin Smart Chinese Analyzer 官方插件 中文分词效果惨不忍睹 https://artifacts.elastic.co/downloads/elasticsearch-plugins/analysis-smartcn/analysis-smartcn-7.16.1.zip Hanlp 目前词库最完善，支持的特性非常多 需要更优的分词效果，学习成本高 https://github.com/hankcs/HanLP/releases 3. ik 分词器远程字典 ik 分词器会==每隔 1 分钟调用一次 head 方法根据 etag 来判断字典是否更新了==，如果更新了的话才会调用 get 方法去下载字典。先调用 head 方法确认更新后再调用 get下载，这样就节省了带宽流量，也省的去更新词典了。\n1. 配置文件 /xxx/elasticsearch-7.12.0/plugins/ik/config/IKAnalyzer.cfg.xml\n\u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;UTF-8\u0026quot;?\u0026gt; \u0026lt;!DOCTYPE properties SYSTEM \u0026quot;http://java.sun.com/dtd/properties.dtd\u0026quot;\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;comment\u0026gt;IK Analyzer 扩展配置\u0026lt;/comment\u0026gt; \u0026lt;!--用户可以在这里配置自己的扩展字典 --\u0026gt; \u0026lt;entry key=\u0026quot;ext_dict\u0026quot;\u0026gt;shfq_ext_dic.dic\u0026lt;/entry\u0026gt; \u0026lt;!--用户可以在这里配置自己的扩展停止词字典--\u0026gt; \u0026lt;entry key=\u0026quot;ext_stopwords\u0026quot;\u0026gt;\u0026lt;/entry\u0026gt; \u0026lt;!--用户可以在这里配置远程扩展字典 --\u0026gt; \u0026lt;entry key=\u0026quot;remote_ext_dict\u0026quot;\u0026gt;http://localhost:8162/test/downloadExtDic?fileName=extWords.dic\u0026lt;/entry\u0026gt; \u0026lt;!--用户可以在这里配置远程扩展停止词字典--\u0026gt; \u0026lt;entry key=\u0026quot;remote_ext_stopwords\u0026quot;\u0026gt;http://localhost:8162/test/downloadExtDic?fileName=stopWords.dic\u0026lt;/entry\u0026gt; \u0026lt;/properties\u0026gt; 2. 接口实现 在下载扩展、停用词典的接口里返回 Last-Modified 或 ETag responder header 。\nimport com.huitongjy.common.util.LogUtils; import io.swagger.annotations.Api; import io.swagger.models.HttpMethod; import org.apache.commons.lang3.StringUtils; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.core.io.UrlResource; import org.springframework.http.HttpHeaders; import org.springframework.http.MediaType; import org.springframework.http.ResponseEntity; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; import javax.servlet.http.HttpServletRequest; import java.io.File; import java.io.IOException; import java.net.MalformedURLException; import java.nio.file.Files; import java.nio.file.Path; import java.nio.file.Paths; import java.nio.file.attribute.BasicFileAttributes; @RestController @Api(value = \u0026quot;PrivateController\u0026quot;, tags = \u0026quot;测试\u0026quot;) public class TestController { private static final Logger LOG = LoggerFactory.getLogger(PrivateController.class); @RequestMapping(value = \u0026quot;/test/downloadExtDic\u0026quot;) public ResponseEntity\u0026lt;org.springframework.core.io.Resource\u0026gt; downloadFileFromLocal(HttpServletRequest request, String fileName) { if (StringUtils.isEmpty(fileName)) { return null; } // 可以把字典的配置文件写到配置中心 String location = \u0026quot;\u0026quot;; if (!location.endsWith(File.separator)) { location += File.separator; } String method = request.getMethod(); boolean isHead = HttpMethod.HEAD.name().equals(method); String pathStr = location + fileName; File file = new File(pathStr); if (!file.exists()) { LogUtils.error(LOG, \u0026quot;字典不存在\u0026quot;, \u0026quot;path\u0026quot;, pathStr); return null; } Path path = Paths.get(pathStr); org.springframework.core.io.Resource resource = null; try { resource = new UrlResource(path.toUri()); } catch (MalformedURLException e) { LogUtils.error(LOG, \u0026quot;加载字典异常\u0026quot;, e); return null; } HttpHeaders httpHeaders = new HttpHeaders(); httpHeaders.add(HttpHeaders.CONTENT_DISPOSITION, \u0026quot;attachment; filename=\\\u0026quot;\u0026quot; + fileName + \u0026quot;\\\u0026quot;\u0026quot;); httpHeaders.add(HttpHeaders.ETAG, generateLastUpdateDate(path) + \u0026quot;\u0026quot;); if (isHead) { resource = null; } return ResponseEntity.ok() .contentType(MediaType.TEXT_PLAIN) .headers(httpHeaders) .body(resource); } private long generateLastUpdateDate(Path path) { try { BasicFileAttributes attr = Files.readAttributes(path, BasicFileAttributes.class); Long l = attr.lastModifiedTime().toMillis(); return l; } catch (IOException e) { return -1; } } } 4. 问题 默认的分词请求 最大字数为10000字, 如何修改\n分词相关参数的含义与使用\n# 只能为指定的index进行分词 PUT /analyze_sample { \u0026quot;settings\u0026quot; : { \u0026quot;index.analyze.max_token_count\u0026quot; : 100000 } } 文档先于关键词添加, 倒排索引已经生成过了, 后续添加了关键词 之后进行搜索的时候能否命中? 验证流程:\n先添加文章 分词器\u0026ndash;添加关键词 搜索 分析词条索引 注意 : ==对新添加的索引数据就会按照扩展词典进行分词。但是原有的索引数据不会，需要对原有数据重新生成索引==\n能否给ik分词器配置多个远程的拓展词库 不行\nElasticsearchTemplate 注入失败的原因是什么? springboot集成es的版本更新很快，在7.x版本已经弃用了ElasticSearchTemplate，进而使用ElasticSearchRestTemplate\n如何进行精确搜索? 使用standard 分词, match_pharse 查询\nes 的index 名必须是小写的 !\n嵌套文档查询\nnested中查询的是嵌套文档的内容，语法与正常查询时一致。\n文档的分数计算需注意\nnested 查询肯定可以匹配到多个嵌套的文档。每一个匹配的嵌套文档都有自己的相关度得分，但是这众多的分数最终需要汇聚为可供根文档使用的一个分数 默认情况下，根文档的分数是这些嵌套文档分数的平均值。可以通过设置 score_mode 参数来控制这个得分策略，相关策略有 avg (平均值), max (最大值), sum (加和) 和 none (直接返回 1.0 常数值分数)。 词条出现次数统计 ​\t配置termvectors, 本质还是空间换时间\nPOST message_index/_bulk {\u0026quot;index\u0026quot;:{\u0026quot;_id\u0026quot;:1}} {\u0026quot;message\u0026quot;:\u0026quot;沉溺于「轻易获得高成就感」的事情：沉溺于有意无意地寻求用很小付出获得很大「huibao」的偏方，哪怕huibao是虚拟的\u0026quot;} {\u0026quot;index\u0026quot;:{\u0026quot;_id\u0026quot;:2}} {\u0026quot;message\u0026quot;:\u0026quot;过度追求“短期huibao”可以先思考这样一个问题：为什么玩王者荣耀沉溺我们总是停不下来huibao\u0026quot;} {\u0026quot;index\u0026quot;:{\u0026quot;_id\u0026quot;:3}} {\u0026quot;message\u0026quot;:\u0026quot;过度追求的努力无法带来超额的huibao，就因此放弃了努力。这点在聪明人身上尤其明显。以前念本科的时候身在沉溺\u0026quot;} PUT message_index { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;term_vector\u0026quot;: \u0026quot;with_positions_offsets_payloads\u0026quot;, \u0026quot;store\u0026quot;: true, \u0026quot;analyzer\u0026quot;: \u0026quot;ik_max_word\u0026quot; } } } } GET message_index/_termvectors/1?fields=message 自动预警 生效范围\n文件类型 部门 发布时间 关键词\n首先 \u0026ndash; \u0026gt; IK 分词器拓展词 \u0026ndash;\u0026gt; 会使用所有启用的关键词\n方案1:\n查询出文章对应的筛查关键词\n计算命中的关键词\n存储到 article_keyword\n方案2:\n直接存储到ES中\n根据每个关键词的生效策略, 使用DSL 语句筛查出对应的文章\n存储到 article_keyword\n如何打印执行的DSL 语句?\n参考资料 https://docs.spring.io/spring-data/elasticsearch/docs/current/reference/html/#repositories.definition\n简单处理\nOptional.ofNullable(searchQuery.getFilter()).ifPresent(r -\u0026gt; log.info(\u0026quot;DSL-filter:{}\u0026quot;, r.toString())); Optional.ofNullable(searchQuery.getQuery()).ifPresent(r -\u0026gt; log.info(\u0026quot;DSL-query:{}\u0026quot;, r.toString())); Optional.ofNullable(searchQuery.getElasticsearchSorts()).ifPresent(r -\u0026gt; log.info(\u0026quot;Sort-query:{}\u0026quot;, r.toString())); 专项筛查 GET article/_search { \u0026quot;query\u0026quot;: { \u0026quot;bool\u0026quot;: { \u0026quot;should\u0026quot;: [ { \u0026quot;match_phrase\u0026quot;: { \u0026quot;contentStandardAnalyzer\u0026quot;: \u0026quot;深圳创新\u0026quot; } }, { \u0026quot;match_phrase\u0026quot;: { \u0026quot;title\u0026quot;: \u0026quot;深圳创新\u0026quot; } } ], \u0026quot;must\u0026quot;: [ { \u0026quot;range\u0026quot;: { \u0026quot;publicationDate\u0026quot;: { \u0026quot;gte\u0026quot;: \u0026quot;2022-09-29\u0026quot;, \u0026quot;lte\u0026quot;: \u0026quot;2022-09-30\u0026quot; } } },{ \u0026quot;terms\u0026quot;: { \u0026quot;publicationDeptId\u0026quot;: [ \u0026quot;4\u0026quot;, \u0026quot;5\u0026quot; ] } } ] } } } elasticsearch 事务 Elasticsearch 支持单个文档级别的原子创建、更新和删除操作，但没有内置对多文档事务的支持。 可以使用乐观锁, 带上version字段, 或者可以通过外置的关系型数据库进行处理\n什么是elasticSearch 的子字段 https://www.elastic.co/guide/en/elasticsearch/reference/current/multi-fields.html\n5. 精确搜索方案 问题描述\n1. multi_field 默认情况下使用标准分析器分析字段。如果您想检查完全匹配，您可以存储未分析的字段 \u0026quot;dog\u0026quot;:{ \u0026quot;type\u0026quot;:\u0026quot;multi_field\u0026quot;, \u0026quot;fields\u0026quot;:{ \u0026quot;dog\u0026quot;:{ \u0026quot;include_in_all\u0026quot;:false, \u0026quot;type\u0026quot;:\u0026quot;string\u0026quot;, \u0026quot;index\u0026quot;:\u0026quot;not_analyzed\u0026quot;, \u0026quot;store\u0026quot;:\u0026quot;no\u0026quot; }, \u0026quot;_tokenized\u0026quot;:{ \u0026quot;include_in_all\u0026quot;:false, \u0026quot;type\u0026quot;:\u0026quot;string\u0026quot;, \u0026quot;index\u0026quot;:\u0026quot;analyzed\u0026quot;, \u0026quot;store\u0026quot;:\u0026quot;no\u0026quot; } } } 然后您可以查询 dog-field 以获得精确匹配，并查询 dog._tokenized 用于分析查询（如全文）\n2. Standard 分词 结合 bool 查询 should + match_pharse\n查询 --\u0026gt; 飞科剃须刀 bool: { should: { \u0026quot;科剃须\u0026quot; } match_pharse: { \u0026quot;科剃须\u0026quot; } } 6. function_score 参考资料: https://juejin.cn/post/6844904022948724744 Elasticsearch进行全文搜索的时候，==默认是使用BM25计算的_score字段进行降序排序的==。当我们需要用==其他字段进行降序或者升序排序的时候，可以使用sort字段==，传入我们想要的排序字段和方式。 当简单的使用几个字段升降序排列组合无法满足我们的需求的时候，我们就需要==自定义排序的特性==，Elasticsearch提供了function_score的DSL来自定义打分，这样就可以根据自定义的_score来进行排序。\n可以再查询结束后对每一个匹配的文档进行重新算分, 根据新生成的分数进行排序\n算法函数\nweight : 为每一个文档设置一个简单而不被规范化的权重 field value factor: 使用该数字来修改_score, 例如将 \u0026ldquo;热度\u0026rdquo; 和\u0026quot;点赞数\u0026quot; 作为算分的参考因素 random score: 为每一个用户使用一个不同的, 随机算分结果 衰减函数: 以某个字段的值为标准, 距离某个值越近, 得分越高 script scores: 自定义脚本完全控制 一致性随机文档\nGET article/_search { \u0026quot;from\u0026quot;: 0, \u0026quot;size\u0026quot;: 2, \u0026quot;query\u0026quot;: { \u0026quot;function_score\u0026quot;: { \u0026quot;random_score\u0026quot;: { \u0026quot;seed\u0026quot;: 314124 } } } } 脚本排序\n\u0026quot;sort\u0026quot;: [ { \u0026quot;publicationDate\u0026quot;: { \u0026quot;order\u0026quot;: \u0026quot;asc\u0026quot; } }, { \u0026quot;_script\u0026quot;: { \u0026quot;script\u0026quot;: { \u0026quot;source\u0026quot;: \u0026quot;def type = doc['book_type'].value; def time = doc['publish_time'].value;if(type==1) return time; else return -time\u0026quot;, \u0026quot;lang\u0026quot;: \u0026quot;painless\u0026quot; }, \u0026quot;type\u0026quot;: \u0026quot;number\u0026quot;, \u0026quot;order\u0026quot;: \u0026quot;asc\u0026quot; } } ] 7. constant_score + filter TF \u0026ndash; 词频 (检索词频率)\nIDF \u0026ndash; 词在文档中出现的概率 (反向文档频率)\n检索词频率 检索词在该字段出现的频率？出现频率越高，相关性也越高。 字段中出现过 5 次要比只出现过 1 次的相关性高。 反向文档频率 每个检索词在索引中出现的频率？频率越高，相关性越低。检索词出现在多数文档中会比出现在少数文档中的权重更低。 字段长度准则 字段的长度是多少？长度越长，相关性越低。 检索词出现在一个短的 title 要比同样的词出现在一个长的 content 字段权重更大。 可以跳过算分步骤提高系统性能\n8. bool 1. 概述 子查询可以任意顺序出现 可以嵌套多个查询 如果bool查询中, 没有must条件, should 中必须至少满足一条查询才会有返回 2. 多值精确匹配 对多值字段进行精确匹配的话 例如: List\n可以通过增加一个 genre count 字段进行计数\n2. 算分 算分过程\n查询should 语句中的两个查询 加和两个查询的评分 乘以匹配语句的总数 除以所有语句的总数 must 必须匹配, 贡献算分 should 选择性匹配, 贡献算分 must_not filter_context 不贡献算分 filter filter_context 不贡献算分 bool 查询的结构也会对算分产生影响 同一级下的竞争字段会具有相同的权重\n通过嵌套bool查询, 可以改变对算分的影响\nBoosting 是控制相关度的一种手段 索引, 字段 或查询子条件 参数 boost 的含义\nbootst \u0026gt; 1, 打分的相关度相对性提高 0\u0026lt; boost \u0026lt;1 , 打分的权重相对性降低 boost \u0026lt; 0, 贡献负分 9. dix max query 单字符串多字段查询\n会将所有查询字段匹配度最高的 字段来算分\ntie_breaker 介于0-1 之间的浮点数, 0 代表最佳匹配, 1 代表所有语句同等重要\n获得最佳匹配语句的评分-score 将其他匹配语句的评分与tie_breaker 相乘 对以上评分求和并规范化 10. multi match 单字符串多字段查询\n三种type:\nCROSS_FIELDS 希望这个词条的分词词汇是分配到不同字段中的, 支持 Operator MOST_FIELDS 越多字段匹配的文档分越高 ,⽆法使⽤ Operator BEST_FIELDS 完全匹配的文档占的评分比较高 11. term\u0026amp;pharse suggester suggester 就是一种特殊类型的搜索,\n搜索引擎中类似的功能，在 Elasticsearch 中是通过 Suggester API 实现的 原理：将输⼊的⽂本分解为 Token，然后在索引的字典⾥查找相似的 Term 并返回 Elasticsearch 设计了 4 种类别的 Suggesters Term \u0026amp; Phrase Suggester Complete \u0026amp; Context Suggester 自动补全和上下文提示, 不是使用倒排索引的, 只能基于前缀 12. 聚合分析 1. bucket \u0026amp; metric aggregation select count(brand) from cars group by brand; metric 相当于 count , 一些统计方法 bucket 相当与group by , 一组满足条件的文档 ​\tES对聚合索引的桶数有限制，默认是10000, 可以进行配置\n1. metric aggregation ● 单值分析：只输出⼀个分析结果\n○ min, max, avg, sum\n○ Cardinality （类似 distinct Count）\n● 多值分析：输出多个分析结果\n○ stats, extended stats\n○ percentile, percentile rank\n○ top hits （排在前⾯的示例）\n2. terms aggregation ==对于Text 类型的数据默认是关闭的, 需要配置mapping 开启, 但是聚合的时候会对text 类型进行分词的==\nterms 不准的问题:\n原因是, 数据分散在多个分片上, Coordinating node 无法获取数据的全貌\n提升shard_size 的参数, 每次从分片上获取额外多的数据, 提升准确率\n● 按照⼀定的规则，将⽂档分配到不同的\n桶中，从⽽达到分类的⽬的。ES 提供的\n⼀些常⻅的 Bucket Aggregation\n==Terms==\n==数字类型==\n​\t● Range / Data Range\n​\t● Histogram / Date Histogram\n==⽀持嵌套==：也就在桶⾥再做分桶 优化terms 聚合的性能, 把以下配置打开, 每当加入新数据的时候会自动加入cache(预加载)\n场景\n在当聚合查询比较频繁, 而且会有新文档不断写入的情况下可以打开这个配置 https://www.elastic.co/guide/en/elasticsearch/reference/7.1/tune-for-search-speed.html\n3. range \u0026amp; histogram aggregation 按照数字的范围，进⾏分桶\n● 在 Range Aggregation 中，可以⾃定义 Key\n​\tDemo：\n​\t● 按照⼯资的 Range 分桶\n​\t● 按照⼯资的间隔（Histogram）分桶\n4. bucket + metrix aggregation Bucket 聚合分析允许通过添加**⼦聚合**分析来进⼀步分析，⼦聚合分析可以是\nbucket metric Demo\n● 按照⼯作类型进⾏分桶，并统计⼯资信息\n● 先按照⼯作类型分桶，然后按性别分桶，并统计⼯资信\n2. Pipeline 聚合 管道的概念: 支持对聚合分析的结果, 再次进行聚合\nPipeline 的 分析结果会输出到原结果中, 根据位置的不同, 分为两类\nSibling - 结果和现有分析结果同级\nMax, min, Avg \u0026amp; Sum bucket Stats, Extended status Bucket Percentiles Bucket Parent - 结果内嵌到现有的聚合分析结果之中\nDerivative （求导）\nCumultive Sum （累计求和）\nMoving Function (滑动窗⼝)\n3. 聚合作用范围 ES 聚合分析的默认作⽤范围是 query 的查询结果集\n同时 ES 还⽀持以下⽅式改变聚合的作⽤范围\nFilter 只对当前的子聚合语句生效\nPost_Filter\n是对聚合分析后的⽂档进⾏再次过滤 size 无需设置为0 使用场景 一条语句, 获取聚合信息 + 获取符合条件的文档 Global, 无视query, 对全部文档进行统计\n4. 聚合分析 在 Terms Aggregation 的返回中有两个特殊的数值\ndoc_count_error_upper_bound ： 被遗漏的term 分桶，包含的⽂档，有可能的最⼤值\nsum_other_doc_count: 除了返回结果 bucket 的 terms 以外，其他 terms 的⽂档总数（总数-返回的总数）\n13. 数据建模 ==问题: 什么是子字段==\n1. 字段 字段建模过程\n字段类型 是否要搜索及分词 是否需要聚合排序 是否需要额外的存储 Text 和keyword 如何选择\n结构数据\n==对字段的index 设置为false , 则该字段不可被搜索了, 但是还是可以进行聚合查询==\n==关闭_source, 可以解决数据在网络中传输过大的问题==\n对于需要显示的信息, 可以再查询中指定stored_fields 禁止_source 后还是 可以使用hignlights API 2. 关联数据概述 反范式设计, 扁平数据结构\n==Elasticsearch 处理关联关系的方法==\n当存储的数据是对象的时候, 使用bool + must搜索可能会搜索到不需要的结果\n● 存储时，内部对象的边界并没有考虑在内，JSON 格式被处理成扁平式键值对的结构\n● 当对多个字段进⾏查询时，导致了意外的搜索结果\n● 可以⽤ Nested Data Type 解决这个问题\n3. nested data type Nested 数据类型：允许对象数组中的对象被独⽴索引\n● 使⽤ nested 和 properties 关键字，将所有 actors 索引到多个分隔的⽂档\n● 在内部， Nested ⽂档会被保存在两个Lucene ⽂档中，在查询时做 Join 处理\n# 创建 Nested 对象 Mapping #PUT my_movies { \u0026quot;mappings\u0026quot; : { \u0026quot;properties\u0026quot; : { \u0026quot;actors\u0026quot; : { \u0026quot;type\u0026quot;: \u0026quot;nested\u0026quot;, \u0026quot;properties\u0026quot; : { \u0026quot;first_name\u0026quot; : {\u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot;}, \u0026quot;last_name\u0026quot; : {\u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot;} }}, \u0026quot;title\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;text\u0026quot;, \u0026quot;fields\u0026quot; : {\u0026quot;keyword\u0026quot;:{\u0026quot;type\u0026quot;:\u0026quot;keyword\u0026quot;,\u0026quot;ignore_above\u0026quot;:256}} } } } } # Nested 查询 #POST my_movies/_search { \u0026quot;query\u0026quot;: { \u0026quot;bool\u0026quot;: { \u0026quot;must\u0026quot;: [ {\u0026quot;match\u0026quot;: {\u0026quot;title\u0026quot;: \u0026quot;Speed\u0026quot;}}, { // 需要指定关键字 \u0026quot;nested\u0026quot;: { // 嵌套对象 \u0026quot;path\u0026quot;: \u0026quot;actors\u0026quot;, \u0026quot;query\u0026quot;: { \u0026quot;bool\u0026quot;: { \u0026quot;must\u0026quot;: [ {\u0026quot;match\u0026quot;: { \u0026quot;actors.first_name\u0026quot;: \u0026quot;Keanu\u0026quot; }}, {\u0026quot;match\u0026quot;: { \u0026quot;actors.last_name\u0026quot;: \u0026quot;Hopper\u0026quot; }} ] } } } } ] } } } 4. 父子文档 对象和nested 对象的局限性\n每次更新都需要重新索引整个对象 (根对象和嵌套对象) ES 提供了类似关系型数据库中的Join 的实现, 使用Join 数据类型, 可以通过维护Parent / Child 的关系, 从而分离两个对象\n父子文档是两个独立的文档 更新父文档 无需重新索引子文档, 子文档不影响父文档和其他子文档 ==Spring Data Es , Field 使用哪种类型?==\n==定义步骤==\n设置索引的 Mapping\n索引⽗⽂档\n索引⼦⽂档\n按需查询⽂档\n==注意==\n● ⽗⽂档和⼦⽂档必须存在相同的分⽚上\n​\t● 确保查询 join 的性能\n● 当指定⼦⽂档时候，必须指定它的⽗⽂档 Id\n​\t● 使⽤ route 参数来保证，分配到相同的分⽚\n14. query 1. query String 2. simple query String 15. minimum_should_match 参考资料: http://events.jianshu.io/p/84789dd89dcf\nminnum_should_match：当operator参数设置为or时，该参数用来控制匹配的分词的最少数量\n==bool 查询中==\n当默认不传minimum_should_match的情况下，查询分为两个情况\n当bool处在query上下文时，若must或者filter匹配了doc，那么should即使一条都不满足也可以召回doc（如图1.3.1）； 当bool处于filter上下文时，或者bool处于query上下文，但没有must或者filter子句，should至少匹配一个才会召回doc（如图1.3.2）； 参数说明\n最低匹配的个数。也可以传入负数\n传入的参数为百分比, 默认向下取整\n负数表示最多不匹配的百分比50%，向下取整为最多不匹配的个数为1个。即最少匹配个数为2个。 16. updateByQuery\u0026amp;reindex ⼀般在以下⼏种情况时，我们需要重建索引\n索引的 Mappings 发⽣变更：字段类型更改，分词器及字典更新\n索引的 Settings 发⽣变更：索引的主分⽚数发⽣改变\n集群内，集群间需要做数据迁移\nElasticsearch 的内置提供的 API\nUpdate By Query：在现有索引上重建\n在索引更新之前的数据也能被搜索到 Reindex：在其他索引上重建索引\n创建新的正确的索引 重新导入 reindex aip 的其他使用场景\n如果目标索引已经存在数据的话, 需要指定额外配置, 实现对新增数据的写入 支持异步的操作, wait_for_completion=false, 通过返回的taskId 查询是否完成 17. ingest node\u0026amp; painless 1. ingest node ==概述==\n可以对文档进行预处理, Es 提供了一些内置的processor, 也可以通过开发插件实现自己的processor\n2. painless ==简介==\n● Painless ⽀持所有 Java 的数据类型及 Java API ⼦集\n● Painless Script 具备以下特性\n○ ⾼性能 / 安全\n○ ⽀持显示类型或者动态定义类型\n==用途==\n可以对⽂档字段进⾏加⼯处理\n更新或删除字段，处理数据聚合操作\nScript Field：对返回的字段提前进⾏计算\nFunction Score：对⽂档的算分进⾏处理\n在 Ingest Pipeline 中执⾏脚本\n在 Reindex API，Update By Query 时，对数据进⾏处理\n18. 深度分页 参考资料 : https://mp.weixin.qq.com/s/Quoym4438irm4Uexb40asw\n1. 基本分页 from + size 分页方式是 ES 最基本的分页方式，类似于关系型数据库中的 limit 方式。from 参数表示：分页起始位置；size 参数表示：每页获取数据条数\nGET /articles/_search { \u0026quot;query\u0026quot;: { \u0026quot;match_all\u0026quot;: {} }, \u0026quot;from\u0026quot;: 10, \u0026quot;size\u0026quot;: 20 } 1.1 Query 阶段 第一步：Client 发送查询请求到 Server 端，Node1 接收到请求然后创建一个大小为 from + size 的优先级队列用来存放结果，此时 Node1 被称为 coordinating node（协调节点）； 第二步：Node1 将请求广播到涉及的 shard 上，每个 shard 内部执行搜索请求，然后将执行结果存到==自己内部的大小同样为 from+size 的优先级队列==里； 第三步：每个 shard 将暂存的自身优先级队列里的结果==返给 Node1==，Node1 拿到所有 shard 返回的结果后，对==结果进行一次合并，产生一个全局的优先级队列==，存在 Node1 的优先级队列中。（如上图中，Node1 会拿到 (from + size) * 6 条数据，==这些数据只包含 doc 的唯一标识_id 和用于排序的_score==，然后 Node1 会对这些数据合并排序，选择前 from + size 条数据存到优先级队列）； 1.2 Fetch 阶段 方案 1. 使用 search_after 适用场景：需要严格的分页访问，且结果顺序固定（例如基于时间或 ID）。 原理：利用上一页的最后一条记录的 sort 值作为下一页查询的起点。 实现步骤： 在查询中设置 sort 字段（如时间戳或唯一 ID）。 获取第一页数据时，同时记录最后一条记录的 sort 值。 下一次查询时，将 search_after 设置为上一页的 sort 值。 示例： GET /index/_search { \u0026quot;size\u0026quot;: 10, \u0026quot;sort\u0026quot;: [\u0026quot;timestamp\u0026quot;, \u0026quot;id\u0026quot;], \u0026quot;search_after\u0026quot;: [1668740999999, 123] } 优点：性能优于 from+size，内存占用小。 缺点：需要记录每次查询的 search_after 值，不支持直接跳页。 2. 使用 scroll API 适用场景：需要从大量数据中逐页读取，例如批量数据处理。 原理：scroll 会保留一个快照上下文，使得查询结果在多个请求中保持一致。 实现步骤： 初始化查询，设置 scroll 参数。 使用返回的 _scroll_id 获取后续页面。 数据处理完成后，清理上下文（clear_scroll）。 示例： POST /index/_search?scroll=1m { \u0026quot;size\u0026quot;: 100, \u0026quot;query\u0026quot;: { \u0026quot;match_all\u0026quot;: {} } } POST /_search/scroll { \u0026quot;scroll\u0026quot;: \u0026quot;1m\u0026quot;, \u0026quot;scroll_id\u0026quot;: \u0026quot;DXF1ZXJ5QWJvdXRDOjE2OTY3MA==\u0026quot; } 优点 高效处理深度分页。 保证查询结果一致性。 缺点 不适合实时数据查询。 对资源（内存和磁盘）有一定占用。 19. 拼音中文搜索 在Elasticsearch中实现输入拼音也能进行模糊匹配到中文的搜索结果，可以通过以下几个步骤来实现：\n安装拼音分词器：使用elasticsearch-analysis-pinyin插件来实现拼音搜索。这个插件可以帮助Elasticsearch支持拼音搜索功能。 配置索引：在创建索引的时候，需要配置一个自定义的分词器（analyzer），这个分词器会使用拼音分词器来处理文本。例如，可以创建一个名为pinyin_analyzer的分词器，它使用pinyin tokenizer来将中文字符转换成拼音。 字段映射：在字段映射中，可以为需要进行拼音搜索的字段设置两个不同的分词器，一个用于索引时使用，一个用于搜索时使用。例如，可以为字段content设置lc_index作为索引分词器，lc_search作为搜索分词器。 构建查询：在执行搜索时，可以使用multi_match查询，它允许在多个字段上进行搜索，包括原始字段和拼音字段。这样，当用户输入拼音时，Elasticsearch会同时在原始字段和拼音字段上进行搜索。 示例配置：以下是一个配置示例，其中title字段既使用了正常的分词器，也使用了拼音分词器和单字分词器 执行搜索：在执行搜索时，可以指定查询关键词在title、title.pinyin和title.word字段上进行匹配： ","date":"2025-01-03T14:19:32+08:00","permalink":"https://mikeLing-qx.github.io/p/elasticsearch%E8%BF%9B%E9%98%B6%E4%B8%8E%E5%BC%80%E5%8F%91%E5%AE%9E%E5%BD%95/","title":"ElasticSearch进阶与开发实录"},{"content":"0. 接口和继承 接口\n定义契约：接口常用于定义不同类之间的公共行为，确保所有实现类都遵守相同的接口契约。\n多态性：允许通过接口引用来操作不同类的对象。\n继承\n代码复用：继承通过重用父类的功能减少代码重复。\n方法扩展：继承允许子类在父类基础上添加新功能或修改已有功能。\n1. 栈与堆的区别是什么 栈内存（Stack Memory）\n分配与管理：栈内存由==编译器自动分配和释放==，内存空间在==方法执行时自动分配给每个线程==。每个线程都有自己独立的栈内存。 自动分配：Java 栈内存是由 JVM 自动管理的，栈帧的分配和释放由方法调用过程决定。 可能栈溢出：在递归调用过深或方法调用过多时可能导致栈溢出。 可调整栈大小：通过 JVM 参数 -Xss 调整线程栈大小，但仍需要优化代码逻辑以避免不必要的栈占用 存储内容：栈内存主要存储局部变量、方法参数、方法调用信息（如返回地址）等。在==执行一个方法时，相关的数据会压入栈，当方法执行结束后，栈中的数据会自动弹出释放==。溢出会报 ==StackOverflowError== 访问速度：栈内存是==连续的，访问速度很快，但空间相对较小==。 生命周期：局部变量的生命周期随着方法调用而开始和结束，方法执行完毕，栈内存会自动回收。 堆内存（Heap Memory）\n分配与管理：堆内存是用于动态分配的内存，由==程序员通过代码（如 new 关键字）显式分配，内存管理通常由垃圾回收器负责回收==。堆内存是==全局共享的==，也就是说所有线程可以访问堆中的对象。 存储内容：堆内存主要存储对象以及对象内部的属性数据。线程共享堆内存，因此多个线程可以访问和修改堆中的相同对象。 访问速度：由于堆内存是动态分配的，而且不像栈那样是连续的，访问速度相对较慢。 生命周期：堆内存中的对象生命周期是动态的，它们的内存不会因为方法结束而自动释放，而是依赖垃圾回收机制（如 JVM 中的垃圾回收器）进行回收。 ==区别==\n线程独立性：==栈是线程私有的==，堆是线程共享的。 生命周期：栈内存随着方法的执行自动管理，堆内存的管理依赖程序员和垃圾回收机制。 存储数据：栈存储局部变量和方法调用信息，堆存储对象和实例数据。 速度：栈的访问速度快，堆的访问速度慢。 2. this 关键字的作用是什么 this 是一个引用变量，==指向当前对象的实例==。\n区分实例变量和局部变量 调用当前对象的其他方法 在构造函数中调用另一个构造函数 返回当前对象的引用，支持链式调用 作为参数传递当前对象 3. 什么是多线程 多线程：通过空间（cpu的时间片的利用）换取响应时间\n线程和进程的概念\n进程：程序的一次执行，进程之间内存是独立的，无法共享内存空间，至少有一个线程。\n线程：进程内部的最小执行单元，线程之间是共享堆内存，栈内存是独立的。\n创建线程的方式\n继承Thread类，重写run方法\n实现Runnable接口，实现里面的run方法\n匿名内部类实现Runnable接口，New Thread(()-\u0026gt;执行内容)\n线程的执行：==一定要用Start方法执行线程，如果run方法执行是直接执行类的方法，不会以线程的方式执行==\n用户线程和守护线程：\n用户线程：一般是用户创建的，不会随着主线程的终止而终止\n守护线程：一般是系统创建的，会随着主线的终止而终止，垃圾回收线程就是守护线程，可以使用\nThread::setDaemon方法将用户线程转化为守护线程\n4. synchronized 线程安全：当多线程执行同一段程序的时候，如果发生了和预期结果不一致的情况，就是线程不安全\n的，如果和预期结果一致就是线程安全的，可以加锁解决(把并行运行的线程变成串行化执行)。\n同步锁的几种方式(锁对象):\n同步代码块加锁：sync\u0026hellip;.(obj) 同步方法加锁：等价于sync\u0026hellip;.(this) 静态同步方法加锁:等价于sync\u0026hellip;.(this.getClass()) ==死锁：线程之间互相等待对方释放锁，就产生了死锁，尽量不要同步中嵌套同步==。\n死锁发生的原因：\n线程 1 先获取了资源 A 的锁，准备获取资源 B 的锁。 线程 2 同时获取了资源 B 的锁，准备获取资源 A 的锁。 线程 1 等待 线程 2 释放资源 B，而 线程 2 正在等待 线程 1 释放资源 A。 由于==两个线程互相等待对方释放资源==，进入了死锁状态，程序无法继续执行。 互斥条件, 共享资源 X 和Y只能被一个线程占用 请求和保持, 线程t1 已经获取 资源X, 在等待共享资源Y的时候不释放 X 不可抢占, 也就是其他线程不能强行占有线程T1 占有的子牙UN 循环等待, 两个线程相互等待对方释放资源 预防死锁\n一次性请求所有资源 如果占用资源的线程申请资源申请不到, 那么可以主动的去释放本身持有的资源 保持统一的获取锁的顺序, 比如都是先申请A -\u0026gt; B -\u0026gt; C 伪码 // 定义两个资源 resource A resource B // 线程 1 的执行过程 Thread 1: lock(A) // 获取资源 A 的锁 wait(100) // 等待一段时间，让线程 2 有机会获取资源 B lock(B) // 尝试获取资源 B 的锁 // 操作资源 A 和 B unlock(B) // 释放资源 B 的锁 unlock(A) // 释放资源 A 的锁 // 线程 2 的执行过程 Thread 2: lock(B) // 获取资源 B 的锁 wait(100) // 等待一段时间，让线程 1 有机会获取资源 A lock(A) // 尝试获取资源 A 的锁 // 操作资源 A 和 B unlock(A) // 释放资源 A 的锁 unlock(B) // 释放资源 B 的锁 5. 线程有哪些状态 new, run, blocked, wait, time-wait, terminate\n==NEW==(新建)\n线程刚被创建，但是并未启动。\n==RUNNABLE==(可运行)\n线程可以在java虚拟机中运行的状态，可能正在运行自己代码，也可能没有，这取决于操作系统处理器。\n==BLOCKED==(锁阻塞)\n当一个线程试图获取一个对象锁，而==该对象锁被其他的线程持有，则该线程进入Blocked状态==；当该线程持有锁 时，该线程将变成Runnable状态。\n==WAITING==(无限等待)\n一个线程在等待另一个线程执行一个（唤醒）动作时，该线程进入Waiting状态。进入这个状态后是不能自动唤\n醒的，==必须等待另一个线程调用notify或者notifyAll方法才能够唤醒==。\n==TIMED_WAITING==(计时等待)\n同waiting状态，有几个方法有超时参数，调用他们将进入Timed Waiting状态。这一状态将一直保持到超时期\n满或者接收到唤醒通知。带有超时参数的常用方法有Thread.sleep 、Object.wait。\n==TERMINATED==(被终止)\n因为run方法正常退出而死亡，或者因为没有捕获的异常终止了run方法而死亡。\n6. wait 和 sleep的区别 sleep()方法，属于Thread类。wait()方法，属于Object类。\nsleep()方法导致了==程序暂停执行指定的时间，让出cpu调度其他线程==，不会释放锁或监视器资源，==当指定的时间到了又会自动恢复运行状态==。\nwait() ==必须在同步代码块或同步方法中调用== , 是把控制权交出去，==线程会释放持有的锁== 直到收到其他线程的通知（==通过同一个对象的== notify() 或 notifyAll() , 通常用于线程间的协作。\n7. 线程interrupt() interrupt() 方法的作用\n中断标志：调用 interrupt() 后，线程的中断状态将被设置为 true。这仅仅是发送了一个中断信号，线程并不会立即停止运行。 配合阻塞操作：如果线程处于阻塞状态（例如 sleep()、wait() 或 join()），那么线程会立即抛出 InterruptedException 异常，并清除中断状态，这样可以让线程提前结束阻塞操作。 检查中断状态：线程可以通过 Thread.interrupted() 或 isInterrupted() 来检查是否有中断请求。 调用interrupt()方法后，线程不会自动停止，但是它会开始尝试响应中断请求。线程是否停止，以及何时停止，取决于线程如何响应中断信号。正确的做法是，在线程的运行逻辑中，定期检查中断状态，或者在可能长时间运行的操作中使用会抛出InterruptedException的方法，以便能够响应中断请求。\n8. 说一下线程的优先级 设置并理解线程的优先级priority , 优先级越高,获取到cpu的时间片越多\n理解并掌握join()方法的使用 : thread.Join把指定的线程加入到当前线程，可以==将两个交替执行的线 程合并为顺序执行的线程==。比如在线程B中调用了线程A的Join()方法，直到线程A执行完毕后，才会继续 执行线程B。\n理解yield()方法的概念 : 暂停当前正在执行的线程，并执行其他线程。（可能没有效果）, 目的是让具有相同优先级的线程之间能够适当的轮换执行\n9. 线程安全的三大特性 有序性: 由jvm指令重排导致的，指令重排的结果对于单线程来说是一致性，没有什么影响（在指令重排的过程中会考虑数据依赖的问题），多线程情况下避免指令重排带来程序执行错误的危害。==有些情 况可以使用volatile，或者使用synchronized==同步块或者同步方法或者同步锁。 可见性: 和java内存模型有关，==多线程访问共享变量时会创建副本变量，修改后再写回主变量，其他线程副本变量的值没有及时更新==，导致可见性的问题 原子性: 不可分割，说明这一段程序要么都执行，要么不执行，并且不能被其他线程影响, 可以使用原子类AtomicXXX 10. java内存 JVM内存结构、Java对象模型和Java内存模型，这就是三个截然不同的概念\n1. JVM内存结构有哪些 PC寄存器 java 虚拟机栈 本地方法栈 java堆 方法区 内包含了运行时常量池 所有线程共享的数据区域 各个线程独享的数据区域 运行时常量 ==JVM内存结构，由Java虚拟机规范定义是Java程序执行过程中，由JVM管理的不同数据区 域。各个区域有其特定的功能。==\n2. java对象模型 HotSpot 虚拟机设计了一个OOP-Klass Model,OOP（Ordinary Object Pointer）指的是普通对象指针, Klass用来 描述对象实例的具体类型\n每一个Java类，在被JVM加载的时候，JVM会给这个类创建一个 instanceKlass 对象，保存在方法 区，用来在JVM层表示该Java类。当我们在Java代码中，\n使用new创建一个对象的时候，JVM会创建一 个 instanceOopDesc 对象，这个对象中包含了对象头以及实例数据。\n1. 堆内存中的实例 对象头（Object Header）： 每个 Java 对象都有一个对象头，用于存储与对象相关的元数据。对象头包含以下信息： Mark Word：==用于存储对象的哈希码、GC 状态、锁状态等信息==。它占用 32 位或 64 位，具体取决于 JVM 和系统架构。 Class Pointer（类型指针）：指向对象的类型信息，即该对象所属的类。==这部分告诉 JVM 该对象属于哪个类==，从而可以访问==类的元数据==（如方法表、字段布局等）。 数组长度：如果对象是数组，Java 对象头还会包含一个额外的字段，用来记录数组的长度。 实例数据（Instance Data）： 实例数据部分包含==对象的所有实例字段==。不同类型的字段（如 int、long、boolean 等）根据它们的字节大小被排列存储。JVM 可能会对字段进行内存对齐，以提高访问效率。 引用类型字段==存储的是对象的引用（指针），而非实际的数据对象。引用指向堆内存中的另一个对象==。 对齐填充（Padding）： Java 对象的内存布局通常要求对象的大小是 8 字节的倍数，因此在对象的末尾可能会有额外的填充字节来满足这一要求。这些填充字节不会存储任何有用的数据，只是为了满足内存对齐的要求。 ​\t==伪共享==是指当多个线程在共享内存区域中操作不同的数据时，==这些数据在缓存中恰好落在同一个缓存行中== , ==由于多个线程频繁导致缓存行失效，缓存失效带来的内存重新加载会显著降低程序的并发性能==。即使每个线程修改的是不相关的数据，这种不必要的同步机制使 CPU 和内存的性能浪费在无效的缓存操作上。\n3. 介绍一下java方法区 在Java内存模型中，方法区（Method Area）是JVM规范中定义的一个内存区域，它用于存储以下内容：\n类信息：==包括类的名称、修饰符、字段、方法、接口等==。 常量池：存储编译期生成的各种字面量和符号引用。 静态变量：==类级别的静态变量==。 即时编译器编译后的代码：==JIT编译器编译后的机器码==。 其他：可能还包括JVM内部的一些其他信息，如对类的锁定、同步等。 方法区是所有线程共享的内存区域，它在JVM启动时创建。在Java 8之前，方法区通常位于永久代（PermGen），但在Java 8及以后的版本中，方法区被移到了元空间（Metaspace），元空间使用的是本地内存。\n需要注意的是，方法区并不是堆内存的一部分，它与Java堆（Heap）是两个不同的内存区域。堆内存主要用于存储对象实例和数组。\n4. java内存模型 JMM Java内存模型就是一种==符合内存模型规范的==，屏蔽了各种硬件和操作系统的访问差异的，保证了Java 程序在各种平台下对内存的访问都能保证效果一致的机制及规范\n==和多线程相关的，他描述了一组规则或规范==，这个规范定义了一个线程对共享变量的 写入时对另一个线程是可见的。\n==第一条关于线程与主内存：线程对共享变量的所有操作都必须在自己的工作内存（本地内存）中进行， 不能直接从主内存中读写==\n==第二条关于线程间本地内存：不同线程之间无法直接访问其他线程本地内存中的变量，线程间变量值的 传递需要经过主内存来完成==\n11. volatile 保证了==可见性和有序性==\n==缓存一致性==: 一个变量如果用volatile修饰, 保证了可见性 将当前处理器==缓存行的数据会写回到系统内存==。 这个写回内存的操作会引起在==其他CPU里缓存了该内存地址的数据无效==。 ==内存屏障防止指令重排==（解决指令重排对volatile修饰的变量不会产生影响） 适用场景\na. 对变量的写入操作不依赖其当前值\n​ 不满足：number++、count=count*5等\n==满足：boolean变量、直接赋值的变量等== b. 该变量没有包含在具有其他变量的不变式中 不满足：不变式 low\u0026lt;up\n12. 原子性 count++并不是原子性操作\ncount = 5 开始，流程分析：\n线程1读取count的值为5 ; 线程2读取count的值为5\n线程2加1操作\n线程2最新count的值为6\n线程2写入值到主内存的最新值为6\n线程1的count为5，线程2的count为6 如果切换到线程1执行，那么线程1得到的结果是6, 写入到主内存的值还是6\n解决方案\n使用==synchronized==\n使用==ReentrantLock==（可重入锁）\n使用==AtomicInteger==（原子操作）\nJava中的原子操作包括： 1）除long和double之外的基本类型的赋值操作 2）所有引用reference的赋值操作 3）java.util.concurrent.Atomic.* 包中所有类的一切操作。 4）cas操作是原子操作。 13. synchronized 同一时刻==只有一个线程执行synchronized声明的 代码块==。还可以保证共享变量的内存可见性。同一时刻只有一个线程执行，这部分代码块的==重排序也不 会影响其执行结果==。也就是说使用了synchronized可以保证并发的原子性，可见性，有序性\n1. 可见性怎么解决的? 线程加锁时（进入同步代码块时）：将==清空本地内存中共享变量的值==，从而使用共享变量时需要从==主内 存中重新读取最新的值==（加锁与解锁是同一把锁）\n==执行过程==\n获得互斥锁（同步获取锁）\n清空本地内存\n执行代码\n将更改后的共享变量的值刷新到主内存\n释放互斥锁\n2. 同步原理 普通同步方法，==锁是当前实例对象this==\n静态同步方法，==锁是当前类的class对象==\n同步方法块，==锁是括号里面的对象==\n==同步操作主要是monitorenter和monitorexit这两个jvm指令实现==\nsynchronized是通过访问锁对象的monitor和mark word实现同步的\n1. markword 什么是? 于存储对象自身的运行时数据，==存储对象状态、锁信息等的结构，在同步、垃圾回收等机制中起到重要作用==。\n哈希码（HashCode）：对象的哈希值，通常通过 hashCode() 方法生成。\n锁状态：用来支持对象的同步机制。当一个对象被锁住时，Mark Word 中会记录这个锁的状态，比如轻量级锁、重量级锁等。\n垃圾回收标记：在垃圾回收（GC）过程中，Mark Word 也可以存储一些 GC 相关的信息，比如对象是否已经被标记为可回收。\n偏向锁：在某些情况下，Mark Word 可以记录对象的偏向线程信息，以优化线程同步性能\n2. monitor 是什么? ==每一个对象都有一个隐式的Monitor, 可以使用 synchronized 关键字进行同步的原因==\n互斥锁：确保同一时刻只有一个线程可以访问被锁定的代码块，避免并发线程之间的冲突。\n线程通信：通过 wait() 和 notify() 等方法，Monitor 也可以用来管理线程之间的协调与通信。这是因为一个对象的 Monitor 还维护了一个等待队列，线程可以通过 wait() 进入等待队列，然后被其他线程通过 notify() 唤醒。\nOwner：初始时为NULL表示当前没有任何线程拥有该monitor record，==当线程成功拥有该锁后保 存线程唯一标识==，当锁被释放时又设置为NULL；\nEntryQ:关联一个系统互斥锁（semaphore），==阻塞所有试图锁住monitor record失败的线程==。\nRcThis:表示blocked或waiting在该monitor record上的所有线程的个数。\nNest:用来实现重入锁的计数。\nHashCode:保存从对象头拷贝过来的HashCode值（可能还包含GC age）。\nCandidate:用来避免不必要的阻塞或等待线程唤醒，因为每一次只有一个线程能够成功拥有锁， 如果每次前一个释放锁的线程唤醒所有正在阻塞或等待的线程，会引起不必要的上下文切换（从阻 塞到就绪然后因为竞争锁失败又被阻塞）从而导致性能严重下降。Candidate只有两种可能的值\n==0 表示没有需要唤醒的线程== ==1 表示要唤醒一个继任线程来竞争锁==。 3. 锁优化有哪些? ==偏向锁==: 如果一个线程获得了锁，那么锁就进 入偏向模式，此时Mark Word 的结构也变为偏向锁结构\n==轻量锁==:\n当线程进入同步代码块时，JVM 会检查当前对象的 Mark Word（对象头中的一部分）。如果Mark Word 处于无锁状态，==JVM 会创建一个用于记录锁状态的锁记录（Lock Record），保存在当前线程的栈中==。 然后 JVM 尝试使用CAS（Compare-And-Swap）操作将对象的 ==Mark Word 拷贝到线程的锁记录中，并将对象的 Mark Word 更新为指向这个锁记录的指针==。此时对象处于轻量级锁定状态。 ==自旋锁==: 自旋锁，就是让该线程等待一段时间，不会被立即挂起，看持有锁的线程是否会很快释放锁。怎\n么等待呢？==执行一段无意义的循环==即可（自旋）。 自旋等待不能替代阻塞，虽然它可以==避免线程切换带来的开销，但是它占用了处理器的时间==。如果持 有锁的线程很快就释放了锁，那么自旋的效率就非常好，反之，自旋的线程就会白白消耗掉处理的资 源\n==适应自旋锁==: 它是由==前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定==。它怎么做呢？线程如果自旋成 功了，那么下次自旋的次数会更加多，因为虚拟机认为既然上次成功了，那么此次自旋也很有可能会再\n次成功，那么它就会允许自旋等待持续的次数更多\n==锁消除==: JVM检测到不可能存在共享数据竞争，这是JVM会对这些同步锁进行锁消除。锁消除的依据是逃逸分析 的数据支持 例如: JDK的内置API时，如StringBuffffer、Vector、HashTable等，这个时 候会存在隐形的加锁操作\npublic void test(){ Vector\u0026lt;Integer\u0026gt; vector = new Vector\u0026lt;Integer\u0026gt;(); for(int i = 0 ; i \u0026lt; 10 ; i++){ vector.add(i); } System.out.println(vector); } 在运行这段代码时，JVM可以明显检测到变量vector没有逃逸出方法vectorTest()之外，所以JVM可以\n大胆地将vector内部的加锁操作消除\n==锁粗化==: 将多个连续的加锁、解锁操作连接在一起，扩展成一个范围更大的锁\n==重量锁==: 加锁解锁会导致线程从用户态到核心态的切换，消耗比较大的资源。\n14. CAS 是什么 CAS在 先比较后修改 : 通过三个参数 当前值v 旧的预期值o 要更新的值n，只有v==o的时候才能更新值o\n这个CAS过程中，根本没有获取锁，释放锁的操作，==Unsafe 是CAS的核心类，它提供了硬件级别的原子操作==,\n缺陷\n自旋, 循环时间长, 一直不成功 只能保证一个共享变量 ABA 问题, 可以通过加AtomicStampedReference, 版本号进行解决 15. native ==native 关键字用于声明本地方法，使得 Java 程序可以调用用其他语言（如 C/C++）实现的代码==\nnative关键词 1. javac生成.class文件，比如javac NativePeer.java 2. javah生成.h文件，比如javah NativePeer 3. 编写c语言文件，在其中include进上一步生成的.h文件，然后实现其中声明而未实现的函数 4. 生成dll共享库，然后Java程序load库，调用即可 native可以和任何除abstract外的关键字连用，这也说明了这些方法是有实体的，并且能够和其他\nJava方法一样，拥有各种Java的特性。\n16. Atomic .atomic包介绍\n里面放了一些原子操作类，我们可以使用这些api对共享变量进行并发操作，底层都是使用的cas，\n性能非常高\n基本类型\nATInteger，ATLong,ATBoolean\nget在前就是获取修改前的值，get在后就是获取修改后的值\n引用类型\nATRef\u0026hellip;：对Object的原子操作\nATS..Ref:给Object加版本号，每次修改都比较版本号和Object，可以有多个版本（int）\nATmak\u0026hellip;Ref:给Object加版本号，每次修改都比较版本号和Object，只能有两个版本号\n（boolean）\n数组类型\nATIntegerArr\u0026hellip;:Integer类型的数组原子类\nATLongArr..:Long类型的数组原子类\n前两个和基本类型使用方式差不多，多了个参数（下标）\nATRef\u0026hellip;Arr:Object类型的数组\n和引用类型使用方式差不多，多了个参数（下标）\n对象的属性修改类型（修改volidate修饰的属性）\n==AtomicIntegerFieldUpdater:原子更新整形字段的更新器==\n==AtomicLongFieldUpdater：原子更新长整形字段的更新器==\n==AtomicReferenceFieldUpdater ：原子更新引用类形字段的更新器==\n通过这些类对要修改的javabean进行包装，能够原子性的修改javabean的属性\n使用修改器要注意的点：\n要修改的类和属性不能static，负责拿不到内存地址\n属性必须是外部可直接访问的，public修饰\n属性不能是fifinal类型，因为fifinal不能被修改\n==属性必须被volidate修饰==，因为cas只能保证原子性，要通过volidate保证有序性和可见性，\n才能保证线程安全。\njdk1.8新增类（LongAddr\u0026hellip;）\n对比ATLong性能更高，底层是使用多个变量对同一个变量进行分割，解决了同一个变量操作并发\n大时，等待较长时间消耗系统cpu资源较高的问题\n17. AQS 介绍一下 ==AbstractQueuedSynchronizer== 它只是一个抽象类，但是==JUC中的很多组件都是基于这个抽象类==， 也可以说这个AQS是多数JUC组件的基础\n提供了两种锁的机制 排他锁 - RenntrantLock 共享锁 - 读锁, 读写互斥, 写互斥 Countdownlatch, Semaphore 定义了 获取锁 是通过CAS 修改 State, 大于0说明有线程获取到资源, 等于0释放了资源 未竞争到锁的线程, 会加入到双向链表中 当资源被释放的时候, 公平锁需要去判断链表中, 是否有等待的线程, 有则需要排队等待 三个核心问题 互斥变量设计, 存在竞争的时候如何保证变量更新的安全性 未竞争到锁资源的线程的等待, 已经竞争到锁资源的线程的唤醒 锁竞争的公平和非公平 1. state , head ,tail state: AQS维护了一个volatile int类型的变量state表示当前同步状态。==当state\u0026gt;0时表示当前已有线程获取 到了资源，当state = 0时表示释放了资源==。\nhead: 是队列的第一个节点，==表示当前正在持有锁或资源的线程==\ntail: 队列的最后一个节点，==表示最新添加到队列中的线程==\n2. 资源获取方式 就是通过cas 去修改state 状态; 改成功就获取到了 资源 ; 没修改成功就没有获取到 资源)\nExclusive（独占，只有一个线程能执行，如ReentrantLock） Share（共享，多个线程可同时执行，如Semaphore/CountDownLatch） 获取同步状态失败时，AQS则会将==当前线程已经等待状态等信息构造成一个节点==（Node）并将其加入到CLH同步队列，同时会阻塞当前线程，当同步状态释放时，会把首节点唤醒 （公平锁)\n3. 总结 知道什么是AQS\n==框架中制定了一个基本流程, 里面的state具体的获取方式 需要它的实现类去实现==，==这些实现类都是不同的锁里面都有一个内部类对应，state表示资源，如 果获取不到资源就将当前线程加入队列，通过自旋的方式重复尝试获取资源==。\n理解AQS工作原理\n==FIFO双向队列==，先进先出，node对应的是被阻塞的线程，==head，tail，这两个变量的操作包括入列操作都是cas原子操作==。但是==出列并不是cas==，因为==出列过程涉及多个步骤和指针的修改==, 独享方式下只有一个线程获取到了state状态\n18. 锁的类型有哪些 加锁会导致性能下降的原因\n上下文切换 内存指令的执行 线程的阻塞和唤醒 并行执行变为了串行执行 ==首先编译器会对我们的代码做优化, 会有锁粗化和锁消除==\n互斥锁: 保证在任一时刻，只能有一个线程访问该对象\n阻塞锁: 可以说是==让线程进入阻塞状态进行等待==，当获得相应的信号（唤醒，时间） 时，才可以进入\n线程的准备就绪状态，准备就绪状态的所有线程，通过竞争，进入运行状态\n自旋锁: ==为了线程在阻塞之前就能获取到锁==, 自旋锁是采用让当前线程不停地的在循环体内执行实现的，当循环的条件被其他线程改变时，才能进入。\n读写锁: 读写锁实际是一种特殊的自旋锁，它把对共享资源的访问者划分成读者和写者，读者只对共享资源进\n行读访问，写者则需要对共享资源进行写操作。 ==写者是排他性的，一个读写锁同时只能有一个写者或 多个读者（与CPU数相关），但不能同时既有读者又有写者==。\n公平锁: ==当一个线程竞争锁的时候，队列中如果有线程按照顺序获取锁==\n锁升级的过程\n无锁 -\u0026gt; 偏向锁 -\u0026gt; 轻量级锁(自旋锁) -\u0026gt; 重量级锁\n偏向锁: ==没有锁竞争或竞争非常少的场景==, 通过记录第一个获取锁的线程Id, ==假设锁的持有者不会改变==, 后续==同一个线程再次尝试获取锁, 无需进行真正的加锁操作==, 因为 锁偏向该线程, 这减少了锁的获取和释放的开销;\n19. ReentrantLock 可重入锁，是一种递归无阻塞的同步机制, ==是基于AQS的== 。等同于 synchronized 的使用\n公平锁: 原则就是是否遵循 FIFO 先进先出, 公平锁, ==在获取锁的时候判断队列中是否有线程在当前线程的前面== 非公平锁 内部实现原理：通过==尝试修改state从0-1获取锁，修改成功表示当前线程获取到锁了==，修改失败表示没有获取到，同一个线程获取到了锁可以重入，==重入的过程就是累计state的过程==，释放锁就是state减到0的过程，==减到0了就说明锁释放完成了==，唤醒后继节点继续执行。\nlock与synchronized 相比的对比\n都是java 用来解决线程安全问题的工具, Lock 是JUC 里的接口, sync 是同步关键字\n拓展性更强, 代码更加灵活 , 提供了条件Condition ,例如 有 tryLock 非阻塞的获取锁的方式, 已经可以实现公平和非公平锁\n注意 ==ReentrantLock 锁的释放一定要在 finally 中进行释放==\n性能差不多\nsynchronized 锁优化过程 无锁, 偏向, 轻量, 重量 lock 有自旋锁 20. ReentrantReadWriteLock ==读操作不互斥，写操作互斥，读和写互斥==\n公平性：支持公平性和非公平性。\n重入性：支持重入。\n锁降级：==写锁能够降级成为读锁，遵循获取写锁、获取读锁再释放写锁的次序。读锁不能升级为写锁==。 ==避免了释放写锁后立即被其他写线程抢占，让当前线程还能继续持有读锁，允许自己和其他读线程并发读取==\n获取写锁 执行写操作 获取读锁（此时仍然持有写锁） 释放写锁（降级，保持读锁） 执行读操作 释放读锁 内部维护了一对锁, 写锁和读锁\n写锁之间是互斥的，读锁之间不互斥，读锁和写锁之间是互斥的\n锁降级：一个线程获取到了同一把锁的写锁之后还能够获取这个对象的读锁。（锁并不是 真的降级了） 锁不能够升级的\n内部原理：\n使用state按位分割，高16位表示读，低16位表示写。 获取写锁的过程就是修改低16位的过程，也表示写的重入次数 获取读锁的过程就是修改高16位的过程，也表示所有获取到读锁的线程的重入总数 每个线程自己的读锁的重入次数保存在ThreadLocal\u0026lt;\u0026gt;里面 不管是写锁还是读锁，重入次数不能超过2^16-1\n21. Condition Condition 对象是和 ReentrantLock 绑定的 , 相对于 Synchronized 控制同步中 Object的wait()、notify()系列方法, 提供了 await() , signal() , signalAll()\n22. CyclicBarrier 同步屏障 ==就是一道门, 堵住了线程执行的道路，直到所有线程都就位，门才打开，让所有线程一起通过==\n内部使用的是 ReentrantLock 和 Condition\n两个构造方法\nCyclicBarrier(int parties) CyclicBarrier(int parties, Runnable barrierAction) ==并在所有线程到达屏障时执行给定的操作, 由最后一 个进入屏障的线程执行== await()方法的逻辑：如果该线程不是到达的最后一个线程，则他会一直处于等待状态，除非发生以下 情况：\n最后一个线程到达，即count == 0\n超出了指定时间（超时等待）\n其他的某个线程中断当前线程\n其他的某个线程中断另一个等待的线程\n其他的某个线程在等待屏障超时\n其他的某个线程在此屏障调用reset()方法。reset()方法用于将屏障重置为初始状态\n23. CountdownLatch 是基于内部的Sync , 而Sync 集成 AQS\n==一个或者多个线程，等待其他多个线程完成某件事情之后才能执行==, 使用是一次性的, 无法被重置,\n起点运动员应该等其他起点运动员准备好才可以起跑（CyclicBarrier）。\n接力运动员不需要关心其他人，只需和自己有关的起点运动员到接力点即可开跑 (CountDownLatch）。\n24. Semaphore ==控制访问多个共享资源的计数器==, Semaphore 通过维护一个许可（permit）计数来管理资源的并发访问，通常用于限制能同时访问某个资源的线程数量。\n==Semaphore维护了一个信号量许可集。线程可以获取信号量的许可；当信号量中有可用的许可时， 线程能获取该许可；否则线程必须等待，直到有可用的许可为止。 线程可以释放它所持有的信号量许 可，被释放的许可归还到许可集中，可以被其他线程再次获取==。\n类似于停车场, 有5个车位, 满了只能等,有出去的才能进来新的\n出Semaphore内部包含公平锁（FairSync）和非公平锁（NonfairSync），继承内部类 Sync，其中Sync继承AQS\nSemaphore(int permits) ：创建具有==给定的许可数和非公平的 Semaphore==。\nSemaphore(int permits, boolean fair) ：创建具有给定的许可数和给定的公平设置的 Semaphore\n25. CLH ==自旋锁队列，常用于实现 AQS 中的等待队列==\n队列结构：\nCLH 队列是一个链表结构，每个节点代表一个线程。队列的头节点表示当前持有锁的线程，尾节点表示最后一个请求锁的线程。 每个节点都有一个指向前驱节点的引用，这使得后继线程能够了解前驱线程的状态。 公平性：\nCLH 队列天然支持公平性，后来的线程会在队列的尾部等待，从而避免了饥饿现象。 自旋等待：\n当线程请求锁时，如果锁被占用，线程会在队列中自旋，等待前驱节点释放锁。自旋方式减少了上下文切换的开销。 26. HashMap HashMap\njdk7 数据结构：==数组+单项链表== 构造方法：public HashMap(int initialCapacity, float loadFactor) initialCapacity：初始化容量，容量必定是一个2^n,如果initialCapacity不是2^n，找大于initialCapacity的最近的一个2^n loadFactor：负载因子，默认0.75 threshold：阈值 = loadFactor*initialCapacity，当数组中的元素数量超过阈值就会触发扩容，扩容原来的两倍 方法 put过程\n对key键进行hash运算获取结果value 使用value和数组的长度进行\u0026amp;运算，得到下标位置 到对应下标位置上查看 如果为空，就直接转化为Entry插入 如果不为空，判断key是否一致，如果key一致就覆盖，如果不一致顺着链表往下面找 如果找到了，就覆盖，如果一直找不到就放到最后一个 当元素个数超过阈值，先扩容再插入（在多线程环境下可能造成死循环） ==多个线程同时操作链表，进行节点迁移时可能会形成环形链表（循环引用== get过程\n对key键进行hash运算获取结果value 使用value和数组的长度进行\u0026amp;运算，得到下标位置 到对应下标位置上查看 如果为空，就返回null 如果不为空，判断key是否一致，如果key一致就返回对应的value，如果不一致顺着链表往下面找 如果找到了就返回，如果一直找不到就返回null jdk8 数据结构：==数组+单向链表+红黑树== 构造方法和jdk7一样 方法 put过程 (先插入再扩容) 对key键进行hash运算获取结果value 使用value和数组的长度进行\u0026amp;运算，得到下标位置 到对应下标位置上查看 如果为空，就直接转化为Entry插入 如果不为空，判断key是否一致，如果key一致就覆盖，如果不一致顺着链表往下面找 如果找到了，就覆盖，如果一直找不到就放到最后一个 如果链表长度超过8个就转化为红黑树 如果该节点为树节点 比较根节点和当前节点的hash值 ==如果当前\u0026lt;根，往左边找，如果\u0026gt;往右边找== 如果找到对应位置，如果key一样就覆盖 key不一样就插入 ==如果发生了hash碰撞，key不一样但是hash值一样== 就通过再hash法，通过其他函数重新hash，==如果小于等于，放左边== ==重新调整树平衡== 当元素个数超过阈值，==先插入再扩容==（==避免在多线程环境下可能造成死循环==） get过程 对key键进行hash运算获取结果value 使用value和数组的长度进行\u0026amp;运算，得到下标位置 到对应下标位置上查看 如果该节点为树节点 比较根节点和当前节点的hash值 如果当前\u0026lt;根，往左边找，如果\u0026gt;往右边找 如果找到对应位置，如果key一样就返回value key不一样就继续找，如果一直找不到就返回null 如果是链表就顺着链表结构往下遍历寻找 ConcurrentHashMap\njdk7\n数据结构 sengment数组+数组+单向链表 sengment继承Reentrent，具备加锁能力 构造方法 public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) initialCapacity 代表当前map总的容量大小，在初始化的时候分配给segment中的hashEntry loadFactor 负载因子，用来对segment中的hashEntry进行扩容 concurrencyLevel 并发数就是segment数量，初始化之后就不能扩容了。 jdk8\n数据结构\n数组+链表+红黑树 保证线程安全\n==cas+synchronized==：==在尽可能小的粒度上加锁==，能使用cas就使用cas，不能使用就使用synchronized\n无锁并发（CAS）：通过 CAS 操作实现部分无锁的更新操作，减少锁的竞争。\n局部加锁：在需要加锁时，==仅锁定某个桶bucket==，避免影响其他桶的并发操作。\n红黑树优化：冲突严重时，将链表转换为红黑树，减少冲突带来的性能影响。\n弱一致性迭代器：在并发操作中允许遍历继续进行，保证遍历时的安全性。\n渐进式扩容：避免扩容时的全局锁定，通过多个线程并发扩容，保证扩容过程中的操作效率\n==只对写操作加锁，读操作不加锁==\nConcurrentHashMap 确保对 Map 本身的操作是线程安全的，但==复合操作或涉及多步骤的业务逻辑可能仍然需要加锁来保证原子性==。\n对于单个操作，如 put(), get(), remove() 等，不需要加锁，但如果你的==业务逻辑涉及多个操作的组合，或者需要跨多个共享资源操作时，依然需要加锁来确保线程安全==。\n可以用 ==map.computeIfAbsent(key, k -\u0026gt; newValue)==; 来避免显示的加锁\n对比HashTable\nConcurrentHashMap锁粒度更小 可以一边遍历一边修改map ConcurrentSkipListMap\n数据结构：多层单向链表, ==因为是顺序的,所以建多个索引, 在索引上再建索引, 可以调表== 特点： 上一层的元素是下一层的索引 上一层的元素，下一层一定有 上一层的个数比下一层要少 第一个元素在所有层都存在 put 抛取随机值 如果值大于当前层数 就新创建一层 将当前元素插入到所有层 如果小于当前层数 就将当前元素保存到对应层，并且下面依次保存 get 从最上层开始找 比较大小，如果大于就往右边继续找 如果小于，就进入下一层往右边找 27. ConcurrentSkipListMap 线程安全：所有操作都是线程安全的，多个线程可以同时对 ConcurrentSkipListMap 进行读写操作，而不会导致数据不一致或竞争问题。\n有序性：与 TreeMap 类似,ConcurrentSkipListMap 会保持键的有序性。它支持键按自然顺序或自定义的比较规则排序。\n无锁机制：大部分读操作不需要加锁，而是通过 CAS（Compare-And-Swap） 操作保证线程安全，这提升了并发性能。\n跳表结构：使用跳表而非红黑树来管理有序的数据。跳表通过在链表上增加多层 \u0026ldquo;跳跃\u0026rdquo; 指针，提供了接近二分查找的性能。相比树形结构，跳表的实现更简单，也更适合高并发场景\n28. JUC 队列 queue 阻塞与否 是否有界 线程安全保障 适用场景 注意事项 ConcurrentLinkedQueue 非阻塞 无界 CAS 对全局的集合进行操作的场景 size() 是要遍历一遍集合，慎用 ArrayBlockingQueue 阻塞 有界 一把全局锁 生产消费模型，平衡两边处理速度 \u0026ndash; LinkedBlockingQueue 阻塞 可配置 存取采用2把锁 生产消费模型，平衡两边处理速度 无界的时候注意内存溢出问题 PriorityBlockingQueue 阻塞 无界 一把全局锁 支持优先级排序 SynchronousQueue 阻塞 无界 CAS 不存储元素的阻塞队列 非阻塞队列\nConcurrentLinkedQueue ==一个基于链接节点的无边界的线程安全队列，遵循队列的FIFO原则，队尾入队，队首出队。采用CAS算法来实现的== 数据结构：单向链表 非阻塞 线程安全 特点 ==不会记录元素个数，size方法需要遍历整个队列，非常消耗资源== 在某些场景下可以代替vector的使用 ==业务操作上还是需要自己加锁==, 例如: queue.isEmpty()后再进行队列操作queue.add()是不能保证安全的 阻塞队列\nblockingQueue接口\n提供4组api\n阻塞api put：放，==当队列放满了，放不进去线程被阻塞== take：取，==当队列为空，取不出来，线程被阻塞== 子类\nArrayBlockingQueue\n总结: ==是一种线程安全的、基于数组的有界阻塞队列。适合生产者-消费者模型== 有界性：==有界, 固定大小==，必须初始化的时候设置边界 构造方法 ArrayBlockingQueue(int capacity, boolean fair) capacity：初始化容量 fair：公平策略 ==内部使用一把锁, 生产者消费者用的同一个ReetrantLock，两个condition, notFull和notEmpty== notFull ：控制生产者线程, 如果满了就阻塞 notEmpty : 控制消费者线程, 如果是空的就阻塞 LinkedBlockingQueue\n数据结构：单向链表 有界性：可设置 如果不设置是 Int最大值 也可以设置边界 公平性：==非公平== 构造方法 public LinkedBlockingQueue(int capacity) capacity：初始化容量，可以不传 ==内部使用两把锁,== , ==所以相较于ArrayBlockingQueue, 消费者生产者可以并行== ==putLock== notFull ：控制生产者线程condition ==takeLock== notEmpty : 控制消费者线程condition PriorityBlockingQueue\n数据结构：==二叉堆数组, 堆顶元素（优先级最高)==\n有界性：无界\n公平性：非公平\n构造方法\npublic PriorityBlockingQueue(int initialCapacity,Comparator\u0026lt;? super E\u0026gt; comparator) initialCapacity:初始化容量\n- comparator：比较器 插入操作：新元素插入到堆的末尾，然后根据优先级上浮，调整堆的结构，确保堆的性质不被破坏。\n取出操作：取出堆顶元素（优先级最高），然后将堆尾的元素移到堆顶，并根据优先级下沉，重新调整堆的结构。\n特点：\n添加到PriorityBlockingQueue队列中的元素会排序，==取出来的永远是优先级最高的, 不保证同级之间的有序性, 先入的不一定先出== 元素必须是可比较的 ==元素本身实现Comparable接口== ==创建PriorityBlockingQueue实例的时候传入Comparator比较器== ==SynchronousQueue==\n数据结构：没有任何元素空间, 内部不存储元素, 确保每次都是立即交付, 实时性要求高、一对一数据交换\n有界性：无意义 公平性：可设置 构造方法 public SynchronousQueue(boolean fair) fair：公平性 特点： 内部没有一个元素，维护一组线程，一个take线程必须等待一个put线程，反之亦然 ==就好比将文件直接交给同事，还是将文件放到她的邮箱中并希望她能尽快拿到文件。== 29. CAP理论 CP 和 AP 是分布式系统理论中的两个重要概念，它们来源于 CAP 定理（CAP Theorem），这是分布式计算领域的一个基本原理。CAP 定理指出，一个分布式系统不可能同时满足以下三个特性：\n一致性（Consistency） - ==所有节点在同一时间看到的数据是一致的。当一个更新操作完成后，所有后续的读取操作都必须返回最新的值，即在任何时刻，所有节点中的数据都是相同的==。 可用性（Availability） - 系统在任何时候都能够响应用户的请求。即使某些节点出现故障，系统仍然能够继续处理请求，而不会出现整个系统不可用的情况。 分区容错性（Partition tolerance）==是分布式系统中必然存在== - 系统能够容忍网络分区，即当网络发生故障时，系统仍然能够继续运行。分区容错性是分布式系统必须具备的特性，因为在现实世界中，网络问题是无法完全避免的。 在 CAP 定理的背景下，CP 和 AP 指的是：\nCP（Consistency and Partition tolerance） - 系统选择保证一致性和分区容错性，但可能会牺牲可用性。在网络分区发生时，系统可能会拒绝处理请求，直到网络恢复，以确保数据的一致性。==典型的 CP 系统例子是分布式数据库，如 MongoDB==。 AP（Availability and Partition tolerance） - 系统选择保证可用性和分区容错性，但可能会牺牲一致性。在网络分区发生时，系统会继续处理请求，即使这意味着==不同节点上的数据可能会暂时不一致==。==典型的 AP 系统例子是一些分布式缓存系统，如 Redis==。 在实际的分布式系统设计中，根据业务需求的不同，可能会倾向于选择 CP 或 AP。例如，对于需要强一致性的场景（如金融交易系统），可能会选择 CP；而对于可以容忍最终一致性的场景（如社交媒体平台），可能会选择 AP。\n30. 线程池 线程数量的控制 已经线程创建和销毁的控制 1. 线程池有哪些状态 类比电脑关机\n正在运行-run 点击关机-shutdown, 不接受任务, 处理正在运行的任务 停止-stop, 不接受任务, 中断任务队列中正在进行的任务 整理-所有任务已终止 终止 RUNNING：处于RUNNING状态的线程池能够接受新任务，以及对新添加的任务进行处理。 SHUTDOWN(关机)：处于SHUTDOWN状态的线程池不可以接受新任务，但是可以对已添加的任务进行处理。 STOP(停止)：处于STOP状态的线程池不接收新任务，不处理已添加的任务，并且会中断正在处理的任务。 TIDYING(整理)：当所有的任务已终止，ctl记录的”任务数量”为0，线程池会变为TIDYING状态。当线程池变为TIDYING状态时，会执行钩子函数terminated()。terminated()在ThreadPoolExecutor类中是空的，若用户想在线程池变为TIDYING时，进行相应的处理；可以通过重载terminated()函数来实现。 TERMINATED(终止)：线程池彻底终止的状态。 2. 线程池参数 corePoolSize(==核心线程==)\n​\t线程池中核心线程的数量（也称为线程池的基本大小）。当提交一个任务时，线程池会新建一个线程来执行任务，直到当前线程数等于corePoolSize。如果调用了线程池的prestartAllCoreThreads()方法，线程池会提前创建并启动所有基本线程。\nmaximumPoolSize(==最大线程数==)\n​\t线程池中允许的最大线程数。线程池的阻塞队列满了之后，如果还有任务提交，如果当前的线程数小于maximumPoolSize，则会新建线程来执行任务。注意，如果使用的是无界队列，该参数也就没有什么效果了。\nkeepAliveTime(==空闲的时间==)\n​\t线程空闲的时间。线程的创建和销毁是需要代价的。==线程执行完任务后不会立即销毁，而是继续存活一段时间==：keepAliveTime。默认情况下，==该参数只有在线程数大于corePoolSize时才会生效==。\nunit\nkeepAliveTime的单位。TimeUnit\nworkQueue(==工作队列==)\n用来保存等待执行的任务的BlockQueue阻塞队列，等待的任务必须实现Runnable接口。选择如下：\nArrayBlockingQueue：基于数组结构的有界阻塞队列，FIFO。 LinkedBlockingQueue：基于链表结构的有界阻塞队列，FIFO。 PriorityBlockingQueue：具有优先级别的阻塞队列。 SynchronousQueue：不存储元素的阻塞队列，每个插入操作都必须等待一个移出操作。\nthreadFactory(==线程的创建工厂==)\n​\t用于设置创建线程的工厂。ThreadFactory的作用就是提供创建线程的功能的线程工厂。他是通过==newThread()方法提供创建线程的功能==，newThread()方法创建的线程都是“非守护线程”而且“线程优先级都是默认优先级”。\nhandler(==拒绝策略==)\n​\tRejectedExecutionHandler，线程池的拒绝策略。所谓拒绝策略，是指将任务添加到线程池中时，线程池拒绝该任务所采取的相应策略。当向线程池中提交任务时，如果此时线程池中的线程已经饱和了，而且阻塞队列也已经满了，则线程池会选择一种拒绝策略来处理该任务。\n线程池提供了四种拒绝策略：\nAbortPolicy：==直接抛出异常，默认策略==； CallerRunsPolicy：用调用者所在的线程来执行任务； DiscardOldestPolicy：丢弃阻塞队列中靠最前的任务，并执行当前任务； DiscardPolicy：直接丢弃任务； 当然我们也可以实现自己的拒绝策略，例如记录日志等等，实现\t接口即可\n3. Excutors 线程池 FixedThreadPool: 复用固定数量的线程处理一个**共享的无边界队列LinkedBlockingQueue ** SingleThreadExecutor: 会使用==单个工作线程==来执行一个无边界的队列, corePool和maximumPoolSize均被设置为1, workQueue使用的是LinkedBlockingQueue CachedThreadPool: 会根据需要，==在线程可用时，重用之前构造好的池中线程==，==否则创建新线程==, 在执行==大量短周期的异步任务时, 性能较好==, SynchronousQueue, 调用 execute 时，可以重用之前已构造的可用线程，如果不存在可用线程，那么会重新创建一个新的线程并将其加入到线程池中。如果线程超过 60 秒还未被使用，就会被中止并从缓存中移除。因此，线程池在长时间空闲后不会消耗任何资源 ScheduledThreadPool ==工作队列是DelayedWorkQueue==: 定时任务：可以安排一个任务在一定的延迟之后执行。 周期性任务：可以安排一个任务定期执行。 固定延迟：在每次==执行完毕后==，等待固定的时间间隔再次执行。 固定周期：在每次==执行开始后==，等待固定的时间间隔再次执行。 线程池管理：可以控制线程池的大小，以及线程的创建和销毁策略。 4. 线程执行出现异常时, 线程池会怎么处理 ==当执行方式是execute时,可以看到堆栈异常的输出==。 当执行方式是submit时,堆栈异常没有输出。但是调用Future.get()方法时，可以捕获到异常。 不会影响线程池里面其他线程的正常执行。 线程池会把这个线程移除掉，并创建一个新的线程放到线程池中。 5. 线程池如何知道线程已经执行完任务 is.isTerminated() 方法 但是需要 先用Executors.shutdown 才能拿到, 因为里面有核心线程数 Executors.submit 后回得到 Future, 用 future.get 是阻塞的获取线程执行结果 CountdownLatch, 思路主要是阻塞唤醒, 在执行完我们的业务的时候, countDown, ==在主线程用await等待,当countDown为0时就会被唤醒,说明线程所有的任务已经执行完毕== 31. ForkJoin 把==大任务分成若干个小任务, 用的是工作窃取算法==\n那么为什么需要使用工作窃取算法呢？\n假如我们需要做一个比较大的任务，我们可以把这个任务分割为若干互不依赖的子任务，\n为了减少线程间的竞争，于是把这些子任务分别放到不同的队列里，并为每个队列创建一个单独的线程来执行队列里的任务，\n线程和队列一一对应，比如A线程负责处理A队列里的任务。\n但是有的线程会先把自己队列里的任务干完，而其他线程对应的队列里还有任务等待处理。\n干完活的线程与其等着，不如去帮其他线程干活，于是它就去其他线程的队列里窃取一个任务来执行。\n而在这时它们会访问同一个队列，\n所以为了减少窃取任务线程和被窃取任务线程之间的竞争，\n通常会使用双端队列，被窃取任务线程永远从双端队列的头部拿任务执行，\n而==窃取任务的线程永远从双端队列的尾部拿任务执行==。\n工作窃取算法的优点是充分利用线程进行并行计算，并减少了线程间的竞争，\n其缺点是在某些情况下还是存在竞争，比如==双端队列里只有一个任务时==。\n并且消耗了更多的系统资源，==比如创建多个线程和多个双端队列==。\n如何使用\nForkJoinTask：我们要使用ForkJoin框架，必须首先创建一个ForkJoin任务。它提供在任务中执行fork()和join()操作的机制，通常情况下我们==不需要直接继承ForkJoinTask类，而只需要继承它的子类==，Fork/Join框架提供了以下两个子类： RecursiveAction：用于没有返回结果的任务。 RecursiveTask ：用于有返回结果的任务。 ForkJoinPool ：ForkJoinTask需要通过ForkJoinPool来执行，任务分割出的子任务会添加到当前工作线程所维护的双端队列中，进入队列的头部。==当一个工作线程的队列里暂时没有任务时，它会随机从其他工作线程的队列的尾部获取一个任务==。 32. Jvm 运行参数 -help 输出的都是标准参数\n-server与-client参数 , JVM会根据硬件和操作系统自动选择使用Server还是Client类型的JVM\n64位操作系统 都模式server\nserver 初始堆空间会大一些，默认使用的是并行垃圾回收器，启动慢运行快\nClient VM相对来讲会保守一些，初始堆空间会小一些，使用串行的垃圾回收器\n-X 查看非标准参数 java -X\n-Xms与-Xmx参数, -Xms与-Xmx分别是设置==jvm的堆内存的初始大小和最大大小==。 -XX参数也是非标准参数 , 主要用于==jvm的调优和debug操作==\nboolean类型 格式：-XX:[+-] 表示启用或禁用属性\n如：-XX:+DisableExplicitGC 表示禁用手动调用gc操作，也就是说调用System.gc()无效\n非boolean类型 格式：-XX:= 表示属性的值为\n如：-XX:NewRatio=1 表示新生代和老年代的比值\n查看JVM 运行的参数\n运行java命令时打印出运行参数, 加上 -XX:+PrintFlagsFinal就可以, 如: java -XX:+PrintFlagsFinal -version\n查看 运行的java进程\njinfo -flags \u0026lt;进程id\u0026gt;\n33. Java 堆内存模型 1. Java7 Young 年轻区（代）\nYoung区被划分为三部分，==Eden区和两个大小严格相同的Survivor区==，其中，==Survivor区间中，某一时刻只有其中一个是被使用的，另外一个留做垃圾收集时复制对象用==，在Eden区间变满的时 候， GC就会将存活的对象移到空闲的Survivor区间中，根据JVM的策略，==在经过几次垃圾收集后==，任然存活于Survivor的对象将被移动到Tenured区间。\nTenured 年老区\nTenured区主要保存生命周期长的对象，一般是一些老的对象，当一些对象在Young复制转移一定\n的次数以后，对象就会被转移到Tenured区，一般如果系统中用了application级别的缓存，缓存中\n的对象往往会被转移到这一区间。\nPerm 永久区\n主要保存class,method,fifiled对象\n2. Java8 年轻代: Eden + 2 Survivor 老年代: OldGen 永久代: 被替换成了元数据空间MetaSpace , ==用的不是Jvm内存了, 在本机内存空间里面== 为什么弃用\n==由于永久代内存经常不够用或发生内存泄露，爆出异常java.lang.OutOfMemoryError: PermGen==\n3. jstat 垃圾回收统计 jstat -gc \u0026lt;进程id\u0026gt; \u0026lt;间隔时间ms\u0026gt; \u0026lt;打印次数\u0026gt; S0C：第一个Survivor区的大小（KB）\nS1C：第二个Survivor区的大小（KB）\nS0U：第一个Survivor区的使用大小（KB）\nS1U：第二个Survivor区的使用大小（KB）\nEC：Eden区的大小（KB）\nEU：Eden区的使用大小（KB）\nOC：Old区大小（KB）\nOU：Old使用大小（KB）\nMC：方法区大小（KB）\nMU：方法区使用大小（KB）\nCCSC：压缩类空间大小（KB）\nCCSU：压缩类空间使用大小（KB）\nYGC：年轻代垃圾回收次数\nYGCT：年轻代垃圾回收消耗时间\nFGC：老年代垃圾回收次数\nFGCT：老年代垃圾回收消耗时间\nGCT：垃圾回收消耗总时间\n4. jmap 可以使用 Arthas的命令来替代, memory: 获取当前 Java 虚拟机（JVM）的整体内存使用状态，包括堆内存、非堆内存以及各细分区域的使用详情\njmap -heap \u0026lt;进程id\u0026gt; 可以对内存情况 dump 到文件中, 然后使用jhat 命令配合 OQL 语句进行查询分析\n35. jstack jstack \u0026lt;pid\u0026gt; 的作用是将正在运行的jvm的==线程情况进行快照==，并且打 印出来：==可以用来排查死锁==\n死锁: ==启动2个线程，Thread1拿到了obj1锁，准备去拿obj2锁时，obj2已经被Thread2锁定==\n34. 设计模式-适配器和策略 适配器模式和策略模式的混合使用\n就比如说我们 , ==平台支持多种支付方式==（如信用卡、支付宝、微信支付等）。为了支持这些支付方式，你可以使用==策略模式来定义不同的支付策略==，同时==使用适配器模式来处理不同支付接口的兼容性==。\n支付策略接口：定义一个支付策略接口 PaymentStrategy，包含一个 pay 方法。 具体支付策 略：实现不同的支付策略，如信用卡支付、支付宝支付等 CreditCardPayment 适配器类：假设你有一个==旧的支付接口，无法直接与新的支付策略兼容==。你可以创建适配器类来适配它。 上下文类：创建一个上下文类，==用于调用策略，持有一个策略对象的引用==，并可以==根据需要更换策略==。 35. 垃圾回收 0. 引用类型 强引用: 当==对象被强引用时，垃圾回收器永远不会回收它==，除非显式地将引用设置为null。 软引用: JVM在==内存不足时会回收软引用指向的对象==，但在内存充足时不会回收这些对象。 弱引用: 弱引用对象在垃圾回收时会被立即回收。 虚引用: 虚引用的对象并不会直接被程序使用，主要用于跟踪对象的垃圾回收。 适合需要释放资源的场景，例如文件、网络连接等 1.垃圾回收算法 引用计数法 在对象中存储一个计数器用来记录被引用次数，当产生一个引用关系，计数+1，当一个引用关系失效计数-1，如果减到0，就将当前对象清除 优点： 实时 区域性，不会造成GC停顿 缺点： 会比较消耗cpu，在不断的计算引用计数和进行gc 循环引用问题无法解决（致命缺点） 如果两个对象互相引用，如果指针指向null，对象的引用计数仍然不为0，所以不会被移除 标记清除法 从根节点开始标记可达对象，将不可达的对象清空 分为两个阶段 标记 清除 两个阶段都会造成GC停顿 优点：解决引用计数法循环引用的问题 缺点： 会造成比较长的GC停顿时间 清理之后内存空间是不连续的 标记压缩法 和标记清除法十分类似，在清理阶段不太一样 一边清理，一边压缩，将存活对象压缩到内存的一端，保证空闲的内存空间是连续的 优点：解决了标记清除法的碎片化问题 缺点：由于标记和清除阶段都需要GC停顿，会造成比较长的GC停顿时间 复制算法 将内存划分为两块完全相等的空间，当gc的时候将一块空间中的存活对象复制转移到另一块空间，保证始终至少有一个空间是空闲的 在年轻代使用复制算法，年轻代的两个Survivor区命名为form区和to区，每次将form区和Eden区的存活对象复制拷贝到to区，转化from和to的角色。 当内存中的垃圾对象较多的时候，只需要复制转移少量的对象，适合使用复制算法 分代收集算法 将内存划分为多个区域，根据对象的特点进行区分，不同的区域使用不同的垃圾回收算法 年轻代：存储刚被创建的和大概率可回收的对象，适合使用复制算法 老年代：大概率不可回收对象，适合使用标记清除/标记压缩算法 三色标记算法 白色: 未被扫描的对象 黑色: 已扫描且存活对象 灰色: 已扫描, 自身是存活的, 但器引用对象存活情况发需要进一步扫描 2.垃圾收集器 串行垃圾收集器 使用单线程进行垃圾回收 -XX:+UseSerialGC设置年轻代和老年代都使用串行垃圾收集器 并行垃圾收集器 Parallel GC 使用多线程进行垃圾回收 ParNew垃圾收集器 运行在年轻代的，老年代仍然使用串行垃圾收集器 通过-XX:+UseParNewGC进行设置 ==ParallelGC垃圾收集器（jdk8默认垃圾收集器）== 通过-XX:+UseParallelGC设置在年轻代使用ParallelGC 通过-XX:+UseParallelOldGC设置在老年代使用ParallelGC 可以通过设置一些参数让jvm自动调整堆空间的分配 -XX:MaxGCPauseMillis=\u0026gt;示最大允许的GC停顿时间 -XX:GCTimeRatio=\u0026gt;表示程序占运行时间的百分比默认99，表示程序运行时间占总时间的99%，1%是垃圾回收时间 -XX:+UseAdaptiveSizePolicy 设置为true表示开启自适应模式，让jvm自动调整达到gc和程序执行的平衡 CMS (==低停顿的==)垃圾收集器 在老年代使用的垃圾收集器在jdk8默认是关闭的,需要手动开启 在标记清除法的基础上进行优化 初始标记根节点：造成STW 并发标记：在运行的同时标记可达对象 预处理 再标记：造成STW，标记在并行过程中遗漏的对象 并发清理 调整堆大小：清除内存碎片 并发重置状态等待下次CMS的触发 总结：这种方式造成STW只有两个阶段（标记根节点和再标记），消耗的时间小于标记清除法两个阶段的时间，所以造成GC停顿时间比较短，效率比较高，但是没办法清理掉所有垃圾，这种方式是在清理所有垃圾和响应时间上做出的权衡 通过参数-XX:+UseConcMarkSweepGC进行设置 G1垃圾收集器（重点） 摒弃了传统的堆内存的划分而是使用了若干个大小相同的Region区域，这些区域分为4个 Eden:保存新创建的对象 Survivor：保存年轻代GC之后存活的对象 Old： 保存多次年轻代GC之后仍然存活的对象 保存年轻代Surviovr存满之后转移过来的对象 Humongous 保存超过Region的50%大小的对象==（巨型对象）超过2mb==，如果一个区存不下，寻找连续的Humongous区进行存储，如果找不到进行FULLGC 精髓就是它的所有空间都是浮动的，不是固定的，是一个动态平衡，需要就用，不需要就归还 记忆集合Remembered Set 将Region以512kb一个card划分为若干个card 每一个card都存在一个集合用来保存被引用的card位置 当GC时需要定位根节点的时候，只需要扫描记忆集合中记录的card的位置就能快速找到根节点，不需要扫描整个内存空间 GC YoungGC：年轻代的GC MixedGC：年轻代的GC+部分老年代GC 触发条件：-XX:InitiatingHeapOccupancyPercent=n 当前老年代占堆空间的n%就触发MixedGC 全局并发标记（和cms标记阶段比较类似） 标记根节点 遍历记忆集合中记录的card位置找到根节点进行初始化标记 并发标记老年代中的节点 并发标记所有堆内存的节点 重新标记（会导致STW） 预清理 并没有真的清理而是在第二个阶段开始清理，主要是检查过程 复制转移对象 全程STW，将标记对象复制转移到另一块Region，并情空当前Region FULLGC：所有内存空间包括Humongous进行GC 使用过程 开启g1 -XX:+UseG1GC 设置最大堆内存 -Xmx 调优参数的设置 -XX:MaxGCPauseMillis=\u0026gt;所能允许的最大gc停顿时间，需要反复调整达到最佳gc情况 -XX:G1HeapRegionSize=\u0026gt;Region区域大小，默认是堆内存的1/2000，设置范围是1-32m -XX:ParallelGCThreads=n =\u0026gt;工作线程的数量，默认是cpu核数，最大是8 -XX:ConcGCThreads=n =\u0026gt;并发标记的线程数量，一般是ParallelGCThreads的1/4 相比于CMS来说 标记根节点的时间短了，因为不需要扫描整个堆空间，只需要通过记忆集合扫描几个card就行了，意味着STW时间变短了，程序响应速度变快了 cms有一个整理碎片的过程，G1不需要整理，因为它是复制算法，而且使用mixedGC只会每次收集几个老年代，意味着每次mixedGC只需要复制转移少量的对象，所以效率很高。 垃圾回收器, 不外乎就两个功能\n标记垃圾 引用计数法 可达性 清理垃圾 标记清除 标记整理 复制 分代收集 36. ThreadLocal 允许==每个线程存储和访问独立的变量副本, 适用于线程隔离的场景==\nThreadLocal 变量==通常由线程的生命周期管理==，并存储在 Thread 对象的内部数据结构中，通常是 ThreadLocalMap。每个线程在使用 ThreadLocal 时，都会创建一个专属于该线程的变量副本。\n潜在的内存泄漏风险出现在以下情况：\nThreadLocalMap ==使用弱引用（weak reference）来引用 ThreadLocal 对象本身==，但它对==ThreadLocal 变量值==使用的是强引用。\nkey：ThreadLocal 对象本身作为 key，但 ThreadLocalMap 对这个 key 的引用是一个弱引用（WeakReference）。这意味着，==如果没有其他强引用指向这个 ThreadLocal 对象，JVM 的垃圾回收器会回收这个 ThreadLocal 对象==。\nvalue：value 是线程使用 ThreadLocal 存储的变量值，ThreadLocalMap 对这些变量值使用的是强引用。因此，只要线程不结束，ThreadLocalMap 会继续持有这些值，即使 ThreadLocal 对象本身已经被回收。\n如果一个 ThreadLocal 被手动设置为 null 或者不再被使用，那么这个 ThreadLocal 对象会被垃圾回收。但对应的变量副本（value）可能仍然存在于 ThreadLocalMap 中，且这个副本与线程的生命周期绑定。\n由于 ThreadLocalMap 持有对这些值的强引用，即使 ThreadLocal 对象被垃圾回收，值仍然不会被回收，导致内存泄漏。\n如果用了线程池, ThreadLocal会随着线程的回收被销毁, 但是线程一直存活的话, 内存泄漏会随着时间推移越来越严重\n==ThreadLocal静态和成员变量的影响==：\nstatic ThreadLocal：因为 static 变量属于类，所以所有线程共享同一个 ThreadLocal 变量，但由于 ThreadLocal 为每个线程维护一个独立的值，这些线程各自有不同的值，而不是共享同一个值。这种方式适合所有对象实例都需要访问相同的线程本地变量。 成员变量 ThreadLocal：成员变量属于每个实例，所以每个对象实例都有自己的 ThreadLocal 变量和独立的线程本地值。这意味着不同的对象实例彼此独立，线程在操作时不会共享线程本地数据。 37. fail-safe 和 fail-fast 两种迭代器的行为模式，用于描述在并发修改集合时如何处理异常。\nFail-fast: 当使用迭代器遍历集合时，如果在遍历过程中检测到集合被修改抛出异常ConcurrentModificationException , 非并发安全的集合, hashMap, arrayList Fail-safe: 迭代器在遍历集合时不会抛出异常，即使集合被并发修改。 遍历时==使用集合的副本==（例如通过 CopyOnWriteArrayList 或 ConcurrentHashMap）来实现的，因此修改不会影响当前的迭代过程。 38. 异常类型 Java的异常类型层次结构是基于继承自 Throwable 类的。Throwable 主要分为两大子类：==Error 和 Exception==，其中 Exception 再分为受检异常和非受检异常。\n受检异常：编译时必须处理的异常，继承自 Exception 类（不包括 RuntimeException）。 例子：IOException、SQLException 需要显式捕获或通过 throws 声明。\n非受检异常：编译时不强制处理的异常，继承自 RuntimeException。 例子：NullPointerException、ArithmeticException由运行时错误或逻辑错误引发，不强制处理。\n39. 代理 静态代理实现较简单，只要==代理对象对目标对象进行包装，即可实现增强功能==，但静态代理只能为一个目标对象服务，如果目标对象过多，则会产生很多代理类。 JDK动态代理需要目标对象实现业务接口，代理类只需实现InvocationHandler接口。 动态代理生成的类为 lass com.sun.proxy.$Proxy4，cglib代理生成的类为class com.cglib.UserDao$$EnhancerByCGLIB$$552188b6。 静态代理在编译时产生class字节码文件，可以直接使用，效率高。 动态代理必须实现InvocationHandler接口，通过反射代理方法，比较消耗系统性能，但可以减少代理类的数量，使用更灵活。 cglib代理无需实现接口，通过生成类字节码实现代理，比反射稍快，不存在性能问题，但cglib会继承目标对象，需要重写方法，所以目标对象不能为final类。 动态代理是什么, ==运行时创建代理对象并处理方法调用的一种机制,只是对原始实现的拦截去做功能的增强==\n==JDK的动态代理是通过 java.lang.reflect.Proxy 类和 InvocationHandler 接口来实现的==。Proxy 类负责创建代理对象，InvocationHandler 接口负责定义代理对象中方法调用时的行为。\nJdk动态代理关键步骤：\n代理对象的创建 ： 使用 Proxy.newProxyInstance() 方法创建代理对象(==继承Proxy类的==)。这个方法要求代理的类必须实现一个或多个接口，==因为代理对象最终是通过实现接口来代理原始对象的方法调用==。 方法调用的拦截： 当调用代理对象的方法时，代理对象会将这个==调用转发给 InvocationHandler.invoke() 方法==。在这个方法里，可以自定义逻辑，拦截并处理实际方法的执行。 1. 为什么只能代理有接口的类? JDK动态代理不能代理没有实现接口的类，==因为代理机制依赖接口==，而不是单继承的限制。\n因为它依赖于接口的存在，==通过代理对象实现接口来拦截方法调用==。\n2. cglib 是怎么实现的 ==通过生成目标类的子类来拦截方法调用==。CGLIB（Code Generation Library）是一种==基于字节码的动态代理技术==，广泛用于框架中（如 Spring）来对类进行代理，尤其是当目标类没有实现接口时。\n如 Spring 中的 AOP（面向切面编程）在某些情况下（目标类没有实现接口时）会使用 CGLIB 代理。\n无法代理 final 类和 final 方法 性能：CGLIB 的性能通常较好，但因为它==生成代理类时需要进行字节码操作== 40. 对象的创建过程 类加载: 加载目标类（如果尚未加载） 加载: 通过类加载器将class文件加载到内存中, 生成Class对象 链接: 验证: 类文件格式是否正确, 字节码是否符合规范 准备: 为静态变量分配内存, 初始化默认值 解析: 初始化: 对静态变量赋初始值, 执行静态代码块 内存分配: 在堆内存中为新对象分配空间 对象初始化: 执行默认初始化、显式初始化和构造器初始化 返回引用: new 返回对象的引用，允许外部访问该对象 41. new String(abc) 创建了几个对象 直接使用字面量 \u0026ldquo;abc\u0026rdquo;： 当你使用 \u0026quot;abc\u0026quot; 这种字面量形式时，Java 会将其放入字符串常量池中。如果在代码的其他地方也使用了相同的字面量 \u0026quot;abc\u0026quot;，JVM 会复用同一个字符串对象，而不会创建新的对象。这种方式提高了效率和内存利用率。\n使用 new String(\u0026quot;abc\u0026quot;)： 当你使用 new String(\u0026quot;abc\u0026quot;) 时，它会显式地创建一个新的 String 对象，且该对象不在字符串常量池中，而是在堆内存中。这意味着即使内容相同，使用 new String(\u0026quot;abc\u0026quot;) 创建的对象与常量池中的字符串是两个不同的对象。\n两种可能\n==常量池里已经存在abc, 就只会创建一个, 不存在就会创建 String 实例对象和 \u0026lsquo;abc\u0026rsquo; 常量== 42. String , String Buffer, String Builder 可变性: String 不可变, 其余两个都是可变的 线程安全: String Buffer用了sync, StringBuilder 不是线程安全的 性能: String Builder是性能最高的, 字符串拼接编译器会帮我们优化, String Buffer其次,String 是最低 存储: String 在常量池, 其他都在堆内存空间 43. Integer 相等判断 包装类型, 因为引入了享元模式, 就是缓存: -127到128 , 如果值在这个区间就会直接从缓存中获取==Integer这样一个实例==, 会出现明明是值相同, 但是== 比较却为false, 因为 比较的是内存地址\n44. 深拷贝, 浅拷贝 这个是对象复制场景\n深拷贝会在堆内存空间创建一个新的对象, 引用指针指向这个新对象\n要继承Cloneable 接口, 重写clone 方法, 实现序列化接口, 有很多工具类可以使用来进行深拷贝 浅拷贝是只拷贝了指针, 都是指向的堆内存里面的同一个内存地址\n45. Java文件拷贝方式 IO包里面的 FileInputStream 和 FileOutputStream\nnio的 Files 类\nnio 的transferTo() 和 transferFrom() 方法可以实现零拷贝（Zero-Copy） , ==避免了将数据在内核空间和用户空间之间来回拷贝==\n允许直接将文件内容从一个通道传输到另一个通道（如文件到网络或反之），而无需将数据复制到用户空间（Java 应用的内存空间） 46. 零拷贝 零拷贝的工作流程：\n内核空间读取文件的数据块（数据位于内核文件缓存中）。 直接将数据传输到网络通道或目标文件的内核缓冲区中，无需拷贝到用户空间。 网络通道或目标文件通道将数据写入目标（如磁盘或网络）。 47. 设计模式 创建型模式：负责对象的创建，帮助解耦对象的实例化过程，如==单例模式、工厂模式和原型模式==。\n结构型模式：用于处理类和对象之间的关系，促进代码的可扩展性和模块化，如==装饰器模式、适配器模式和代理模式==。\n行为型模式：专注于对象之间的交互和职责分配，如==观察者模式、策略模式和命令模式, 责任链==。\nSpring 解决循环依赖主要使用了 代理模式（Proxy Pattern）和 工厂模式（Factory Pattern）。\n具体实现：\n代理模式：在 Spring 中，==当检测到循环依赖时，它会创建一个代理对象而不是直接返回原始对象==。这个代理对象可以==在后续需要时引用实际的对象，从而打破循环依赖==。 工厂模式：Spring 使用 BeanFactory 来管理 Bean 的生命周期。在创建 Bean 的过程中，如==果遇到循环依赖，Spring 会通过工厂方法先返回一个代理对象，等到真正需要该 Bean 的时候再去注入依赖的实例==。 工作流程：\n当 Spring 检测到 A 和 B 之间存在循环依赖时，首先创建 A 的代理对象，并将其注入到 B 中。 然后，Spring 再继续创建 B 的实例并注入 A 的代理对象。 当真正需要 A 的时候，代理会被替换为实际的 A 实例。 这种方式使得 Spring 能够有效地处理循环依赖问题，同时保持了松耦合和高内聚的设计原则。\n1. 单例模式 私有构造方法 提供静态方法 在多线程的时候可以使用静态代码块, 或者饿汉式加载, 或者可以使用枚举来实现 2. 策略模式 ==可拓展, 易于维护, 动态选择==\nContext（上下文）：==持有对某个策略的引用，并负责调用该策略的算法==。 Strategy（策略接口）：定义一个统一的接口，用于所有支持的算法。 ConcreteStrategy（具体策略类）：实现策略接口，提供具体的算法实现。 3. 观察者模式 ==低耦合, 动态关系, 广播通信==\nSubject（主题）：被观察者，管理观察者的注册和通知。\nObserver（观察者接口）：定义一个更新接口，用于接收主题的通知。\nConcreteObserver（具体观察者类）：实现观察者接口，并在接收到通知时更新自己。\n4. 责任链模式 ==降低耦合性 , 增强灵活性 简化代码==\nHandler（处理者接口）：定义一个处理请求的方法，并包含对下一个处理者的引用。\nConcreteHandler（具体处理者类）：实现处理者接口，负责处理请求。如果自己无法处理请求，则将其转发给链中的下一个处理者。\nClient（客户端）：发送请求的对象，通常只与链的起始处理者交互。\n==假设我们有一个审批流程，涉及多个级别的审批（如经理、总监和CEO），每个级别都可以处理不同金额的请求==\n48. finally 块一定会执行吗 不一定\n捕获范围不够大, 没进到try 的代码块里面,就异常退出是不会执行的 在try catch 里面 用了 System.exit() 也不会执行 49. Raft 算法 分布式系统中的一致性算法, 多个节点（或服务器）在面对节点故障或网络分区时，==能够一致地维护一份数据副本==。\n1. 重要组件 领导者（Leader）：Raft 中的节点通过选举形成一个领导者，所有写入操作都通过领导者进行。领导者负责处理客户端的请求并将数据复制到其他节点。 跟随者（Follower）：除了领导者，其他节点都是跟随者，==负责响应领导者的请求和转发数据==。 候选者（Candidate）：在领导者失效时，节点可以转变为候选者，发起选举以选出新的领导者。 2. 工作流程 选举过程：如果一个节点在一定时间内未接收到领导者的心跳信号，它会转变为候选者并开始选举。在选举中，节点通过投票选出新的领导者，确保在集群中始终有一个活跃的领导者。 日志复制：==领导者将接收到的客户端请求以日志条目的形式保存==，并将其复制到所有跟随者。如果==大多数节点（多数决）确认接收，领导者会提交这些日志条目==，并通知跟随者更新状态机。 状态机应用：所有节点（包括领导者和跟随者）在接收到提交的日志条目后，会将其应用于各自的状态机，从而保证状态的一致性。 50. SPI Service provider interface ==基于接口的动态拓展机制==, 广泛应用于 Java 和其他编程语言中 , 它允许在不修改现有代码的情况下，==动态地加载和使用服务实现==。\n解耦：SPI 提供了一种方式，让接口和其实现可以分开管理，从而实现灵活的替换和扩展。\n动态加载：==通过使用反射和配置文件，SPI 允许在运行时动态加载服务的实现==。\n插件式架构：SPI 支持插件机制，允许开发者在不修改核心代码的情况下，添加新的功能或实现。\n实现步骤\n定义接口：首先，定义一个服务接口（API），该接口提供了一组功能。\n实现接口：然后，创建一个或多个实现该接口的类（服务提供者）\n配置文件：在 META-INF/services/ 目录下，创建一个以接口全名命名的文件，文件内容是实现类的全名。JAR 包中的所有实现类都会被列在这个文件中。\n加载实现：使用 ServiceLoader 类，可以在运行时查找并加载实现类。例如\n51. 包装类和基础数据类型 short int long double float byte char boolean\n==内存==\n基本数据类型：存储在栈内存中，效率高，分配和释放速度快。 包装类：存储在堆内存中，创建对象需要更多的内存和处理时间。 ==用途==\n基本数据类型：用于处理简单的数值、字符和布尔值，适合对性能要求较高的场景。 包装类：用于需要对象的场景，如集合（ArrayList、HashMap 等）、泛型等，因为这些集合只能存储对象，而不能存储基本类型。 ==功能==\n基本数据类型：不提供任何方法或功能，简单存储值。 包装类：提供了一些方法，可以进行类型转换、比较、转换为字符串等。例如： Integer.parseInt(String s)：将字符串转换为 int。 Integer.valueOf(int i)：将 int 转换为 Integer 对象。 52. equal, hashCode 在 Java 中，当你重写 `equals()` 方法时，必须同时重写 `hashCode()` 方法，原因主要与==哈希表（如 `HashMap`、`HashSet`）的工作机制==有关 ​\thashCode() 方法返回对象的哈希码，==它是一个整数，通常是对象的一种唯一标识==。哈希码的计算通常基于对象的属性。\n​\t重写 equals() 方法后重写 hashCode() 方法是==为了确保对象在哈希表中正常工作，维护它们之间的一致性==。这是 Java 的规范，确保了集合框架（如 HashSet 和 HashMap）能正确处理对象的查找和存储。\n53. 集合 时间复杂度: 用于描述算法在执行时所需的时间与输入规模之间的关系\nO(1)：常数时间复杂度，不受输入规模影响。操作时间固定，例如数组元素访问。\nO(log n)：对数时间复杂度，通常出现在每次操作都将问题规模减半的算法中，例如二分查找。\nO(n)：线性时间复杂度，算法的执行时间与输入规模成正比，例如遍历数组。\nO(n log n)：线性对数时间复杂度，常见于高效的排序算法，如归并排序和快速排序。\nO(n²)：平方时间复杂度，通常出现在嵌套循环的算法中，例如冒泡排序和选择排序。\nO(2^n)：指数时间复杂度，算法执行时间随输入规模呈指数增长，例如某些递归算法。\nO(n!)：阶乘时间复杂度，通常出现在排列组合问题中，如旅行商问题的暴力解法。\n0. 算法 冒泡排序: 比较相邻元素并交换它们的位置，如果它们的顺序错误 , ==每一轮最大的数已经被排到了最后==\n为什么不是 size 次？ 当第 size - 1 次循环结束时，数组中已经有 size - 1 个元素被放置到了正确的位置，因此最后一个元素自然也是正确的，无需再进行额外的循环。 对于一个大小为 6 的数组，最多只需要 5 次外循环：\n为什么是 size - 1 - i？\nsize - 1 是在数组中进行比较的总次数（因为数组索引从 0 开始，所以是 size - 1）。 减去 i 是因为经过 i 轮外循环，数组的最后 i 个元素已经排好序了，因此在接下来的循环中无需再比较这些元素。 例如： 在第 1 轮（i = 0），你需要遍历整个数组，所以内循环次数是 size - 1； 在第 2 轮（i = 1），你只需要比较前 size - 2 个元素，因为最后一个元素已经排好序； 依此类推，直到最后只剩一个元素需要比较。 public class BubbleSort { public static void bubbleSort(int[] arr) { int n = arr.length; for (int i = 0; i \u0026lt; n - 1; i++) { // 标志位，用于检测是否发生了交换 boolean swapped = false; for (int j = 0; j \u0026lt; n - 1 - i; j++) { if (arr[j] \u0026gt; arr[j + 1]) { // 交换 arr[j] 和 arr[j + 1] int temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; swapped = true; } } // 如果没有发生交换，说明数组已经有序 if (!swapped) { break; } } } public static void main(String[] args) { int[] arr = {64, 34, 25, 12, 22, 11, 90}; bubbleSort(arr); System.out.println(\u0026quot;Sorted array: \u0026quot;); for (int num : arr) { System.out.print(num + \u0026quot; \u0026quot;); } } } 快速排序\npublic class QuickSort { public static void quickSort(int[] arr, int low, int high) { if (low \u0026lt; high) { // 获取划分点 int pi = partition(arr, low, high); // 递归排序 quickSort(arr, low, pi - 1); // 对左侧进行排序 quickSort(arr, pi + 1, high); // 对右侧进行排序 } } public static int partition(int[] arr, int low, int high) { int pivot = arr[high]; // 选择最后一个元素作为基准 int i = (low - 1); // 小于基准的元素索引 for (int j = low; j \u0026lt; high; j++) { // 如果当前元素小于或等于基准 if (arr[j] \u0026lt;= pivot) { i++; // 增加小于基准的元素索引 // 交换 arr[i] 和 arr[j] int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; } } // 将基准元素放到正确的位置 int temp = arr[i + 1]; arr[i + 1] = arr[high]; arr[high] = temp; return i + 1; // 返回基准元素的索引 } public static void main(String[] args) { int[] arr = {10, 7, 8, 9, 1, 5}; int n = arr.length; quickSort(arr, 0, n - 1); System.out.println(\u0026quot;Sorted array: \u0026quot;); for (int num : arr) { System.out.print(num + \u0026quot; \u0026quot;); } } } 1. Set ==HashSet==\n数据结构：基于哈希表（Hash Table）\n不保证元素的顺序。\n允许存储 null 元素。\n基本操作（添加、删除、包含）的平均时间复杂度为 O(1)。\n==LinkedHashSet==\n数据结构：基于==哈希表和双向链表==。\n保持插入顺序（按元素添加的顺序）。 允许存储 null 元素。 基本操作（添加、删除、包含）的时间复杂度为 O(1)，但由于维护链表，性能稍逊于 HashSet。 ==TreeSet==\n数据结构：==基于红黑树==（自平衡的二叉搜索树） 保持元素的自然顺序（或使用提供的比较器进行排序）。 不允许存储 null 元素（因为无法与其他元素进行比较）。 基本操作（添加、删除、包含）的时间复杂度为 O(log n)。 2. List 54. 类加载 安全性：通过双亲委派模型，核心类库（如 java.lang.String）不会被自定义类加载器篡改。即使应用程序中定义了一个类名为 java.lang.String 的类，类加载器也不会加载它，而是优先加载核心类库的 String 类。\n避免重复加载：同一个类在 JVM 中只会被加载一次，双亲委派模型可以==防止类的重复加载==。\n0. 层级 Bootstrap ClassLoader（==启动类加载器==）：==负责加载 Java 核心类库==，如 rt.jar。这是 ==JVM 自带的类加载器==，用 C/C++ 编写，==无法直接获取引用==。\nExtension ClassLoader（==扩展类加载器==）：负责加载扩展类库 ext 目录下的类。\nApplication ClassLoader（==应用类加载器==）：负责加载应用程序的类路径下的类，通常是我们开发者编写的 Java 类。\n其他类加载器\nWeb ClassLoader / Servlet ClassLoader： Web 应用中，应用服务器（如 Tomcat）会==使用特定的类加载器来加载 Web 应用的类。每个 Web 应用都有自己的类加载器==，用于加载 WEB-INF/lib 和 WEB-INF/classes 下的类 , ==隔离不同应用之间的类加载，避免类冲突==\nThread Context ClassLoader（线程上下文类加载器）==可以破坏双亲委派==：\n作用：==允许线程使用自定义类加载器==，通常用于实现与应用服务器、框架集成的场景。可以通过 Thread.currentThread().getContextClassLoader() 设置和获取当前线程的上下文类加载器。 使用场景：==常用于 Java EE 服务器中的线程池或多线程应用==，以便不同线程使用不同的类加载器加载类。 ==可以通过继承ClassLoder 这个类来自定义类加载器, 重写方法可以实现打破双亲委派==, 打破双亲委派就是 类加载器可以加载不属于当前作用范围的类\n1. 双亲委派 双亲委派模型（Parent Delegation Model）是 ==Java 类加载机制中的一种设计模式==\n其核心思想是：==一个类加载器在加载一个类时，首先将这个任务委派给它的父类加载器，如果父类加载器无法完成这个加载任务，再由当前加载器来尝试加载。==\n检查缓存：JVM 首先会检查类是否已经被加载（通常通过缓存）。\n委派父类加载器：==如果缓存中没有找到该类，当前类加载器不会立即尝试自己加载，而是先委托父类加载器去加载==。\n逐级向上委派：==每个类加载器都会向它的父类加载器委派==，直到最顶层的根类加载器（Bootstrap ClassLoader）。如果父类加载器能够成功加载类，就返回这个类。\n当前类加载器加载：如果父类加载器无法加载这个类（即类不在父类加载器的加载范围内），则由当前类加载器尝试加载该类。\n报错：如果当前类加载器也无法加载，==抛出 ClassNotFoundException 异常==。\n2. Tomcat 是怎么打破双亲委派的 ==Tomcat 通过自定义的类加载器打破了传统的双亲委派模型==，以解决 Web 应用的==类隔离、类冲突、热部署等问题==。这种设计使得 Web 应用的类加载更加灵活，同时能够满足多个 Web 应用同时运行且互不干扰的需求。\nTomcat 的类加载器体系如下：\nBootstrap ClassLoader：加载 JRE 核心类。 System ClassLoader：==加载 lib/ 目录下的核心库（如 Servlet API）==。 WebApp ClassLoader：==每个 Web 应用都有自己独立的类加载器==，负责加载应用的 WEB-INF/classes 和 WEB-INF/lib 下的类。 Tomcat 打破双亲委派的原因\n类隔离：\n在 Web 服务器中，每个应用都需要各自独立的类加载空间，防止不同应用之间的类发生冲突。例如，两个不同的 Web 应用可能依赖不同版本的相同库，Tomcat 通过自定义类加载器，确保每个 Web 应用有自己独立的类加载空间，避免应用 A 加载到应用 B 的类。 热部署和类重新加载：\nWeb 应用通常需要支持热部署，即在运行时重新加载类或更新应用。==这意味着某些类在运行过程中需要被卸载并重新加载。Tomcat 通过自定义类加载器，使得某些类可以被动态地重新加载，而不必依赖 JVM 的默认加载机制==。 Servlet API 与 Web 应用类的冲突避免：\nTomcat 的类加载机制需要区分两种类：\n服务器级别的类：如 Servlet API 等，由 Tomcat 的类加载器（通常是父类加载器）加载。 应用级别的类：包括 Web 应用的类和第三方依赖库。这些类应由 Web 应用自己的类加载器加载，不能与服务器类产生冲突。 具体实现\n在 Tomcat 中，==**WebAppClassLoader** 是自定义的类加载器，它会首先尝试加载 Web 应用自身的类，而不是直接委托父类加载器（如 Application ClassLoader 或 Extension ClassLoader）==。这种方式允许 Web 应用自定义实现某些核心类库，甚至覆盖父类加载器加载的某些类。 55. Spring Boot、Spring MVC 和 Spring Spring Framework（Spring）： 是一个开源的Java平台，用于==简化企业级应用程序的开发==。它提供了一系列的功能，比如依赖注入（DI）、面向切面编程（AOP）、数据访问、消息传递、测试支持等。 Spring框架的==核心是控制反转（IoC）容器，它负责管理对象的创建、生命周期、配置和其他对象之间的依赖关系==。 它是一个全面的企业级服务框架，==提供了广泛的功能，但不包括Web框架的功能==。 Spring MVC： 是Spring框架的一个模块，==专门用于构建Web应用程序==。 它实现了MVC（Model-View-Controller）设计模式，帮助开发者将应用程序分为模型（Model）、视图（View）和控制器（Controller）三个部分，以实现关注点分离。 Spring MVC==提供了一个灵活的、易于使用的Web框架，支持RESTful Web服务==，并集成了Spring的其他模块，如数据访问、安全性等。 Spring Boot： 是基于Spring框架的一个项目，旨在==简化Spring应用程序的创建和部署==。 它通过提供一系列的“Starters”（启动器），自动配置Spring和第三方库，使得开发者可以快速启动和运行Spring应用程序。 ==Spring Boot消除了传统Spring应用程序中繁琐的配置工作，支持自动配置、嵌入式服务器（如Tomcat、Jetty等）、独立运行等特性==。 它非常适合微服务架构，因为它可以快速启动和部署，并且易于与其他微服务集成。 56. CPU 使用率高 有没有频繁的上下问切换, 上下文切换做了两件事, 需要执行CPU内核相关指令, 实现状态的保存和恢复, ==例如文件IO, 网络IO, 锁等待==\n保存,运行中的线程中的执行状态 恢复,让处于等待的线程执行 程序中创建了大量的线程, 或者有线程一直占用CPU 资源 , 比如说死循环\ntop 命令查看 java 进程占用CPU 的情况\njstack 导出进程栈, ==看是否有不恰当的锁使用,有没有频繁的锁资源的争抢, 有频繁的IO 操作, 密集的CPU运算, 或者是频繁的触发full GC,大量日志写入==\n57. mybatis 中的 #{} 和 ${} 的区别 都是用来传递参数的, # 号不会造成sql 注入, $ 符号 将传递的参数值直接替换到SQL语句中，不会进行预处理, 不适用于预处理的场景使用 如, ==动态表名, 列名==\n\u0026lt;!-- 使用预处理参数 --\u0026gt; \u0026lt;select id=\u0026quot;selectUsers\u0026quot; resultType=\u0026quot;User\u0026quot;\u0026gt; SELECT * FROM users WHERE id = #{id} \u0026lt;/select\u0026gt; \u0026lt;!-- 使用字符串替换参数 --\u0026gt; \u0026lt;select id=\u0026quot;selectUsersByTableName\u0026quot; resultType=\u0026quot;User\u0026quot;\u0026gt; SELECT * FROM ${tableName} WHERE id = #{id} \u0026lt;/select\u0026gt; 58. mybatis 如何进行分页 三种方式\n物理分页: select 语句里面加上 页码和每页条数的参数\n逻辑分页: mybatis 提供了 RowBounds 对象, 实现了内存级别的分页, 不依赖于数据库层的分页功能，而是通过 MyBatis 在内存中对结果集进行截取，从而实现分页\n基于mybatis interceptor 拦截器, 对select 语句进行动态的分页关键字的拼接\n59. Mysql索引结构为什么使用B+树 从三个方面看\n了解二叉树, AVL树, B树的概念 B树和B+树的应用场景 为什么使用B树或者B+树来做索引结构 二叉树\n左右两个子树的高度值不能超过1, 所以引入了左旋和右旋的机制\nB树-多路平衡查找树, ==可以有多个子树, 子树的数量取决于关键字的数量==, 可以拥有的子树的数量等于关键字的数量加上1\nB树存储结构\nB+树存储结构\n树节点只存储关键字, 所有数据存储在叶子节点 并且叶子节点的所有数据使用的是双向链表来关联 Innodb ==树的高度决定了磁盘IO的次数, 磁盘IO次数越少, 对于性能的提升就越大==\n存储数据量维度：==B树的数据存储在每个节点上, B + 树内节点仅存索引关键字与指针，叶节点存完整数据记录或指向记录的指针==，相同磁盘页空间可容纳更多索引项，树的阶数更高，索引层次减少，在海量数据存储场景下能高效管理并快速定位数据，像大型电商商品数据表，可轻松应对海量商品信息索引构建与查询需求。 范围查询效能：**B + 树叶节点以链表形式有序连接，范围查询时，从起始点顺链表依次读取即可，无需回溯上层节点，**磁盘 I/O 次数稳定可预测，相比之下，B 树范围查询可能多次回溯上层节点，I/O 操作繁杂，B + 树大幅提升范围查询效率，适用于查询日期区间内订单记录等场景，保障数据有序读取与快速检索。 数据检索速度：B + 树数据存于叶节点，查询路径固定从根至叶，检索时依索引键快速定位叶节点获取数据，缓存命中率因路径稳定而提升，数据读取高效。B 树内节点存数据，检索可能中途获取数据，访问路径多变，缓存利用欠佳，B + 树在频繁数据检索场景，如高频用户信息查询系统中，优势尽显，加快数据读取响应。 全局扫描表现：B + 树叶节点链表有序，全表扫描顺链表遍历即可，高效且有序。B 树无此链表结构，扫描需按层次遍历，节点访问无序易致磁盘频繁寻道，I/O 负担重。在执行统计全库数据量、计算平均值等操作时，B + 树可快速完成，降低系统资源消耗，提升整体运算效率。 避免叶子节点分裂优势：数据插入更新时，B + 树叶节点以页为单位管理，满时分裂以页为单位分配调整，保持叶节点数据紧凑有序。B 树节点分裂按关键字平衡原则，易引发连锁反应，致树结构频繁调整与磁盘 I/O 波动，B + 树分裂策略简单高效，维护索引结构稳定，减少索引维护成本，保障数据库持续稳定运行与高效性能发挥，在高并发数据更新场景中优势显著。 总结\n减少磁盘IO, 同层高存储更多索引数据 范围查询比较高效 全表扫描, 叶子节点存储所有的数据, 只需要扫描叶子节点 自增方面看, 用自增的整型数字作为主键的话, B树会造成叶子节点分裂 60. Mysql 事务实现原理 MySQL 中的事务遵循 ACID 属性，这保证了事务的可靠性和一致性：\n原子性（Atomicity）：事务中的所有操作要么全部完成，要么全部不执行。使用日志管理机制（如二进制日志和事务日志）来确保原子性。 一致性（Consistency）：事务必须使数据库从一个一致性状态转换到另一个一致性状态。==数据的完整性没有被破坏==。 隔离性（Isolation）：多个事务并发执行时，彼此之间的操作不会相互干扰。MySQL 支持多种隔离级别来控制事务之间的可见性。 持久性（Durability）：一旦事务提交，其结果是永久性的，即使系统崩溃也能保留事务的结果。使用持久化机制（如将数据写入磁盘）来确保持久性。 ==存储引擎==\nMySQL 支持多种存储引擎（如 InnoDB、MyISAM 等），不同存储引擎的事务实现机制不同。InnoDB 引擎是支持事务的主要引擎，采用以下机制实现事务：\n==锁机制==\n行级锁：InnoDB 通过使用行级锁来允许多用户并发访问同一数据，降低了锁竞争，提高了并发性能。 间隙锁：用于防止幻读问题，锁定某些范围内的记录。 意向锁：帮助管理行级锁的过程，避免死锁。 ==日志机制==\n重做日志（Redo Log）：用于保证事务持久性。在事务提交时，InnoDB 会将变更写入Redo Log，以便在系统崩溃后恢复数据。\n回滚日志（Undo Log）：实现事务的原子性和隔离性。它记录了事务修改数据的历史，可以使用这些日志进行数据的回滚。\n==多版本并发控制（MVCC）==\nMVCC 是一种常用的并发控制机制 ==MVCC 允许多个事务同时读取和修改数据而不会互相干扰==，从而提高数据库系统的并发性能和响应速度 ，避免了读锁和写锁的竞争。每次修改记录时，创建一个新的版本，读取操作可以读取旧版本，而不必等待写操作完成。InnoDB 通过时间戳来标识事务，支持实现高效的并发访问。\n在执行过程中，事务可能处于以下状态之一：\n活跃（Active）：事务正在执行。 部分提交（Partially Committed）：事务可能已经执行了一部分，但还未完全提交。 失败（Failed）：事务因错误而失败，需要回滚。 已提交（Committed）：事务已成功提交，数据已持久化。 已回滚（Rolled Back）：事务已失败，所有更改被撤消。 MySQL 通过结合 ACID 属性、存储引擎的功能、锁机制、日志机制和 MVCC 等技术来实现事务管理。通过这些机制，MySQL 能够确保在多用户并发环境中数据的完整性和一致性，同时提供高效的性能。\n61. Mysql 事务隔离级别 定义了多个事务之间的可见性，主要用于控制事务之间的数据访问和并发性。\nREAD UNCOMMITTED（未提交读）： 在此级别下，一个事务可以读取另一个未提交事务的数据。 存在 “脏读” 的风险，即读取到的数据可能在后续被回滚。 适合对数据一致性要求不高的场景，但通常不推荐使用。 READ COMMITTED（已提交读）： 事务只能读取已提交事务的数据。 避免了脏读的发生，但仍然可能出现 “不可重复读” 问题，即在同一事务中多次读取同一数据时，结果可能不同。 一般在许多应用中被广泛使用，因为它在平衡一致性和并发性方面提供了较好的选择。 REPEATABLE READ（可重复读）： 在该级别下，同一事务内多次读取同一数据所得到的结果是一致的，即不会发生不可重复读。 此级别是 MySQL 的默认隔离级别。 可能仍然会发生 “幻读” 现象，即在同一事务中，查询的结果集在两次查询之间可能会增加或减少新行。 SERIALIZABLE（可串行化）： 这是最高的事务隔离级别，所有的事务按顺序执行，完全避免了幻读、不可重复读和脏读。 由于强大的数据一致性保障，这一级别会显著降低并发性能，可能导致较高的锁争用和事务阻塞。 适用于对数据一致性要求极高的场景，但通常会影响系统的吞吐量。 READ UNCOMMITTED: 最低隔离级别，允许脏读。 READ COMMITTED: 避免脏读，可能出现不可重复读。 REPEATABLE READ: 避免不可重复读，可能出现幻读（MySQL 默认）。 SERIALIZABLE: 最高隔离级别，避免所有读数据冲突，但性能最差。 62. MVCC MVCC 通过为**每个数据行创建多个版本来实现并发控制**。每个事务在**读取数据时可以看到事务开始时的数据状态，而不是当前的数据状态**。这种方式可以避免读操作与写操作之间的相互阻塞。 每次修改都会创建新的版本：每次执行 INSERT、UPDATE 或 DELETE 操作时，InnoDB 会创建一个新的版本，并通过事务 ID 来管理版本的可见性。\n数据行有多个版本：每个版本包含自己的事务 ID（创建时间戳），以及在删除的情况下，记录删除的事务 ID（删除时间戳）。这些时间戳用于判断事务是否能看到该版本的记录。\nMVCC 通过时间戳控制可见性：\n==在数据库中维护多个版本的数据行，确保高并发环境下的事务隔离性，同时提高并发性能==\n==版本管理==\n在 MVCC 中，每条记录都包含与其版本相关的两个时间戳：\n创建时间戳：表示事务创建时的时间戳，用于标识记录何时被创建。 删除时间戳：表示事务删除记录时的时间戳，用于标识记录何时被逻辑删除。 ==垃圾回收==\n随着事务的执行和数据版本的增加，旧版本的数据会占用越来越多的存储空间。因此，MVCC 需要定期进行垃圾回收，以清理那些不再被任何活动事务引用的旧版本。\n1. 读取数据的机制 读取快照：当事务开始时，MVCC 会为其创建一个数据快照，保存数据行的版本信息。事务在执行查询时，总是读取快照中的数据版本，而非正在更新的版本。 可见性判断：在执行读取操作时，MVCC 会根据事务的时间戳判断哪个版本的记录是可见的： ==只有创建时间戳小于或等于当前事务时间戳，且删除时间戳大于当前事务的记录，才会对当前事务可见==。 2. 写入数据的机制 当事务修改数据时，它不会立即覆盖原有数据，而是创建一个新的版本。旧版本的数据仍然保留，直到没有事务再访问它。从而实现数据的修改而不影响正在读取这些数据的其他事务。 63. 行锁, 临键锁, 间隙锁 锁类型 锁定范围 用途 行锁 单行记录 防止特定行被修改或者写入冲突 临键锁 行记录+相邻间隙 防止幻读和插入 间隙锁 数据间的区间(不含具体行) 仅防止插入 1. 行锁 (记录锁) UPDATE、DELETE或SELECT ... FOR UPDATE的操作\n行锁是一种粒度较小的锁，仅锁定数据库中的某一行记录，从而允许多个事务同时访问表中的不同记录。这种锁粒度小，可以实现高并发的事务处理。 并发性高：相比于表锁，行锁允许更多的事务并发执行，适合高并发访问的场景。 锁定具体行：只有被修改的数据行会被锁定，其它行仍然可以被访问。 可重复读：通过行锁机制，可以防止不可重复读和幻读现象的发生。 2. 临键锁 它结合了行锁和间隙锁\n临键锁是 InnoDB 的一种组合锁，它==既锁定了符合条件的记录（行锁），也锁定了该记录的前一个位置==。这种锁机制通常用于避免幻读的发生。\n范围锁定：在添加或修改记录时，临键锁会锁定一个范围，包括目标行以及该行的上一个间隙，从而避免其他事务在这个范围内插入新记录。 用于范围查询：当进行范围查询时，可以防止其他事务插入新的符合查询条件的行，从而避免幻读。 ==用于REPEATABLE READ隔离级别下的范围查询（如SELECT ... WHERE==\n场景： 当查询范围为id BETWEEN 10 AND 20时，数据库会锁住符合条件的行以及这些行之间的间隙，防止其他事务插入新行（比如id=15）\n3. 间隙锁 间隙锁是一种锁定在某一记录之间的空间，而不是锁定具体记录。它用于锁定 “间隙”，防止其他事务在此间隙中插入新记录，从而避免幻读现象。\n==特点==\n锁定间隙：在不同记录之间的空隙处添加锁，阻止其他事务在这个空间内插入新数据。 适用于防止幻读：当应用了范围查询时，可以有效防止插入幻读的发生。 不能直接访问：间隙锁不会锁定具体的行，而是锁定行之间的空间。 64. Mysql索引失效的情况 使用了不支持索引的表达式（如计算字段） 不适当的 LIKE 操作 OR 条件的使用 数据类型不匹配 使用了 NULL 值进行比较 函数或操作导致索引失效 缺乏合适的索引覆盖。 在索引列上做运算, 比如说加函数, Mysql 在生成执行计划的时候会根据统计信息判断, 是否要去使用索引的, 加了函数运算, 导致mysql 无法识别索引列, 不过从8.0开始Mysql 加了函数索引来解决这个问题\n在一个多列构成的组合索引里面, 需要按照最左匹配原则,\n最左匹配原则的定义 最左匹配原则是 MySQL 在使用联合索引（即由多个列组成的索引）时遵循的一个重要原则。==它指的是在查询条件中使用联合索引时，索引的匹配是从最左边的列开始的，并且要按照索引列的顺序依次进行匹配==。只有当查询条件中使用了联合索引最左边的列，索引才会被部分使用；如果从最左边开始连续的列都在查询条件中使用，那么索引可以被更充分地利用。 联合索引的存储结构基础 联合索引在 MySQL 的存储引擎（如 InnoDB）中是按照索引列的顺序构建 B + 树的。以一个包含列 A、B、C 的联合索引为例，在 B + 树的叶子节点中，数据是按照 A、B、C 列的值进行排序存储的。最左列 A 的值首先被用来构建索引树的第一层排序，对于 A 列值相同的记录，再根据 B 列的值进行排序，以此类推。 索引列存在隐式转换, 索引列是字符串类型, 但是sql 查询的时候没有使用引号, mysql 就会去做自动类型转换, 导致索引失效, 常见的隐式转换\n数值与字符串 字符串和日期 数字和布尔值 在索引列使用不等于号, 或者 not\nlike 操作 使用左通配符\nor 当 OR 的左右两边的条件没有共同的索引时。\n1. 聚簇索引（Clustered Index） 定义：\n聚簇索引是将表的数据行与索引行按照相同的顺序存储在一起的一种索引方式。一个表只能有一个聚簇索引，因为表的数据行只能按照一种顺序存储。\n特点：\n索引和数据存储在一起： 聚簇索引的叶子节点存储的是数据本身。 索引值与数据行的物理存储顺序一致。 主键默认作为聚簇索引： 如果表定义了主键，MySQL（InnoDB 存储引擎）会默认将主键作为聚簇索引。 如果没有主键，MySQL 会选择一个唯一非空索引作为聚簇索引。 如果既没有主键也没有唯一非空索引，MySQL 会生成一个隐藏的聚簇索引。 查询效率高： 查询基于主键的范围时性能更高，因为数据已经按照主键的顺序存储。 优点：\n查询主键或主键范围时速度很快。 数据存储与索引顺序一致，减少了磁盘 I/O。 缺点：\n插入速度可能较慢，尤其是主键非连续的情况下（需要重新排列存储位置）。 需要更新数据时，可能涉及更多的磁盘操作。 聚簇索引可能导致数据存储碎片。 2. 非聚簇索引（Non-Clustered Index） 定义：\n非聚簇索引是指索引结构与数据存储是分离的，索引的叶子节点存储的是数据行的指针（如主键或行号），而不是实际数据。\n特点：\n索引与数据分离： 非聚簇索引的叶子节点包含指向数据行的引用（如主键值或 ROWID）。 索引结构与表数据分开存储。 可以有多个非聚簇索引： 一个表可以有多个非聚簇索引，每个非聚簇索引对应一个独立的索引结构。 查询需要回表： 使用非聚簇索引查询时，可能需要通过索引找到对应的主键或行号，然后再回到聚簇索引中查找实际数据，这一过程称为回表。 优点：\n可以为不同的列创建多个非聚簇索引，灵活支持多种查询。 不受数据行存储顺序的限制。 缺点：\n查询非主键列时，可能需要回表操作，导致性能开销增加。 索引维护开销较高（如插入、删除、更新时需要更新索引结构）。 3. 总结 InnoDB 默认使用聚簇索引：\nInnoDB 存储引擎默认将主键作为聚簇索引。 MyISAM 存储引擎不支持聚簇索引，所有索引都是非聚簇索引。 回表操作：\n如果查询字段不在非聚簇索引中，需要通过索引的指针回到聚簇索引中查找数据。 覆盖索引：\n如果非聚簇索引包含了查询需要的所有列，则可以避免回表操作，提高查询效率。 索引选择：\n主键列适合作为聚簇索引。 对频繁查询但不作为主键的列，可以建立非聚簇索引 聚簇索引：数据和索引存储在一起，主键查询效率高，适用于频繁的范围查询和排序。\n非聚簇索引：数据和索引分离，支持多列查询，但可能需要回表操作。\n65. MyISAM 和 InnoDB 引擎的区别 存储引擎\n定义数据的存储方式 数据读取的实现逻辑 MyISAM 两个文件\n.MYD 存数据 .MYI 存索引 Innodb 只有一个文件 ibd, 包含了索引和数据\n数据存储不同 事务的支持 锁的支持 Innodb 支持外键 66. MD5 值应该用char 还是varchar 因为MD5 长度是固定的, 32或者16位\n占用空间不同, char 是固定的, varchar 是可变的 存储效率不同, varchar 每次更新都需要更新存储空间的长度, 效率较低 67. update 语句用什么锁 取决于执行的条件where , 事务隔离级别\n行锁情况：在 InnoDB 存储引擎下，默认事务隔离级别为可重复读（Repeatable Read）时，UPDATE语句通常施加行级锁。当更新操作涉及特定行数据时，仅锁住该行，不同事务可并发更新其他行，减少锁冲突、提升并发性能。 例如，在员工信息表中， 事务 T1 更新员工 A 的薪资， 事务 T2 更新员工 B 的薪资， 两者可并行，互不干扰。 此机制利用记录中的隐藏字段及索引实现精准锁定，确保数据一致性与事务隔离性，在高并发的 OLTP（联机事务处理）场景中优势显著，如电商订单处理、金融交易系统等频繁修改少量行数据的应用场景。 表锁情况：若未使用索引或索引失效，InnoDB 可能退化为表锁。 如在数据量小且频繁全表更新的表中，使用UPDATE语句更新大量行数据（超表行数一定比例），InnoDB 为保证事务一致性与操作简便性，可能对整个表加锁，虽牺牲并发性能，但避免复杂锁管理开销。 MyISAM 存储引擎执行UPDATE语句一般用表锁，因其设计侧重简单高效，适用于读多写少、对并发要求不高的场景，如数据仓库中的历史数据报表生成场景，频繁全表更新时表锁可快速完成操作，减少资源竞争与事务协调成本。 join不要超过三张表\n性能问题 维护性, 可读性 68. Mysql 性能优化 查询优化：合理使用索引，避免全表扫描，Explain 优化 SQL 语句, 慢查询优化\n数据库配置优化：调整 MySQL 配置参数（如缓存、连接池、日志等）以提升性能。\nInnoDB Buffer Pool：innodb_buffer_pool_size 是最关键的参数之一，它决定了 InnoDB 缓存的数据页的大小。如果它设置得过小，会导致磁盘 I/O 增加。 ==专用数据库服务器总内存的 70%-80%== 硬件优化：使用 SSD、调整 RAID 配置等提升硬件性能。\n架构优化：使用数据库分区、读写分离、分库分表等方法提高数据库扩展性和并发能力。\n69. Spring boot 约定大于配置 繁琐的配置\n管理jar包依赖 web.xml 文件维护 Dispatch-Servlet.xml 文件维护 应用部署到web容器 第三方组件, Mybatis 集成 需要维护配置文件 约定大于配置的体现\nspring boot 会自动为你处理常用依赖的版本, 例如引入 spring-boot-starter-web 会依赖 spring-web 和 spring-webmvc，并且这些库的版本会自动和 Spring Boot 的版本兼容。 Spring boot 自动装配机制, 通过扫描约定路径下的spring.factories 文件, 用来识别配置类, 从而实现Bean的自动装载 配置文件 每个 Starter 都是一个预定义的依赖集合，包含了默认的库和配置：\nspring-boot-starter-web：包含了开发 Web 应用所需的常用库和配置，比如 spring-webmvc、Tomcat、Jackson 等，开发者只需添加这个 Starter 就能自动获得这些功能。 spring-boot-starter-data-jpa：为 JPA 项目提供默认的配置，自动配置数据源、实体管理器工厂、事务管理等。 70. Spring boot 是如何实现自动配置的 自动装配: 自动把第三方的bean装载到 Spring IOC 容器里面, 不需要再去写Bean 相关的一些配置\n@SpringBootApplication 里面的 @EnableAutoConfiguration 注解 是实现自动装配的关键\n自动装配的实现主要依靠\n引入starter 启动依赖组件的时候, 这个组件里面必须包含一个 @Configuration 配置类, 配置类里面需要通过 @Bean 注解声明需要装载到 IOC 容器的对象 这个配置类是在第三方的 jar 包里面, 这个类的全类路径放在 classpath/META-INFO/spring.factories 中, 这样 spring boot 就能知道 第三方jar 包的这个配置类的位置, 这个步骤是通过 SpringFactoriesLoader 来完成的 Spring boot拿到所有的配置类之后, 通过 @Conditional 注解条件判断之后 再 通过 ImportSelector 这样一个接口 来实现对这些配置类的动态加载, ImportSelector 是 Spring 框架中的一个接口, AutoConfigurationImportSelector 是springboot 的继承了该接口的实现类，用于在==配置类中实现动态导入其他配置类的功能== ==@Enable 注解就是用来帮助我们对这个模块的bean 的自动注入的 思想相关的注解==\nImportSelector 工作原理\n当在一个带有@Configuration注解的类上使用@Import注解并指定一个实现了ImportSelector接口的类时，Spring 会在容器初始化阶段调用该接口的selectImports方法。这个方法返回一个包含要导入的配置类全限定名的字符串数组。Spring 会根据这些返回的类名，将对应的配置类加载到容器中。 71. starter 的作用 会把所有需要的jar 包给全部倒入进来, 并且jar 包版本自动管理\nstarter 可以自动装配\n所有自定义的配置都可以集成到 spring boot, 只需要通过维护 applicatio.yml 文件就可以了\n72. @Conditional 注解的作用 条件化装配机制, 允许在特定条件满足时加载或者跳过 Bean的定义与配置\n可以接受一个或者多个实现了 Condition 接口的类, Condition 接口里面\n@FunctionalInterface public interface Condition { boolean matches(ConditionContext var1, AnnotatedTypeMetadata var2); } 作用：基于条件动态配置 Bean，增强应用的灵活性与可扩展性。\n使用场景：环境适配、属性控制、自动配置等。\n设计理念：关注点分离，避免硬编码，提升可维护性与可读性。\n73. Spring Aop的原理 动态代理是 运行时动态生成的, 基于Java 反射机制或者字节码操作, ==代理对象是在运行时在 JVM 内存中生成的, 在第一次使用的时候生成的==, 通过文件系统是看不到的\n切面 Aspect, ==业务场景日志拦截,事务管理,权限校验== 通知 Advice, ==决定了拦截方法的前后或其他状态，具体增强代码写在哪里由== 前置通知 后置通知 环绕通知 返回通知 异常通知 连接点, Joinpoint, ==代码执行过程中可以被切入的点, (方法调用),拦截的是方法的话就是被拦截的方法就是连接点,包含的内容== 方法签名：即方法的包路径、类名和方法名。 方法参数：目标方法的参数值。 目标对象：被代理的具体实例。 代理对象：AOP 生成的代理对象。 切点, PointCut, 定义在连接点上织入通知的表达式 织入 Weaving , 动态代理完成 public void getUser() { System.out.println(\u0026quot;Getting user...\u0026quot;); } @Aspect @Component public class LoggingAspect { @Before(\u0026quot;execution(* com.example.service.*.*(..))\u0026quot;) public void logBefore(JoinPoint joinPoint) { System.out.println(\u0026quot;拦截的方法签名：\u0026quot; + joinPoint.getSignature()); // 获取方法签名 System.out.println(\u0026quot;目标对象：\u0026quot; + joinPoint.getTarget()); // 获取目标对象 System.out.println(\u0026quot;方法参数：\u0026quot; + Arrays.toString(joinPoint.getArgs())); // 获取参数 } } 是基于动态代理 ( JDK, CGLIB ) 实现的\n如果目标对象的实现类实现了接口，Spring AOP 将会采用 JDK 动态代理来生成 AOP 代理类；\n如果目标对象的实现类没有实现接口，Spring AOP 将会采用 CGLIB 来生成 AOP 代理类——不过这个选择过程对开发者完全透明、开发者也无需关心\nAspectJ 是专门为 Java 语言设计的 AOP 框架，它基于 Java 语言进行扩展，提供了强大的面向切面编程（AOP）能力。然而，它的核心原理和运行机制使它能够在一定程度上支持其他与 Java 生态兼容的语言，如 Kotlin 和 Groovy，但这种支持是有限的。\n四个步骤\n创建代理对象 拦截目标对象 调用代理对象 调用目标对象 代理对象: Spring 代理策略生成的对象 目标对象: 业务代码 织入代码: 就是代理方法的代码 切面通知: 封装织入代码片段的回调方法 MethodInvocation: 负责执行拦截器链, 在processd() 方法中执行 1. 创建代理目标 ==按需加载==只有当被代理的 Bean 被注入到容器或首次使用时，Spring 才会为其创建代理对象\n在Spring 中创建Bean 实例都是从getBean() 方法开始的 , 在实例创建之后, Spring IOC 容器将会根据AOP 的配置, 去匹配目标类的类名, 看目标类的类名是否满足切面规则\n调用ProxyFactory 创建代理bean , 并且缓存到IOC 容器中, 根据目标对象会选择不同的代理策略\n代理类实现了接口用jdk代理 没有实现就用CGlib 代理 ==也可以通过配置, 强制Spring 去使用 CGlib 代理==\nJDK 动态代理：基于接口的代理，通过 Java 的 Proxy.newProxyInstance 动态生成代理对象。 CGLIB 动态代理：基于子类的代理，通过生成目标类的子类字节码来实现，运行时生成子类。 2. 拦截目标对象 当用户调用目标对象的某个方法的时候, 将会被 AopProxy 的对象拦截, Spring 将所有的调用策略封装到了该对象\n​\tAopProxy 有两种实现\nJdkDynamicAopProxy ObjenesisCglibAopProxy 3. 调用代理对象阶段 Spring Aop 拦截器链中每个元素都会被命名为 MethodInterceptor, ==其实就是切面中的 Advice 通知==, 被织入的代码片段, 会在这个阶段被执行\n拦截器链的典型结构\n假设有以下增强：\n一个前置通知（@Before） 一个后置通知（@After） 一个环绕通知（@Around） ==调用过程==\n调用代理方法 -\u0026gt; 环绕通知（进入前） -\u0026gt; 前置通知 -\u0026gt; 调用目标方法 -\u0026gt; 后置通知 -\u0026gt; 环绕通知（退出后） -\u0026gt; 返回结果 74. Spring Bean的定义 三个阶段\nclass：指定Bean的实现类。Spring容器会使用这个类来创建Bean实例。 scope：定义Bean的作用域。常见的作用域包括单例（singleton）、原型（prototype）、会话（session）等。 lazy-init：一个布尔值，用于指定Bean是否应该延迟初始化。如果设置为true，Bean在首次使用时才会被创建。 depends-on：指定Bean在创建之前需要先创建的其他Bean。这用于处理Bean之间的依赖关系。 name 或 id：Bean的名称或ID，用于在Spring容器中唯一标识一个Bean,并且不能以大写字母开头的。 constructor-arg：没有无惨构造的话, 此属性是必须的, 用于指定构造函数参数，以便在创建Bean实例时传递给构造函数。 properties：用于设置Bean的属性值。这些属性值可以在Bean创建后通过setter方法设置。 init-method：指定初始化方法，该方法在Bean创建并设置完所有属性后被调用。 destroy-method：指定销毁方法，该方法在Bean即将从Spring容器中移除时被调用，用于执行清理工作。 75. Spring Bean的生命周期 BeanFactory 是 Spring 框架中的 核心接口，它是 IOC 容器（Inversion of Control，控制反转容器） 的==基础实现==，负责管理 Bean 的创建、初始化、生命周期和依赖注入等。\nAware 接口是一组特定的接口，它们使得 Spring Bean 能够在初始化阶段获得 Spring 容器的某些核心功能。通过这些接口，Spring Bean 可以访问容器的一些关键组件或信息（如 ApplicationContext、BeanFactory、Environment 等）\n假设我们有一个 MyBean 类，它需要访问 ApplicationContext。我们可以实现 ApplicationContextAware 接口来实现这一点\n@Component public class MyBean implements ApplicationContextAware { private ApplicationContext applicationContext; // 通过实现 ApplicationContextAware 接口，注入 ApplicationContext @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException { this.applicationContext = applicationContext; } public void printBeanDefinitionNames() { // 使用获取到的 ApplicationContext 打印 Bean 的定义 String[] beanDefinitionNames = applicationContext.getBeanDefinitionNames(); System.out.println(\u0026quot;Bean definitions in the context: \u0026quot; + Arrays.toString(beanDefinitionNames)); } } 五个阶段\n创建前 准备阶段 创建实例阶段 依赖注入阶段 容器缓存阶段 销毁实例阶段 BeanFactoryPostProcessor 有什么用? 和 BeanPostProcessor 有什么区别 BeanFactoryPostProcessor BeanFactoryPostProcessor 会在 Bean 实例化之前执行，它是在 BeanFactory 中的 BeanDefinition 完成加载并且准备创建 Bean 之前执行的。 它的执行顺序发生在 Spring 容器初始化阶段的早期，目标是修改容器中的 Bean 定义，而不是 Bean 的实例 修改 Bean 的配置，例如修改某个 Bean 的 scope、lazy 属性。 在容器启动时调整 Bean 定义，以便动态改变容器的行为。 BeanPostProcessor BeanPostProcessor 会在 Bean 实例化之后、初始化之前、以及 初始化之后 的不同阶段执行。 为 Bean 添加一些功能（如 AOP 代理、日志记录、性能监控等）。 修改 Bean 实例的属性、字段等（比如为某个 Bean 动态注入值） dubbo 是怎么通过BeanPostProcessor 对Bean 进行拓展的 Dubbo 发现一个 @Service 注解标记的 Bean 时，它会通过 BeanPostProcessor 来增强该 Bean，把它暴露为一个 Dubbo 服务。 Dubbo 发现一个 @Reference 注解标记的 Bean 时，它会通过 BeanPostProcessor 来动态创建该 Bean 的代理，代理后的 Bean 将会负责与 Dubbo 服务端进行远程调用。 1. 加载配置文件或注解 过程：Spring 通过 BeanDefinitionReader 解析配置文件（如 XML）或注解（如 @Configuration、@Component）。 作用：将每个 Bean 的定义（BeanDefinition）加载到容器中。 图中位置：左侧的 BeanDefinition Reader 模块，解析了 XML 和注解，形成 BeanDefinition。 2. BeanDefinition 处理 过程: Spring 使用 BeanFactoryPostProcessor对 BeanDefinition进行修改或扩展。 ==比如修改 Bean 的属性值，或注册新的 Bean==。 作用：在 Bean 实例化之前进行全局的配置调整。 图中位置：BeanFactoryPostProcessor 模块，用于处理 BeanDefinition 的相关信息。 3. Bean 实例化 过程：Spring 容器通过反射，根据 BeanDefinition 创建 Bean 的原始实例（未进行属性填充）。 作用：将 Bean 加载到内存中，分配内存空间。 图中位置：右侧的 “实例化” 流程。 4. 属性填充 过程: Spring 使用依赖注入（DI），将 Bean 所需的依赖注入到对象中。\n​\t包括通过==构造方法、Setter 方法或字段注入依赖==。\n作用：完成 Bean 的依赖配置，确保它可以正常工作。\n图中位置：初始化阶段中的 “填充属性”。\n5. 设置 Aware 接口属性 过程：如果 Bean 实现了某些特殊接口（如 BeanNameAware、ApplicationContextAware），Spring 会调用这些方法，==传递容器相关的上下文信息==22。 作用：让 Bean 感知容器环境，比如 Bean 的名称或 ApplicationContext。 图中位置：设置 Aware 接口的属性。 6. 调用 BeanPostProcessor 的 postProcessBeforeInitialization 过程：Spring 调用所有注册的 BeanPostProcessor 的 postProcessBeforeInitialization方法，对 Bean 进行前置增强处理。 比如自定义一些逻辑，如动态修改 Bean 的某些属性。 作用：允许开发者在 Bean 初始化之前插入额外逻辑。 图中位置：BeanPostProcessor#before。 7. Bean 初始化 过程： 如果 Bean 实现了 InitializingBean 接口，会调用其 afterPropertiesSet() 方法。 如果在配置中指定了 init-method，Spring 也会调用该方法。 作用：初始化 Bean，完成它的内部逻辑配置。 图中位置：initializingBean#afterPropertiesSet 和 执行init-method方法。 8. 调用 BeanPostProcessor 的 postProcessAfterInitialization 过程：Spring 调用所有注册的 BeanPostProcessor 的 postProcessAfterInitialization方法。 比如在这个阶段进行代理逻辑的实现（AOP）。 作用：为 Bean 添加更多增强功能（如代理）。 图中位置：BeanPostProcessor#after。 9. Bean 完成初始化（Ready to Use） 过程：Bean 已经完成所有初始化流程，Spring 容器将其标记为可用。 作用：Bean 可以被其他组件调用或使用。 图中位置：完成对象 -\u0026gt; 添加到 BeanFactory 的单例池中。 10. 销毁（当容器关闭时） 过程： 如果 Bean 实现了 DisposableBean 接口，Spring 会调用其 destroy() 方法。 如果配置了 destroy-method，Spring 也会执行该方法。 作用：清理资源，确保 Bean 的生命周期完整结束。 图中位置：此阶段未在图中标明，但与初始化的对称操作类似。 76. Spring 里面可以存在Id 相同的两个Bean吗? 同一个配置文件不可以\n不同的配置文件可以, ==后加载的会覆盖前面加载的==\nID 相同的 Bean 会导致冲突，在 Spring 配置（XML 或注解）中不能直接定义多个 ID 相同的 Bean。\n如果有多个 Bean 的 ID 相同，Spring 会根据注解或配置使用 @Primary 或 @Qualifier 来确定注入哪个 Bean。\n==@Qualifier 注解来指定注入特定的 Bean 实例==。通过这种方式，你可以在同一个上下文中使用多个相同类型的 Bean，只要它们有不同的 id 或 name。 多个相同类型 Bean 的 ID 冲突 可能会抛出 NoUniqueBeanDefinitionException，这时需要明确指定 Bean 的 ID 或使用 @Qualifier。\n@Scope(\u0026quot;prototype\u0026quot;) 类型的 Bean 虽然 ID 相同，但每次请求都会返回一个新的实例。\n使用 @Configuration 类时，==多个同名的 Bean 只会加载第一个==\n总之，Spring 要求 Bean 的 ID 必须是唯一的，只有在符合特定规则（如使用 @Primary、@Qualifier）的情况下，才能避免 ID 冲突问题。\n77. Spring 如何解决循环依赖的问题 spring 循环依赖的问题是在Bean哪个生命周期的步骤中产生的问题\n产生循环依赖的情况有哪些?\n互相依赖\n间接依赖, 循环调用\n自我依赖\n三级缓存\n一级缓存存放的是完全被初始化好的bean 二级缓存, 存放原始Bean, 属性还没有被赋值, 没有被依赖注入 三级缓存放的是 BeanFactory 对象, 用来生成原始Bean 对象并存放到二级缓存里面 解决这个问题的==核心思想就是把bean的实例化和bean的属性依赖注入的这个过程给分离开来==\n步骤 操作描述 一级缓存（singletonObjects） 二级缓存（earlySingletonObjects） 三级缓存（singletonFactories） 1 Spring 创建 Bean A，发现需要注入 B，进入创建 B 的流程。 A 的工厂方法进入三级缓存 2 创建 B，发现需要注入 A，检查是否存在已创建的 A。 A 的工厂方法存在 3 从三级缓存中获取 A 的工厂，并通过工厂方法生成早期的 A 半成品，加入二级缓存。 A 半成品 4 B 完成依赖注入（引用 A 的半成品）并初始化完成，放入一级缓存。 B A 半成品 5 回到 A 的创建流程，发现 B 已完成注入，继续初始化 A 并完成创建，放入一级缓存。 B, A spring本身只能解决单实例存在的循环引用问题\n以下四种情况需要人为干预\n多实例的 Setter 注入导致的循环依赖\n构造器注入导致的循环依赖\n==注解方式：使用 @Autowired 标注在构造器上。== 使用构造器注入时，如果两个 Bean 相互依赖（A 的构造器需要 B，B 的构造器又需要 A），会导致循环依赖。 ==使用 @Lazy 注解延迟初始化==。这样可以让某个依赖在实际需要时才加载，从而避免循环依赖。 @DependsOn 导致的循环依赖 , ==注解显式指定 Bean 的加载顺序时==，如果两个 Bean 的加载顺序互相依赖，可能导致循环依赖问题。\n单例代理对象的 Setter 注入导致的循环依赖\n单例代理对象通过 Setter 注入另一个 Bean，而这个被注入的 Bean 又依赖于==代理对象==本身，会导致循环依赖。\n解决方式: 使用 @Lazy 延迟加载，避免在 Bean 初始化阶段立即触发依赖。\n或者使用 @DependsOn 注解，显式指定加载顺序，确保代理对象的依赖在正确的时机被加载\n1. 为什么必须要三级缓存 主要原因是 动态代理 和 增强机制。下面从几个关键点分析：\n二级缓存的局限性\n如果一个 ==Bean 在创建过程中需要进行动态代理（如 AOP 增强）==，它的==最终实例不是原始对象，而是动态代理对象==。\n==如果只用二级缓存（直接存储半成品对象），Spring 无法提前暴露代理后的对象，导致依赖的对象拿到的不是最终增强后的实例==。\n如果检测到可能存在循环依赖，Spring 会将该 Bean 的 \u0026ldquo;早期引用\u0026rdquo; 放入三级缓存。\n这个早期引用是通过 ObjectFactory 提供的，可以延迟加载。如果需要代理，这里生成的是代理对象。\n@Component public class A { @Autowired private B b; } @Component public class B { @Autowired private A a; } 假设 A 是需要动态代理的（如使用了 @Transactional）。 在二级缓存中，A 的原始对象被提前暴露给 B，但 B 需要的是增强后的代理对象。 二级缓存只能存储原始对象，因此无法满足需求。 ​\t通过 ObjectFactory 延迟生成 Bean 的最终形式（如动态代理对象），确保注入的对象是完整可用的。\n必要性：==在支持动态代理、AOP 增强等场景时,三级缓存是解决循环依赖的关键机制==\n78. Spring BeanFactory 和 FactoryBean的区别 BeanFactory\u0026ndash; ==是容器本身,用来管理Bean生命周期和依赖注入==\nBeanFactory 是 Spring 的 IoC 容器的核心接口，是 Spring 中用于管理和提供 Bean 的基础容器。 它负责创建、管理和检索 Bean Factorybean 不是容器，而是一个 Bean 类型。 用于定义复杂对象的创建逻辑\n实现了 FactoryBean 接口的类可以自定义生成某种对象。 常见的使用场景是创建复杂对象，如动态代理、ORM 框架中的 SessionFactory 等。 二者的关系是：BeanFactory 可以管理 FactoryBean，并通过 getObject() 方法返回 FactoryBean 创建的对象。\n79. Spring 事务 PROPAGATION_REQUIRED ，==默认==的spring事务传播级别，==如果上下文中已经存在事务，那么就加入到事务中执行，如果当前上下文中不存在事务，则新建事务执行==。所以这个级别通常能满足处理大多数的业务场景。\nPROPAGATION_SUPPORTS ，==如果上下文存在事务，则支持事务加入事务，如果没有事务，则使用非事务的方式执行==。所以说，并非所有的包在transactionTemplate.execute中的代码都会有事务支持。这个通常是用来处理那些并非原子性的非核心业务逻辑操作。应用场景较少。\nPROPAGATION_MANDATORY（强制） ， ==该级别的事务要求上下文中必须要存在事务，否则就会抛出异常！==配置该方式的传播级别是有效的控制上下文调用代码遗漏添加事务控制的保证手段。比如一段代码不能单独被调用执行，但是一旦被调用，就必须有事务包含的情况，就可以使用这个传播级别。\nPROPAGATION_REQUIRES_NEW ，==每次都会新建一个事务，并且同时将上下文中的事务挂起，执行当前新建事务完成以后，上下文事务恢复再执行==\n这是一个很有用的传播级别，举一个应用场景：现在有一个发送100个红包的操作，在发送之前，要做一些系统的初始化、验证、数据记录操作，然后发送100封红包，然后再记录发送日志，发送日志要求100%的准确，如果日志不准确，那么整个父事务逻辑需要回滚。 怎么处理整个业务需求呢？就是通过这个PROPAGATION_REQUIRES_NEW 级别的事务传播控制就可以完成。发送红包的子事务不会直接影响到父事务的提交和回滚。 PROPAGATION_NOT_SUPPORTED ，当前级别的特点就是上下文中存在事务，==则挂起事务，执行当前逻辑，结束后恢复上下文的事务==。\nPROPAGATION_NEVER , 不允许当前方法在事务中运行 ，就抛出runtime异常，强制停止执行！\nPROPAGATION_NESTED ，字面也可知道，nested，嵌套级别事务。该传播级别特征是，如果上下文中存在事务，则==嵌套事务==执行，如果不存在事务，则新建事务。\n还是子事务先提交，父事务再提交, 子事务是父事务的一部分，由父事务统一提交。 回滚特性 主事务和嵌套事务属于同一个事务 嵌套事务出错回滚不会影响到主事务 主事务回滚会将嵌套事务一起回滚了 require, 默认, 如果有则加入, 没有则新建\nrequire_new, 新建事务, 挂起上下文事务, 先执行自己的, 不会因为子事务异常回滚父事务\nnested, 不存在会新建, 嵌套事务, 嵌套事务异常 ,主事务不会回滚, 主事务异常会全部回滚\nsupport, 有则加入, 没有就非事务方式执行\nnot_support, 不支持, 有也不会执行事务\nmandatory, 强制, 上下文中必须存在事务, 否则抛异常\nnever, 强制非事务执行, 有就抛异常\n传播行为 当前有事务 当前无事务 应用场景 REQUIRED 加入当前事务 新建事务 默认行为，绝大多数场景适用 SUPPORTS 加入当前事务 非事务方式运行 查询等对事务依赖不强的场景 MANDATORY 加入当前事务 抛异常 必须在事务中运行的场景 REQUIRES_NEW 挂起当前事务，新建事务 新建事务 独立子事务，例如日志或补偿事务 NOT_SUPPORTED 挂起当前事务，非事务运行 非事务方式运行 非事务操作，例如批量操作或查询 NEVER 抛异常 非事务方式运行 明确不允许事务运行的场景 NESTED 创建嵌套事务 新建事务 子事务与父事务部分独立且可单独回滚的场景 80. Spring Bean注入IOC容器的方式 注入方式 特点 适用场景 注解（@Component） 简洁高效，自动扫描管理 大部分场景，特别是简单项目 Java 配置（@Bean） 灵活、可定制 需要复杂创建逻辑或条件注入 XML 配置 可配置性强，但冗长 早期项目或对注解不支持的环境 FactoryBean 动态创建复杂 Bean 需要创建动态或代理对象 @Conditional 条件化注入 环境依赖、动态注入场景 @Profile 基于环境配置 多环境切换（如开发、生产） 动态注册 编程式，灵活 特殊场景，如运行时动态加载 Bean 使用XML 使用@CompontScan 注解 扫描 @Controller @Configuration 声明配置类, @Bean 实现Bean 定义, 这种方式是 Xml 配置的一种演变 @Import 注解, 导入配置类或者普通的bean FactoryBean 工厂Bean, 动态构建Bean 实例, Spring cloud open feign 里的 动态代理实例, 就是基于它 实现 ImportSelector 接口, Spring boot starter自动装配的时候有用到 81. 过滤器和拦截器的区别 维度 过滤器（Filter） 拦截器（Interceptor） 依赖规范 Java EE 规范的一部分，==依赖 Servlet API==。 Spring 或其他框架的扩展机制，==依赖 Spring 容器==。 作用范围 全局 HTTP 请求和响应。 Spring MVC 层（控制器及之后的逻辑层）。 触发时机 Servlet 执行前后。 控制器方法执行前后，以及请求完成之后。 访问能力 无法直接访问 Controller、Service 的信息。 可以访问 Controller 的上下文和业务逻辑层的返回值。 运行顺序不同：过滤器在 Servlet 容器接收到请求后、Servlet 被调用前运行；拦截器在 Servlet 被调用后、响应发送到客户端前运行。 依赖关系不同：过滤器依赖 Servlet 容器；拦截器不依赖 Servlet 容器。 操作对象不同：过滤器只能对 request 和 response 操作；拦截器可对 request、response、handle、model and view、exception 操作。 82. Spring Mvc 执行流程 客户端请求 ↓ DispatcherServlet(前端控制器) 作为中央调度器，负责将请求分发到具体的处理组件。 HttpServletRequest 和 HttpServletResponse 对象通常是由 Web 容器在请求到达 DispatcherServlet 时自动创建的 ↓ HandlerMapping → 查找合适的 Controller ↓ HandlerAdapter → 反射调用 Controller 方法 ↓ Controller 返回 ModelAndView 或 视图名 ↓ ViewResolver →视图解析器, 解析视图名，找到具体视图 ↓ 渲染视图 → 返回 HTML 或其他格式 ↓ 客户端接收响应，渲染页面 83. Spring IOC 原理 Spring IoC 的核心原理就是**通过容器管理 Bean 的生命周期、依赖关系和初始化过程。**容器通过依赖注入来==解耦各个 Bean 之间的关系，减少了类与类之间的紧耦合，提高了应用程序的可维护性和扩展性==。整个过程依赖于反射、工厂模式、策略模式等设计模式。\n通过 IoC 和 DI，Spring 实现了松耦合的编程模型，使得 Bean 的创建和管理交由容器负责，开发人员只需关注业务逻辑。\nIoC 控制反转 是==将对象的管理责任交给容器，而非由程序显式地管理==。\nDI 依赖注入 , 将类的依赖（通常是其他类的实例）注入到目标类中 , ==构造器注入、Setter 方法注入和字段注入==\nBeanFactory：最基础的容器，通常用于轻量级的应用。BeanFactory 负责 Bean 的创建和管理，但其不处理任何 Bean 的生命周期管理和事件机制。\nApplicationContext：是 BeanFactory 的==子接口，扩展了更多功能（例如事件传播、AOP 支持、国际化等）==。ApplicationContext 是 Spring 最常用的容器，具有更加丰富的功能。\n84. Spring Mvc的理解 本质上是一个web框架, 基于servlet, servlet 是一个基础的技术规范, 提供了处理请求和响应的基本能力, mvc 在 此基础上进行了扩展，封装了请求分发、处理和响应的流程，通过 DispatcherServlet 来管理请求的流转和控制。\n把传统MVC框架做了拆分 前端控制器 DispatcherServlet 后端控制器 Controller Model 模型 拆分, Model 是负责应用程序的数据和业务逻辑。 业务层service 数据访问层 Repository 85. Spring Bean 作用域 作用域的主要作用是==保护bean的使用安全==\nSingleton：\n应用场景：常见于服务层 Bean、工具类、数据库连接池等需要全局共享的 Bean。 优点：内存占用小，性能好。 缺点：所有请求共享同一个实例，可能导致线程安全问题。 Prototype：\n应用场景：适用于每次操作需要一个新的实例的情况，例如处理不同用户的请求时，每次都需要一个新的 Bean。 优点：能够为每次请求创建独立的 Bean 实例。 缺点：可能增加性能开销，因为每次都会创建新的实例，且 Spring 容器不会管理其生命周期。 Request：\n应用场景：用于 Web 应用程序中，每次请求需要一个新的 Bean 实例，适合于请求级别的状态管理（如分页信息、表单提交数据等）。 优点：Bean 在请求结束后会被销毁，有效隔离了不同请求的数据。 缺点：仅适用于 Web 环境。 Session：\n应用场景：用于 Web 应用程序中，每个用户会话需要一个独立的 Bean 实例，适合于保存用户的会话状态（如购物车、用户登录信息等）。 优点：可以跨多个请求保持用户的状态。 缺点：会话结束后，Bean 会被销毁。如果会话过多，可能导致内存压力。 Application：\n应用场景：用于 Web 应用程序中跨整个应用共享的 Bean，比如缓存、全局配置等。 优点：适用于全局共享的数据和资源。 缺点：如果 Bean 状态被修改，可能会影响到应用的其他部分。 WebSocket：\n应用场景：适用于 WebSocket 连接中的 Bean 状态管理，支持为每个 WebSocket 连接创建独立的 Bean。 优点：能够保持 WebSocket 会话的状态。 缺点：仅适用于 WebSocket 环境。 86. WebApplicationContext 和 ApplicationContext WebApplicationContext 是 ApplicationContext 的==子接口==，专门为 Web 环境设计，扩展了 ApplicationContext 提供的功能，增加了 Web 环境的支持（如 HTTP 请求、会话管理等）。 ApplicationContext 是通用的 Spring 容器接口，适用于非 Web 环境，而 WebApplicationContext 则是基于 Web 环境的一种实现。 87. Seata 见seata文档\n支持的模式有 AT, TCC, SAGA 长事务 可以无侵入的实现事务控制, 代理的 DataSource AT 模式的流程 88. Dubbo请求失败重试 默认是会进行多两次的重试的\ndubbo 是 rpc 框架, 衍生的能力有\n动态路由 容错重试 负载均衡 集群容错策略\nFailover Cluster（默认，失败自动切换）：\n调用失败后，切换到其他节点重试。 配合 retries 参数，控制总调用次数。 Failfast Cluster（快速失败）：\n一次调用失败即返回错误，不进行重试。 Failsafe Cluster（失败安全）：\n调用失败直接忽略，不抛出异常（适用于日志等非关键操作）。 Failback Cluster（失败自动恢复）：\n==调用失败后，异步记录并定期重试==。 Forking Cluster（并行调用）：\n并行调用多个节点，只要一个成功即返回。 无重试逻辑，但可以通过并行多调用实现类似效果。 89. redis和mysql 如何保持数据一致 ==问题产生的根源==\nredis和mysql 数据不一致 缓存被提前删除,更新 缓存失效和创建时机不对 并发操作, 多个线程同时修改数据 网络或服务异常, 更新数据库或缓存失败 常用的解决方案\nCache Aside 模式（常用） 缓存和数据库的更新分开操作：\n读操作：先读缓存，如果缓存没有数据（缓存穿透），则查询数据库并将结果写入缓存。 写操作：先更新数据库，再删除缓存（推荐）。 优点: 简单易用 缺点: 会有短暂的数据不一致的问题, 更新完数据库后还没来得及更新缓存 Write Through 模式 数据写入操作先更新缓存,再更新数据库\n优点: 缓存数据库实时同步 缺点: 写操作延迟高, ==适用读多写少的场景== Read Through 模式 先找缓存, 没有就找数据库并更新缓存\nWrite Behind 模式 先更新缓存, 再异步刷新数据库\n优点: 提高写性能\n缺点: 数据不一致的\n90. Spring cloud 理解 微服务的一套标准, 提供一些解决方案, 降低了微服务架构的开发难度, 只用在spring-boot 中引入相关的starter 就可以集成相关的组件\nRibbon 负载均衡\nGateway 网关\nHystrix 服务熔断\nalibaba\ndubbo\nnacos 配置中心, 注册中心\nsentinel 限流和降级\n91. Rpc 和 Http rpc ==不是通信协议, 是一个远程过程调用协议==, 是方便开发人员在对远程方法进行调用的时候就像调用本地方法一样, rpc 里面服务之间的通信协议可以是http, 也可以是tcp, 或者是自定义的协议\nhttp 是通信协议, 基于Tcp协议\n什么是序列化和反序列化\n序列化是为了解决网络传输过程中, 对象通信的问题, ==序列化就是把对象转化为字节流==\n反序列化就是从网络上获取对象的字节流, 重新构建一个新的对象\n92. 一致性Hash算法 分布式系统中常用的**负载均衡和数据分布算法**。它通过**将数据分布到多个节点上，减少节点增删时数据迁移的范围，从而提高系统的可扩展性和稳定性**。 redis 里面就有用到 基本原理 哈希环： 一致性哈希将整个哈希值空间（例如 0 ~ 2^32-1）抽象为一个环状结构，称为哈希环。 环的起点和终点相连，形成一个闭合的哈希空间。 节点映射到环上： 通过哈希函数（如 Hash(nodeId)），将节点映射到哈希环上的某个点。 这些点代表物理节点或服务器。 数据映射到环上： 通过哈希函数（如 Hash(dataKey)），将数据键映射到哈希环上的某个点。 数据分配规则：数据存储在顺时针方向的第一个节点上。 传统哈希缺点 如果节点数量发生变化（增减节点），所有的数据需要重新计算哈希值并迁移到新节点，成本高。 一致性哈希改进 当节点增加或减少时，只影响哈希环上相邻的一小部分数据，不会影响整个系统。 新增节点：数据只从新节点的顺时针后继节点迁移。 移除节点：数据只从被移除节点迁移到其顺时针后继节点。 93. 分布式和微服务的理解 分布式系统是把一个大系统拆分到多个机器上运行，解决性能瓶颈和高可用问题，重点是让多台机器协作完成一个任务。 微服务架构是把一个大应用拆成多个小服务，每个服务独立负责一个功能，重点是按业务模块解耦，并且可以通过分布式技术来部署。 区别：分布式更关注系统架构层面，微服务更关注业务功能划分。 关系：微服务通常基于分布式技术实现，但分布式不一定是微服务。\n94. Nacos 配置更新流程 Nacos 采用的是长轮训的方式向Nacos Server 端去发起配置更新的查询 长轮训就是客户端发起一次轮训请求到服务端, 当服务器端的配置==没有任何的变动的时候, 连接会一直的打开==, 当配置更新或者连接超时之后进行返回 客户端把需要去进行比较的==配置会进行分片,3000个配置一个分片== 客户端会 分包进行比较和更新, 3000个配置key, value 拼接的字符串进行MD5 比较 服务端会逐个比较, 把存在更新的key Nacos 既支持 CP（一致性）模型，也支持 AP（可用性）模型。具体来说：\nAP 模型：Nacos 默认采用 AP 模型，即在网络分区的情况下，优先保证系统的可用性，而不是一致性。这意味着在网络分区时，Nacos 仍然可以对外提供服务，但可能会出现数据不一致的情况。 CP 模型：Nacos 也支持 CP 模型，即在网络分区的情况下，优先保证数据的一致性，而不是可用性。这意味着在网络分区时，Nacos 可能会暂时无法对外提供服务，直到网络恢复并达到一致性 95. 分布式ID设计方案 需要考虑因素\n有序性, 在Mysql B+数的存储结构中, 范围查询的效率更高, B+树数据的维护效率更高 安全性, 反爬 可用性要高, 如果出现问题会导致大部分业务不可用 性能 通常用的是 雪花id, 64位长度组成的, 区域划分\n97. Nosql Nacos 既支持 CP（一致性）模型，也支持 AP（可用性）模型。具体来说：\nAP 模型：Nacos 默认采用 AP 模型，即在网络分区的情况下，优先保证系统的可用性，而不是一致性。这意味着在网络分区时，Nacos 仍然可以对外提供服务，但可能会出现数据不一致的情况。 CP 模型：Nacos 也支持 CP 模型，即在网络分区的情况下，优先保证数据的一致性，而不是可用性。这意味着在网络分区时，Nacos 可能会暂时无法对外提供服务，直到网络恢复并达到一致性 98. 分布式锁 使用分布式锁是因为在同一个时间, 多个服务之间有多个线程去争抢同一个资源所造成的线程安全的问题\n==redis==\n使用 setNx, 如果不存在就设置\n使用redission, 他提供了分布式锁的封装\n红锁（RedLock）是什么？ 确保锁在多个 Redis 节点上获取到，避免单点故障带来的问题。 如果仅在一个 Redis 节点上加锁，Redis 节点宕机后，锁就失效了。\n在分布式系统中，多个 Redis 节点会部署在不同的服务器上。\n当需要实现一个全局唯一的分布式锁（如协调多个服务同时对一个资源操作），就需要一种机制来在多个节点上保证锁的一致性。\n红锁是 Redis 提供的一种分布式锁算法，适用于集群模式和多节点部署场景。\n==zookeeper==\n使用同一个节点的唯一性或者有序节点, 这样的特性 99. 时间轮算法 概念 时间轮：一个轮状结构，可以想象成时钟的“表盘”，每一格是一个槽。 时间槽（Slot）：时间轮中的每个区域，代表一个单位时间（比如1秒、1分钟等）。 指针（Pointer）：轮子的指针表示当前的时间，随着时间流逝不断向前移动，指向下一个时间槽。 时间轮的工作流程 时间轮的结构： 时间轮由一个固定大小的槽数组（Slot array）构成，每个槽代表一个时间单位（比如1秒、1分钟等）。每个槽可以存放一个定时任务队列。 定时任务的加入： 当一个定时任务需要执行时，系统会根据任务的执行时间（延时）计算出应该放置在时间轮的哪个槽中。任务会被放入对应的槽队列中。 时间轮的转动： 每当轮子“走一步”时，时间轮指针就会移动到下一个槽。当指针指向某个槽时，如果该槽内有定时任务，系统就会执行这些任务。 任务的执行： 如果指针指向的槽内有任务，系统会将这些任务取出并执行。每次执行后，指针继续前进。 任务的超时处理： 时间轮的每一圈就是一个时间周期，指针每移动一步，表示时间的流逝。如果任务已经到达它的执行时间，它就会被触发执行。如果任务还没有到时间，它会留在该时间槽中等待下一次轮转。 100. 令牌桶, 漏桶 ==令牌桶==\n系统以固定速率向桶中添加令牌（单位时间生成一定数量的令牌），代表系统的处理能力。 桶的容量是固定的，超过容量的新令牌会被丢弃 ==请求处理方式==: 每次请求到来时，系统从桶中取出一个或多个令牌。如果桶中有足够的令牌，则请求被处理；否则，请求被拒绝或延迟处理。 ==漏桶==\n模拟一个有固定容量的漏桶（如水桶）。 桶内的水（请求）以固定速率流出。 ==请求处理方式==\n如果桶未满，请求被放入桶中。\n如果桶已满，新的请求将被丢弃。\n无论请求到达的速率如何，桶中的内容都会以固定速率“漏出”（处理）\n特性 令牌桶 漏桶 突发流量处理 支持 不支持 实现方式 通过生成令牌控制流量 通过固定速率漏水控制流量 限流行为 限制平均速率，允许短时突发流量 限制恒定速率，平滑输出流量 101. 滑动窗口算法 滑动窗口的核心在于维护一个窗口（范围），并在窗口中动态调整以找到目标解。窗口的范围可以通过起点和终点两个指针表示。\n窗口左端点：表示当前子区间的起始位置。\n窗口右端点：表示当前子区间的结束位置。\n通过移动左端点和右端点，可以动态调整窗口的大小和位置，从而在数据结构中进行遍历和筛选。\n用来解决数组的统计问题 ","date":"2024-12-31T18:39:58+08:00","permalink":"https://mikeLing-qx.github.io/p/interview/","title":"Interview"},{"content":"参考文档\nhttps://mp.weixin.qq.com/s/aX8XoccfwbjDwo8O8HRJRg\n1. Nacos 架构 namespace 层级关系\n命名空间 namespace ​\t组 group dataId Provider APP：服务提供者 Consumer APP：服务消费者 Name Server：通过VIP（Virtual IP）或DNS的方式实现Nacos高可用集群的服务路由 Nacos Server：Nacos服务提供者，里面包含的Open API是功能访问入口，Conig Service、Naming Service 是Nacos提供的配置服务、命名服务模块。Consitency Protocol是一致性协议，用来实现Nacos集群节点的数据同步，这里使用的是Raft算法（Etcd、Redis哨兵选举） Nacos Console：控制台 1. nacos 安装模式 单机模式 Derby: 这种模式是极简模式，数据没法持久化存储，适合开发环境。 单机模式 MySQL:(支持MySQL5.7和MySQL8.0，我们这里学习MySQL5.7安装模式，因为当前主流还是MySQL5.7) 这种模式支持数据持久化，数据会存储到MySQL中，适合生产环境。 集群模式: 这种模式适合生产环境并且服务节点个数较多，不存在单点故障问题。 2. nacos 集群 2. 注册中心的原理 ==注册过程==\n服务提供方 使用 OpenApi 发起服务注册, 并与注册中心建立心跳机制 服务消费放 查询服务提供方的 实例列表, 定时任务 每10s 拉取一次服务列表 注册中心检测到服务提供者异常, 会基于UDP 协议推送更新给消费者 Nacos提供了SDK和Open API两种形式来实现服务注册。\nOpenApi\nSDK\n==底层都是基于HTTP协议完成请求的==。所以注册服务就是发送一个HTTP请求：\n服务实例在启动时注册到服务注册列表, 并在关闭时注销\n服务消费者查询服务注册列表, 获得可用实例\n服务注册中心需要调用服务实例的健康检查API来验证它是否能够处理请求\nNacos服务端收到请求后，做以下三件事：\n构建一个Service对象保存到ConcurrentHashMap集合中 使用定时任务对当前服务下的所有实例建立心跳检测机制 基于数据一致性协议 Nacos服务端 数据进行同步 0. Nacos 服务动态更新原理 HostReactor类，它的功能是实现服务的动态更新，基本原理是：\n客户端发起时间订阅后，在HostReactor中有一个==UpdateTask线程==，每10s发送一次==Pull==请求，获得服务端最新的地址列表 对于Nacos服务端，它和服务提供者的实例之间维持了心跳检测，一旦服务提供者出现异常，则会发送一个Push消息给Nacos客户端，也就是服务端消费者 服务消费者收到请求之后，使用HostReactor中提供的processServiceJSON解析消息，并更新本地服务地址列表 1. 负载均衡 ​ 服务注册到Nacos中，有一个==权重属性==，这个权重属性就是Nacos的负载均衡机制，此时需要用到Nacos的负载均衡策略NacosRule，我们可以在程序中先初始化负载均衡算法，再到bootstrap.yml中配置权重\nhailtaxi-order中配置\n初始化负载均衡算法 /*** * Nacos负载均衡算法 * @return */ @Bean @Scope(value=\u0026quot;prototype\u0026quot;) public IRule loadBalanceRule(){ return new NacosRule(); } 如果把算法NacosRule注释，默认就是和Ribbon集成，且默认开启，可以通过如下配置实现关闭或开启\nribbon: nacos: enabled: true\n权重配置 spring: application: name: hailtaxi-driver cloud: nacos: discovery: # nacos 服务注册地址 server-addr: 192.168.200.200:8848 weight: 1 config: server-addr: 192.168.200.200:8848 并在VM Options 中进行配置\n2. 配置中心 spring: application: name: hailtaxi-driver profiles: active: dev cloud: nacos: discovery: # nacos 服务注册地址 server-addr: 192.168.200.200:8848 weight: 1 # 指定命名空间的id namespace: 1ebba5f6-49da-40cc-950b-f75c8f7d07b3 config: server-addr: 192.168.200.200:8848 # 指定命名空间的id namespace: 1ebba5f6-49da-40cc-950b-f75c8f7d07b3 # 如果将配置信息保存到nacos，指定配置文件扩展名 file-extension: yaml # nacos config dataid name 默认加载 ${spring.application.name}.${file-extension},当然也可指定 #name: hailtaxi-driver.yaml # 加载共享配置信息 shared-configs[0]: dataId: datasource.yaml refresh: true # 加载扩展配置 extension-configs: - dataId: custom.yaml 1. 配置刷新 nacos 控制台配置 app: name: itheima 项目中添加测试代码 @SpringBootApplication @EnableDiscoveryClient @MapperScan(basePackages = \u0026quot;com.itheima.driver.mapper\u0026quot;) public class DriverApplication { public static void main(String[] args) { ApplicationContext applicationContext = SpringApplication.run(DriverApplication.class,args); while(true) { //当动态配置刷新时，会更新到 Enviroment中， String name = applicationContext.getEnvironment().getProperty(\u0026quot;app.name\u0026quot;); String version = applicationContext.getEnvironment().getProperty(\u0026quot;app.version\u0026quot;); System.out.println(\u0026quot;app.name=\u0026quot;+name+\u0026quot;;app.version=\u0026quot; + version); try { TimeUnit.SECONDS.sleep(5); // 每隔5秒中从Enviroment中获取一下 } catch (InterruptedException e) { e.printStackTrace(); } } } } 默认情况下，shared-configs 和 extension-configs 是不自动刷新的，【其他配置可以】，如果要支持刷新，需要添加refresh属性，如下 # 加载共享配置信息 shared-configs[0]: dataId: datasource.yaml refresh: true # 加载扩展配置 extension-configs: - dataId: custom.yaml refresh: true 序中如果写了@Value注解，可以采用@RefreshScope实现刷新，只需要在指定类上添加该注解即可 3. Spring cloud 完成注册的时机 在Spring-Cloud-Common包中有一个类org.springframework.cloud. client.serviceregistry .ServiceRegistry ,它是Spring Cloud提供的服务注册的标准。==集成到Spring Cloud中实现服务注册的组件,都会实现该接口==\n该接口有一个实现类是==NacoServiceRegistry==。\n在spring-clou-commons包的META-INF/spring.factories中包含自动装配的配置信息如下\nAutoServiceRegistrationAutoConfiguration 就是服务注册相关的配置类\n@Configuration( proxyBeanMethods = false ) @Import({AutoServiceRegistrationConfiguration.class}) @ConditionalOnProperty( value = {\u0026quot;spring.cloud.service-registry.auto-registration.enabled\u0026quot;}, matchIfMissing = true ) public class AutoServiceRegistrationAutoConfiguration { @Autowired( required = false ) private AutoServiceRegistration autoServiceRegistration; @Autowired private AutoServiceRegistrationProperties properties; public AutoServiceRegistrationAutoConfiguration() { } @PostConstruct protected void init() { if (this.autoServiceRegistration == null \u0026amp;\u0026amp; this.properties.isFailFast()) { throw new IllegalStateException(\u0026quot;Auto Service Registration has been requested, but there is no AutoServiceRegistration bean\u0026quot;); } } } 配置类中注入了 AutoServiceRegistration\nAbstractAutoServiceRegistration抽象类实现了该接口, NacosAutoServiceRegistration继承了AbstractAutoServiceRegistration\n==AbstractAutoServiceRegistration抽象类== 关系图\nEventListener 可知 Nacos是通过Spring的事件机制继承到SpringCloud中去的\nAbstractAutoServiceRegistration实现了onApplicationEvent抽象方法,并且监听WebServerInitializedEvent事件(当Webserver初始化完成之后) , 调用this.bind ( event )方法。\n","date":"2024-12-12T15:16:25+08:00","permalink":"https://mikeLing-qx.github.io/p/spring_cloud_nacos/","title":"Spring_cloud_nacos"},{"content":"main Q:\nSpring的架构设计是怎样的?\n有哪些核心组件\nBeanFactory 和 ApplicationContext 的区别\n都是容器对象, Beanfactory 提供了对bean的基本操作, 读写 ApplicationContext 具备Beanfactory 基本功能, 还有一些拓展 spring 的后置处理器 (是一种拓展机制)\nbean的生命周期\n循环依赖怎么解决的\n代理相关 (代理对象是什么时候产生的? )\n事务, mvc 流程\nIOC 容器初始化的流程 是怎样搞的\nAware接口属性\n什么是bean覆盖? \u0026ndash;重复的bean定义 (默认策略允许 : 不同的配置文件中, 后加载的可以覆盖前面加载的)\nspirng 的 事件驱动 (观察者模式, 定义了一对多的依赖关系, 当主提对象发生变化就会通知观察对象)\nBean组件\nBean的定义 封装成Bean定义 Bean的创建 根据Bean定义信息 通过beanFactory创建bean (工厂模式) Bean的解析 other Q:\nprotected 关键字 碎片笔记\nBeans 和Core 是实现IOC和DI特性的关键\nServlet 里面包含了 ModelAndView\nBeanFactory\nBeanFactoryPostProcessor \u0026ndash;\u0026gt; bean定义\n是有顺序的, 实现Order接口 BeanPostProcessor \u0026ndash;\u0026gt; bean\n任务:\nspring 源码阅读环境搭建 课程上完 流程图\n一. Refresh 方法解读 1. obtainFreshBeanFactory 2.0 概述 作用: 用于获取一个新的BeanFactory 流程: 该方法会解析所有spring配置文件, 将所有Spring配置文件中bean定义封装成beanDefinition 加载到beanFactory中\n加载到beanFactory中 , 主要指的是三个缓存Map\nbeanDefinitionNames缓存: beanName 集合 beanDefinitionMap缓存: beanName 和 BeanDefinition 映射 aliasMap 缓存; beanName 别名映射 BeanDefinition接口\n保存了我们的bean信息, 比如 指向的是哪个类, 是否是单例的, 是否懒加载, 依赖了哪些bean,\n2.1 流程 刷新beanfactory, AbstractRefreshableApplicationContext 实现\n是否已存在, 存在则先销毁, 关闭该beanfactory 创建一个新的beanfactory 设置两个配置属性: 是否允许bean覆盖, 是否允许循环引用 加载bean到beanfactory loadBeanDefinitions (DefaultListableBeanFactory beanFactory) 加载bean到beanfactory\n通过BeanDefinitionReader解析xml为Document 将Document注册到BeanFactory 中（这时候只是bean的一些定义，还未初始化） loadBeanDefinitions(XmlBeanDefinitionReader reader) 重载\ngetConfigLocations() 获取配置文件路径：如果 configLocations 属性不为空，则返回 configLocations 的值；否则，调用getDefaultConfigLocations() 方法。获取到配置文件路径（Spring 默认的配置路径：/WEB-INF/applicationContext.xml。） 根据配置文件路径加载 bean 定义 doLoadBeanDefinitions\n根据 inputSource 和 resource 加载 XML文件，并封装成 Document。 根据返回的 Document 注册 bean 信息 ","date":"2024-11-23T18:07:34+08:00","permalink":"https://mikeLing-qx.github.io/p/spring_%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","title":"Spring_源码解析"},{"content":"1. 介绍 ETL是数据抽取（Extract）、转换（Transform）、装载（Load）的过程。 Kettle是一款国外开源的ETL工具，有两种脚本文件transformation和job，transformation完成针对数据的基础转换，job则完成整个工作流的控制。 Job：一个作业，由不同逻辑功能的entry组件构成，数据从一个entry组件传递到另一个entry组件，并在entry组件中进行相应的处理。 Transformation：完成针对数据的基础转换，即一个数据转换过程。 Entry:实体，即job型组件。用来完成特定功能应用，是job的组成单元、执行单元。 Step:步骤，是Transformation的功能单元，用来完成整个转换过程的一个特定步骤。 Hop:工作流或转换过程的流向指示，从一个组件指向另一个组件，在kettle源工程中有三种hop，无条件流向、判断为真时流向、判断为假时流向。 ==其他ETL工具: Sqoop, DataX, Kettle, Talend 等==\n2. 体系结构 两种工作单元文件\nktr 文件, kettle transformation: 此文件定义一个转换任务, 包含多个步骤, 这些步骤可以并行或者按顺序执行, 转换的目的是从一个或者多个数据源读取数据, 然后进行一些处理 (比如 清洗, 验证, 转换, 合并), 并将这些数据输出到一个或者多个目标, 是xml 格式 kjb 文件 （Kettle Job）: 此文件表示一个作业，作业是一种更高级别的工作流控制，可以包含多个转换（ktr）、作业（kjb）、脚本、邮件发送或其他任务等。这些任务按照某种特定的顺序（可以是并行、串行执行，也可以依赖于任务之间的成功与否）进行执行 1. 元数据管理引擎 元数据管理引擎管理ktr、kjb或者元数据库，插件通过该引擎获取基本信息，主要包括TransMeta、JobMeta和StepMeta三个类。\nTransMeta类，定义了一个转换（对应一个.ktr文件），提供了保存和加载该文件的方法； JobMeta类，同样对应于一个工作(对应一个.kjb文件)，提供保存和加载方法； StepMeta类，保存的是Step的一些公共信息的类，每个类的具体的元数据将保存在显示了StepMetaInterface的类里面。 2. 数据集成引擎 数据集成引擎包括Step引擎、Job引擎和数据库访问引擎三大部分，主要负责调用插件，并返回相应信息。\n3. 功能模块 Kettle的主要包括四大功能模块：\nChef——工作(job)设计工具 (GUI方式)； Kitchen——工作(job)执行器(命令行方式)； Spoon——转换(transform)设计工具 (GUI方式)； Span——转换(trasform)执行器(命令行方式)。 3. transformation 1. 步骤的特性 步骤需要有名字, 同一个转换范围内唯一 每个步骤都会读, 写数据行 , 唯一的例外是 \u0026lsquo;生成记录\u0026rsquo; 步骤, 该步骤只写数据 步骤将数据==写到与之相连的一个或者多个输出跳 hop==, 再传送到 hop另一端的步骤 大多数的步骤 都可以有多个输出 hop, 一个步骤的数据发送可以被设置==为分发 和 复制==, 分发是轮流接受, 复制是发送到所有的目标步骤 跳实际上是两个步骤之间的 ==被称为行集的 数据行缓存,== ==行集的大小可以在转换的设置里定义==, 当行集满了, 向\n所有步骤都是并行执行的\n输出: 字段拆分之后, 原字段不会存在数据流里面\n行扁平化: 需要规整的数据, 需要先进行排\n2. 转换控件 1. ==列转行== 需要注意的是: 列转行之前, 数据流必须按照==分组字段==进行排序, 否则数据会错乱\n2. 行转列 3. 流程控件 switch 过滤控件 空操作\u0026ndash;一般作为数据流的终点 中止操作 4. 查询控件 1. 数据库查询 2. 流查询控件 可以把两个表输入, 根据指定条件进行连接\n==lookup step \u0026ndash; 被连接的表 (次表)==\n5. 连接控件 1. 合并记录 ==用于 新 旧数据的比较==, 必须要相同的结构, 而且需要对数据先进行排序\n2. 记录集连接 6. 统计控件 1. 分组 就类似与group by, 但是在分组之前, 最好先进行排序\n7. 映射控件 类似于一个方法\n8. 从结果获取记录 这个步骤通常与「Execute SQL script」步骤配合使用，==用于执行 SQL 查询并从查询结果中获取记录==。\n4. job 5. kettle传递参数 可分为全局参数和局部参数\n一般全局参数只在临时调试中使用, 存储在 kettle.properties 文件中\n1. 全局变量 全局参数定义是通过当前用户下.kettle文件夹中的kettle.properties文件来定义。\n定义方式是采用键=值对方式来定，如：start_date=20130101\n注：在配置全局变量时需要重启Kettle才会生效。\n2. 局部变量 局部参数 在kettle 流程的开发中使用较多, 主要的传递方式 为 ==“设置变量/获取变量”，“转换命名参数”，“常量引用”三种==, 参数（变量）在transformation 之间传递，参数（变量）传递时，每次只能传递一个变量。\n在ktr转换中设置变量 在转换中通过设置变量组件实现变量的设置，变量在当前转换中设置，不能在当前转换中使用，必须在下一转换中使用，在下一转换中通过获取变量组件实现上一转换中变量的获取使用。\n使用方式：使用获取变量组件获取后直接使用\n在job作业中设置变量 在job作业中也可以设置变量，在作业中通过==设置变量设置完成后，在下一转换中可以使用获取变量组件获取或直接在使用时通过${变量名}使用==（不需要使用获取变量组件）。\n使用方式：使用获取==变量组件获取后直接使用 或者直接在需要使用时使用${变量名}==\n常量传递 在任意组件都可以定义，定义后使用？获取使用。\n常量定义的顺序即？对应的顺序\n6. 疑问 kettle 中的日志打印, 在使用 代码调用的时候可以单独进行配置吗? 不需要, 有日志组件可以使用 如何配置本地kettle 模板, 并使用etl工程完成数据装载? ","date":"2024-10-03T18:14:03+08:00","permalink":"https://mikeLing-qx.github.io/p/kettle/","title":"Kettle"},{"content":"1. linux 基础 **Linux文件系统和目录结构：**了解基本的文件系统层次结构和常见的目录，比如/root,/home,/etc, /var等。了解文件权限以及如何改变文件权限(chmod, chown)。这些都是Linux基础知识。 chmod: 命令来改变文件或目录的权限\nchown: 用来修改文件的所有者和组, 使用这个命令需要用超级用户\nLinux磁盘管理： 理解磁盘如何在Linux中被管理，学习如何查看磁盘使用情况（df, du命令）。 分区和文件系统： 需要了解如何使用Linux系统工具（比如fdisk, parted）进行磁盘分区，以及如何格式化分区并创建文件系统（mkfs命令）。 **挂载和卸载：**理解挂载点的概念，并学习如何挂载（mount）和卸载（umount）文件系统。 **文件的复制和移动：**掌握如何复制和移动文件（cp, mv命令），并理解硬链接和软链接的区别和用法(ln命令)。 **备份和恢复：**熟悉如何备份文件和文件系统（使用tar等工具），如何同步文件（rsync）。 **问题排查：**如何查看和搜索日志文件，如何使用Linux内置的诊断工具进行问题排查（dmesg, fdisk, lsblk等）。 1. 系统启动过程 内核的引导。 运行 init。 系统初始化。 建立终端 。 用户登录系统 关机的话一般都要先运行 sync将数据由内存同步到硬盘中 然后再执行shutdowon命令 2. 系统目录结构 /bin： bin 是 Binaries (二进制文件) 的缩写, 这个目录存放着最经常使用的命令。\n/boot： 这里存放的是启动 Linux 时使用的一些核心文件，包括一些连接文件以及镜像文件。\n/dev ： dev 是 Device(设备) 的缩写, 该目录下存放的是 Linux 的外部设备，在 Linux 中访问设备的方式和访问文件的方式是相同的。\n/etc： etc 是 Etcetera(等等) 的缩写,这个目录用来==存放所有的系统管理所需要的配置文件和子目录==。\n/home： 用户的主目录，在 Linux 中，==每个用户都有一个自己的目录，一般该目录名是以用户的账号命名==的，如上图中的 alice、bob 和 eve。 类似 /home/daheng\n/lib： lib 是 Library(库) 的缩写这个目录里存放着系统最基本的动态连接共享库，其作用类似于 Windows 里的 DLL 文件。几乎所有的应用程序都需要用到这些共享库。\n/lost+found： 这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。\n/media： linux 系统会自动识别一些设备，例如U盘、光驱等等，当识别后，Linux 会把识别的设备挂载到这个目录下。\n/mnt： 系统提供该目录是==为了让用户临时挂载别的文件系统的==，我们可以将光驱挂载在 /mnt/ 上，然后进入该目录就可以查看光驱里的内容了。\n/opt： opt 是 optional(可选) 的缩写，这是给==主机额外安装软件所摆放的目录==。比如你安装一个ORACLE数据库则就可以放到这个目录下。默认是空的。\n/proc： ==proc 是 Processes(进程) 的缩写==，/proc 是一种伪文件系统（也即虚拟文件系统），存储的是当前内核运行状态的一系列特殊文件，这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息。 这个目录的内容不在硬盘上而是在内存里，我们也可以直接修改里面的某些文件，比如可以通过下面的命令来屏蔽主机的ping命令，使别人无法ping你的机器：\necho 1 \u0026gt; /proc/sys/net/ipv4/icmp_echo_ignore_all /root： 该目录为系统管理员，也称作超级权限者的用户主目录。\n/sbin： s 就是 Super User 的意思，是 Superuser Binaries (超级用户的二进制文件) 的缩写，这里==存放的是系统管理员使用的系统管理程序==。\n/selinux： 这个目录是 Redhat/CentOS 所特有的目录，Selinux 是一个安全机制，类似于 windows 的防火墙，但是这套机制比较复杂，这个目录就是存放selinux相关的文件的。\n/srv： 该目录存放一些服务启动之后需要提取的数据。\n/sys：\n这是 Linux2.6 内核的一个很大的变化。该目录下安装了 2.6 内核中新出现的一个文件系统 sysfs 。\nsysfs 文件系统集成了下面3种文件系统的信息：针对进程信息的 proc 文件系统、针对设备的 devfs 文件系统以及针对伪终端的 devpts 文件系统。\n该文件系统是内核设备树的一个直观反映。\n当一个内核对象被创建的时候，对应的文件和目录也在内核对象子系统中被创建。\n/tmp： tmp 是 temporary(临时) 的缩写这个目录是用来存放一些临时文件的。\n/usr： usr 是 unix shared resources(共享资源) 的缩写，这是一个非常重要的目录，==用户的很多应用程序和文件都放在这个目录下==，类似于 windows 下的 program files 目录。\n/usr/bin： ==系统用户使用的应用程序==。\n/usr/sbin： 超级用户使用的比较高级的管理程序和系统守护程序。\n/usr/src： 内核源代码默认的放置目录。\n/var： var 是 variable(变量) 的缩写，这个目录中存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。包括各种日志文件。\n/run： 是一个临时文件系统，存储系统启动以来的信息。当系统重启时，这个目录下的文件应该被删掉或清除。如果你的系统上有 /var/run 目录，应该让它指向 run。\n3. linux 文件基本属性 chown (change owner) ： 修改所属用户与组。 chmod (change mode) ： 修改用户的权限。 ls –l 命令来显示一个文件的属性以及文件所属的用户和组\n4. linux 磁盘管理 机械硬盘\n磁盘的最小存储单位是 扇区, 512Bytes, 0.5kb,, sector 操作系统文件存取的 最小单位是 块, 8个 连续的扇区 (横向的), 磁盘的最小存储单位block, 是4kb, 一个block 只能存放一个文件的内容, ==因此文件占用空间的大小, 只能是 block 的整数倍==, 在windows 中, 多个连续的扇区被称为==簇== 1. fdisk 分区 fdisk 对磁盘进行格式化和分区\n2. 分区表类型 mbr 分区表, 硬盘容量受限制, 最大只支持2T\ngpt 分区表, 没有大小限制\n1. MBR分区 MBR是一种较传统的分区方法，早在DOS时代就开始使用。MBR的分区信息存储在位于物理硬盘的开头的Master Boot Record中（这也是MBR名称的由来）。\nMBR的一个主要限制是，它只支持==最大2TB大小的磁盘==，并且==只能创建最多4个主分区==，或者3个主分区加一个扩展分区。扩展分区可以被分为更多的逻辑分区。\n2. GPT分区 ==该类型的分区表示没有拓展分区的==\nGPT是一种更新的分区方案，它是UEFI规范的一部分。GPT没有MBR分区的以上限制，它支持的磁盘大小理论上可以达到9.4 ZB（1ZB是1024的7次方字节），并且可以创建数量几乎无限的分区（具体数量取决于操作系统，Windows最多支持128个GPT分区）。\n相对于MBR，GPT提供了更加现代化的分区方式，包括磁盘和分区的唯一识别符，以及可恢复的分区表头等等。GPT也能在某些分区损坏的情况下，提供更好的数据安全性。\n目前主流的新硬件和操作系统，包括Windows，Linux，Mac OS等，都已经支持GPT分区。\n需要注意的是，不同的分区表结构对应不同的启动模式；MBR通常用于BIOS启动，而GPT则用于UEFI启动。在清楚自己的需求和硬件条件下选择合适的分区方法是非常重要的。\n3. loop \u0026ldquo;loop\u0026rdquo; 分区表类型并不是指 GPT 或 MBR，而是指这个设备被用作回环设备。回环设备通常不使用传统的分区表，因为它们通常代表单个文件系统，而不是一个包含多个分区的物理磁盘。\n3. 文件系统 操作系统中==专门用于管理和存储文件的 信息软件被称为文件系统==\n==常见的文件系统==\nfat16 fat32, 最早的windows 文件系统, 单个文件不能超过2gb NTFS 文件系统, 支持文件加密, ==采用日志形式的文件系统, 详细的记录磁盘读写的操作, 支持数据恢复== 突破了单个问价4g 大小的限制 exFAT 文件系统, 单个文件支持16gb 大小, 可以 在windows , linux, macos 同时识别 ==磁盘的RPM, 磁盘内电机主轴的旋转速度==, 一分钟内能够完成的最大旋转数\nlinux 中==一切皆文件, 磁盘设备在系统中也是以文件形式 展示==\n1. inode 是什么? ==stat 可以查看文件的元信息==\n文件是以==文件数据 + 文件元信息组成的==, 文件的inode 号 + 文件数据内容, 代表一个单个文件\n文件系统存储 文件元信息的地方就是 inode, 叫做索引节点\n查看文件的 inode号\nls -li #打印出来的信息, 除了文件名, 其他信息都是inode存储的元信息 文件大小 属主信息 用户组信息 文件权限数字 文件的修改时间 文件的实体指针, 指向block 的位置 df -hT 可以查看文件系统的类型 4. parted 分区 对超过2T的硬盘进行分区\n5. 文件系统 1. 文件系统类型 vfs 虚拟文件系统\n磁盘为什么要格式化\n==不同的系统, 使用的文件系统也各不同==\nlinux ==ext3 以后都是日志文件系统==\n文件系统创建工具\nmkfs 可以把某个分区格式化为某种文件系统 fsck 命令, 修复文件系统的命令 默认读取/etc/fatab 开机挂载文件的 对磁盘进行修复检查 fsck -t 文件系统类型 设备名 查看文件系统的属性 lsbks NFS 网络上共享文件系统 , UNIX 和类 UNIX 系统之间共享文件的标准方法\nshowmount -e localhost 查看当前设置为NFS 的 共享目录 也可以直接查看 cat /etc/exports 2. 文件系统挂载 没有挂载的设备,相当于没有开窗的物资, 进不去也出不来, 相当于一个屋子没有出入口\n挂载通常是一个存储设备挂接到另一个已存在的文件夹中, 访问这个文件夹, 就是访问该存储设备的内容了\n挂载点就是 一个普通文件夹\n3. 开机自动挂载 由于mount 命令直接输入是临时生效, 下次启动, 挂载的设备分区就无法使用了\n/etc/fstab 文件, 存放系统一些静态文件的地方\n6. swap 交换分区 swap 是 linux 系统磁盘管理的一块特殊的分区, 当实际的物理内存不足的时候, 操作系统会从整个内存中, 取出一部分暂无使用的内存, 拿出来放到交换分区, 从而给当前在使用程序更多的内存\nswap 分区大小, 必须根据物理内存和硬盘容量来计算\n创建交换分区并使用\n7. 内存不足 无故提示内存不足, 但是在cache, buff 中又看到大量的内存, 如何释放\n以下都是临时释放缓存的命令\n释放cache命令 echo 1 \u0026gt; /proc/sys/vm/drop_caches 等同于 sysctl -w vm.drop_caches=1 清除目录缓存 echo 2 \u0026gt; /proc/sys/vm/drop_caches 等同于 sysctl -w vm.drop_caches=2 清除内存页的缓存 echo 3 \u0026gt; /proc/sys/vm/drop_caches 等同于 sysctl -w vm.drop_caches=3 8 清理僵尸进程 将内存缓冲区中的数据 写入到磁盘中\nsync 9. raid 1.级别 RAID（冗余独立磁盘阵列）是一种用于连接多块硬盘驱动器为一体，以提供数据冗余、增强性能或两者兼有的存储技术。有多种不同的 RAID 级别，每种 RAID 级别都在性能和冗余性之间实现了不同的平衡。以下是常见的 RAID 级别：\nRAID 0：条带化。在 RAID 0 设置中，数据会被均匀地分布在两个或更多的磁盘上。这种方法可以增加数据吞吐量，但如果其中任何一块磁盘失效，所有数据都会丢失。\nRAID 1：镜像。在 RAID 1 设置中，两块磁盘会包含完全相同的数据，因此提供了数据冗余。如果一块磁盘出现故障，系统可以无缝地切换到另一块磁盘。然而，因为每次写入都必须写入两块磁盘，RAID 1 可能会对性能产生影响。\nRAID 5：带奇偶校验的分块分条处理，至少需要 3 块磁盘，它提供了数据冗余和性能增强。在 RAID 5 设置中，==数据和奇偶校验位将在所有磁盘之间分布==。如果一块磁盘出现故障，数据仍然可以从剩余的磁盘中恢复。\nRAID 6：是 RAID 5 的一个变种，至少需要 4 块磁盘。它使用两个奇偶校验块分布在每个磁盘，因此可以容忍两块磁盘的同时失败。\nRAID 10：也被称为 \u0026ldquo;1+0\u0026rdquo;，是 RAID 1 和 RAID 0 的组合。它提供了 RAID 0 的性能提升，以及 RAID 1 的数据冗余。需要最少 4 块磁盘。 只要不是同一个硬盘组同时损坏, 就还是可用的\nRAID 可以通过软件（使用操作系统的功能）或硬件（使用专用的 RAID 控制器）实现，除了上述的标准RAID级别，还有一些制造商特定的RAID级别，例如 RAID 50（RAID 5 的封套+ RAID 0）、RAID 60（RAID 6 + RAID 0）等。每种级别都各有其优势和劣势，你应根据你的存储需求和可接受的故障风险来选择适合的RAID级别\nraid 3的特点是 存异或值 的磁盘不可以损坏, raid\n2. 硬,软 1. 硬raid 2. 软raid 软raid 需要额外的消耗cpu 资源, 造成服务器压力 硬raid 更稳定, 兼容性更好 3. 搭建raid 检查raid10的详细信息 4. raid 10 故障修复 10 .lvm 1. 相关概念 概要\n图解lvm\nlvm 相关名词\nLVM原理\nLVM 动态扩容, 其实就是通过互相交换PE的过程, 达到能够单行扩容分区大小 想要减少空间容量, 就是提出PE的大小 想要扩大容量, 就是把其他的PE 添加到自己的LV 中 PE 默认大小是 4M, LVM 最多可以创建 65534 个PE, 因此 LVM最大 VG 卷组单位是256G PE 就是 LVM 最小存储单位, 类似操作系统的block LV 逻辑卷的概念 (理解为普通的分区概念, /dev/sda /dev/sdb) 2. lvm 管理常见命令 3. 创建流程 物理磁盘被格式化成pv, 空间被分成一个个pe 不同的pv假如通一个vg, 不同的pv的pe 全部将进入vg 的pe 池内 lv基于 pe 创建, 大小为pe的整数倍, 组成lv的pe 可能来自不同的物理磁盘 lv 现在就可以直接格式化后挂载使用了 lv 的扩充缩减 实际上就是增加或减少组成该 lv 的 pe 数量, 其过程不丢失原始数据 对刚创建的 LV1 逻辑卷 进行格式化文件系统 2. 文本三剑客 1. awk 默认情况, awk使用空白字符（如空格或制表符）作为字段分隔符\necho \u0026quot;Hello World\u0026quot; | awk '{print $1}' 可以自定义分隔符来满足你的需要。可以使用 -F 选项指定一个自定义的字段分隔符。例如：\necho \u0026quot;Hello:World\u0026quot; | awk -F':' '{print $1}' awk在行字段切分上这么强大的是，分隔符还可以是正则表达式。举个例子，你可以使用多个空格或者制表符作为分隔符，甚至混合使用它们：\nawk -F'[ \\t]+' ... 命令解析\nawk '{print $11}' /var/log/nginx/access.log | sort | uniq -c | sort -nr | head -20 |：这是管道符，用于将前一个命令的输出作为后一个命令的输入。 awk {print $1}：这是一个awk命令，作用是打印输入行的第一个字段。 sort：sort命令用于排序输入行。默认行为是进行字典排序。 uniq -c：uniq命令用于去重连续的重复行。-c选项会在去重的同时，显示每行的出现次数。 sort -nr：这是一个sort命令，-n 参数表明按数值顺序排序（而非字典顺序），-r 参数用于反转比较结果，使得排序结果为降序（默认为升序）。 head -20：head命令用于显示输入的前N行，-20参数指定显示前20行。 总的来说，这串命令的作用是：将输入行第一个字段提取出来，然后统计每个不同字段出现的次数，按出现次数进行降序排序，最后取出现次数最多的前20个字段。\n2. grep ==文件中搜索匹配指定模式的文本行 ==\n常用选项：\n-i：忽略大小写。 -r：递归搜索目录中的文件。 -v：反转匹配，显示不包含模式的行。 -n：显示匹配行的行号。 3. sed sed 是一个流编辑器，用于逐行处理文本。它可以==执行查找和替换、插入、删除等操作==，适用于简单的文本操作和复杂的脚本处理。\n4. vim 在 Vim 中撤回（撤销）修改可以使用 u 键。以下是一些常用的撤回和恢复操作：\n撤销上一步操作： ==在普通模式下（按 Esc 确保处于普通模式），按 u 键撤销上一步操作==。 重复撤销： 每按一次 u 键都会撤销一次操作，你可以连续按 u 多次撤销多个操作。 重做撤销的操作： 如果你撤销了一些操作后想要恢复，可以按 Ctrl + r 键（在普通模式下）重做上一步被撤销的操作。 和 u 键类似，你可以连续按 Ctrl + r 多次恢复多个被撤销的操作。 查看撤销历史： Vim 中的撤销操作是基于撤销树的，你可以使用 :undolist 查看当前缓冲区的撤销历史。 执行 :undolist 会显示撤销树中各个节点的信息，包含每次修改的时间戳和编号。 跳转到特定的撤销点： 你可以使用 :earlier 命令跳转到较早的编辑状态。例如，:earlier 5m 会将文件恢复到 5 分钟前的状态。 类似地，使用 :later 可以恢复到较晚的状态。例如，:later 5m 会恢复到 5 分钟后的状态。 撤销树导航： 使用 g- 可以回到上一个分支点（撤销树中的前一个节点）。 使用 g+ 可以前进到下一个分支点（撤销树中的下一个节点）。 复制当前行: 3. 命令 vi 文本编辑保存并退出 shift + Z + Z vi 根据关键搜索 command模式下(默认就是) '/关键字' 查看进程 关闭防火墙 systemctlstop firewalld systemctl iptables stop 更改nginx配置文件后 cd /usr/local/openresty/nginx/sbin 重新加载配置文件 ./nginx -s reload docker inspect mysql 查看docker的详细信息 创建文件 touch a.txt 或者vim 查看文件 cat a.txt 输入内容 echo 123 \u0026gt; a.txt 清屏 ctrl + L 查看redis 进程 ps -ef| grep redis 杀掉进程 用 kill 端口号 给文件操作授权(文件名字是白色的话就是没有权限执行) chmod 777 startup.sh (文件名) 查找文件夹下的 se 开头的文件 cat se vim 查看行号 :set nu vim 查找 /`关键字` 或者 ?`关键字` 按n查找下一个 查找特定的文件类型 ll *.rb 安装tomcat 并且启动 tar -zxvf apache-tomcat cd bin 开启tomcat 再查看日志 ./startup.sh tail -f ../logs/catalina.out 查看进程id ps -ef|grep tomcat 查看正在运行的java 进程 jps -l 查看 tomcat 运行的详细信息; 查看该jvm的运行参数 jinfo -flags 进程端口 文件打包 进入指定文件夹内 tar -cvf 例如: tar -cvf device-service.tar config/ lib/ lib-2021-11-27/ lib-2021-11-29/ lib-old/ shutdown.sh startup.sh netstat -lnpt | grep 10666\n查看当前所有tcp端口\n通过进程号查看运行文件目录\nll /proc/10666/cwd\nctrl + x 强制退出, ctrl + z 强制退出\n1. ps -ef 列出系统上运行的所有进程的详细信息。\n这个命令的各部分含义如下：\nps：process status的缩写，用于查看系统中的进程状态。 -e：这个参数代表“every”，意味着列出系统上的所有进程，不仅仅是当前用户的进程。 -f：这个参数代表“full”，意味着以全格式显示进程信息，这包括：UID（用户ID），PID（进程ID），PPID（父进程ID），C（CPU使用），STIME（进程开始时间），TTY（终端类型），TIME（CPU时间），CMD（命令行命令）。 所以，ps -ef命令会列出系统上所有进程的详细信息，这是系统管理员和开发者排查问题时非常常用的一个命令。\n2. grep grep命令是Linux系统中的一种强大的==文本搜索工具==，它能使用正则表达式搜索文本，并==打印出匹配的行==。\n例如，如果你想在文件中查找包含单词\u0026quot;error\u0026quot;的行，可以使用以下命令：\nbash\ngrep 'error' filename grep有许多参数可以改变其行为，以下是一些常见的参数：\n-i：忽略大小写 -v：反向查找，即打印出不匹配模式的行 -r 或 -R：递归搜索，grep会查找指定目录下所有文件及子目录中文件 -l：仅显示含有匹配行的文件名 -n：显示匹配行及其行号 -e：可以指定多个搜索模式 3. tar linux 下非常强大的打包工具，常常配合 gzip 或 bzip2 这样的压缩工具进行使用。它不仅可以用于备份和恢复文件，还经常用于在不同的系统间传输文件\n==创建tar 包==, 使用 -c 选项\n-v 选项, 会==显示==打包, 或者查看操作过的文件信息\n-t 会显示包里面的内容\n提取tar 包, -x, 可以==解压==到当前目录, 可以加==-C 命令 指定目录==\n创建并压缩 tar 包, 选项 -z 表示使用 gzip, -j 表示使用 bzip2\n命令中的 -f 选项后面是 tar 包的名字，它告诉 ==tar 使用的文件名==\n4. partx partx 是一个在 Linux 下用于增加和删除分区的实用程序。它是一种可以直接在设备上操作分区而不需要重新启动系统的方法。\n这个工具主要在你需要临时添加一个设备分区，或者==系统未能正确识别出设备上的新分区时使用==。它也是一个非常适合脚本使用的工具，以便进行自动化的设备管理。\npartx -a /dev/sda # 尝试添加所有在 /dev/sda 上的分区。如果有的分区已经存在，它将会被跳过。 这个命令不会在物理磁盘上创建或者删除分区，==它仅仅是在操作系统级别添加或者删除对分区的识别==\n5. ln 软硬链接 创建快捷方式\n软链接 ln -s 目标文件绝对路径 快捷方式绝对路径 软链接的 inode 号是不一样的, 但是硬链接的inode 号是一样的\n目录文件夹不支持硬链接 硬链接不可以跨文件系统, 软链接可以 可以通过 ls -l 查看文件的硬链接数量\nln 目标文件绝对路径 快捷方式绝对路径 删除软,硬链接对源文件都是没有影响的 删除源文件对硬链接无影响, 但是影响软链接 只有删除所有源文件和硬链接, 文件连接数为0, 文件数据丢失 软链接和源文件 inode 是不同的, 硬链接是相同的 6. df 可以显示 挂载点的使用情况\ndf 可使用的参数是\n-h 显示kb, mb 单位大小 -i 显示inode 数量 -T 显示文件系统类型 7. du 显示磁盘空间大小\n显示文件大小, 以kb 为单位 du -h 显示当前目录所有文件大小 du -h * 所有文件的大小总和 du -sh 8. 查询占用 fuser -muv /data lsof /data 9. 查看系统启动时间 w uptime 10. less less是一个程序，用于在终端中分页或逐步查看文件的内容。与cat和more命令不同，less在打开文件时不会立即显示文件的全部内容，这使得它特别适用于查看大型文件。 用户可以向前或向后浏览文件，搜索文本字符串，跳转到文件的特定行等。 less命令是交互式的，在查看文件时，用户可以使用键盘命令进行操作，==如搜索（/关键词）==，上下滚动（↑↓或PgUp/PgDn），跳转到文件开头（g）或结尾（G），以及==退出查看（q）==。 11. gunzip 解压.gz文件到当前文件夹，并且保留源文件不进行修改，您可以使用gunzip（或gzip -d）命令并加上-c选项。-c选项会将解压的内容输出到标准输出（stdout），然后您可以使用重定向\u0026gt;将输出内容写入到一个新文件中。下面是具体操作步骤：\n假设您的文件名为example.gz，您希望解压这个文件到当前文件夹，并保留原文件example.gz不变，可以使用以下命令：\ngunzip -c example.gz \u0026gt; example 或者使用gzip -d命令的等价形式：\ngzip -d -c example.gz \u0026gt; example 这两个命令通过-c选项将example.gz解压的输出内容导向到example文件中。因为使用了重定向\u0026gt;而不是直接解压覆盖，所以原.gz文件不会被修改或删除。\n这样，就能够在当前目录下解压.gz文件而不修改源.gz文件。\n12. tee tee 命令是一个在 Unix 和类 Unix 系统中非常有用的命令。它的主要作用是将标准输入的数据写入到标准输出的同时，另存到一个或多个文件中。这在需要将输出数据同时查看和保存的场景中非常方便。\n1. 基本用法 bash复制代码command | tee file command 是你希望运行的命令，它的输出将被 tee 接收。 file 是你希望将输出数据保存的文件名。 2. 示例 将命令输出保存到文件并显示在终端\nbash复制代码ls -l | tee file.txt 这个命令会列出当前目录下的文件和文件夹，并将这些信息写入 file.txt 文件中，同时也显示在终端中。\n追加输出到文件\nbash复制代码echo \u0026quot;New line\u0026quot; | tee -a file.txt 这里，-a 选项表示以追加模式打开文件，这样不会覆盖文件中的现有内容，而是将新内容添加到文件末尾。\n同时写入多个文件\nbash复制代码echo \u0026quot;Hello\u0026quot; | tee file1.txt file2.txt 这个命令会将 \u0026ldquo;Hello\u0026rdquo; 同时写入到 file1.txt 和 file2.txt 文件中，并显示在终端。\n3. 常用选项 -a：以追加模式写入文件，而不是覆盖。 -i：忽略中断信号。 tee 是一个非常有用的工具，特别是在调试脚本时，你可以实时查看输出，并同时将其保存到日志文件中以供后续分析。\n4. 服务器缩容 1. 常规磁盘 查看所有磁盘\nlsblk 这个命令会列出系统中所有的块设备，包括硬盘、分区、USB驱动等。输出的信息中还包括设备名称、大小、挂载点等等 , lsblk -f 列出分区的文件系统类型 fdisk -l：这个命令会列出系统中所有的磁盘和它们的分区。这个命令还会输出很多其他信息，如每个磁盘的扇区数、每个分区的起止位置等等。通常，这个命令需要root权限来运行，你可能需要用 sudo fdisk -l 来执行 df 这个命令会显示所有==已经挂载的文件系统的磁盘使用情况==，包括总大小、已使用、可用空间和使用百分比等信息 cat /etc/partitions 这个文件就存放了 系统的==磁盘分区表号==\n在将文件从 /dev/vdb 拷贝到 /dev/vdc 之前，你需要确保他们都已经被==正确地分区并格式化为文件系统==，然后在系统中进行==挂载==。\n典型的步骤可能是这样的：\n首先查看 /dev/vdb 是否已经含有分区并格式化为文件系统。可以使用 fdisk -l /dev/vdb 或 lsblk 命令查看。如果已经挂载，你可以在输出中找到挂载点。 对于 /dev/vdc，如果尚未分区，你要首先为其创建分区。可以使用 fdisk /dev/vdc, cfdisk /dev/vdc 或 parted /dev/vdc 等工具来创建分区。 如果新创建的分区（假设是 /dev/vdc1）尚未被格式化为文件系统，你需要使用 mkfs 命令进行格式化，如 sudo mkfs -t ext4 /dev/vdc1。 创建一个挂载点，然后挂载新的文件系统，例如： bash\nsudo mkdir /mnt/newdisk sudo mount /dev/vdc1 /mnt/newdisk 现在，你可以使用 cp 命令将文件从 /dev/vdb 对应的挂载点复制到 /mnt/newdisk。例如，如果 /dev/vdb 的挂载点为 /mnt/olddisk，你可以： bash\nsudo cp -R /mnt/olddisk/* /mnt/newdisk/ 注意事项：\n上述所有操作都需要root权限，你可能需要使用 sudo 命令。 格式化操作会清除目标分区上所有现有的数据，请在确认数据已经备份或数据可以丢失的情况下再进行格式化操作。 cp -R 命令将会递归复制所有文件和子目录。如果你只希望复制特定文件或目录，你可以直接指定他们的路径。 复制大量文件可能会需要一段时间，具体取决于你的硬盘速度和数据量。 挂载新硬盘的操作在重启后不会保留，如果你想在每次启动时自动挂载，你需要将其添加到 /etc/fstab 文件中。 查看是否被进程使用\n实际步骤\nfdisk /dev/vdc\ng 创建gpt分区表\nn 创建分区\nmkfs 格式化分区\n新建文件夹 data1\n挂载 mount\nblkid 查看设备id\n编辑 /etc/fstab\n使用rsync\n复制 sudo rsync -avh --progress /data/ /datanew 验证数据 sudo rsync -avhn --progress /data/ /datanew/ | grep -v '^\\.copying' 2. lvm 缩容 1. 理论回顾 逻辑卷\u0026ndash;动态磁盘管理, 以逻辑卷的形式展现给上层系统, 就是取代原本的硬盘和分区, 对其进行格式化和挂载操作, ==将底层的物理硬盘抽象封装起来==, 大小可以动态调整, 且不会丢失现有数据\nPE physical extend 默认的大小是4m\nPV physical volume 把硬盘条带化, 实际上就是把硬盘变成可以使用的PE\nVG volume group 就是 一个空间池, 卷组\nLV logical volume 逻辑卷\n逻辑卷的拉伸操作可以在线执行, 不需要卸载逻辑卷\npvs, pvdisplay 查看所有的物理卷\n缩小逻辑卷\n卸载已经挂载的逻辑卷 umount /dev/vdb1 缩小文件系统 resize2fs /dev/ka ==注意==\n当文件系统大小resive 和实际缩小的容量不一致的时候, 可能会造成磁盘内容丢失, 如果一定要缩容, 最好的给一定缓冲额度\n2. 实际场景操作 ==AI 建议==\n你做任何操作之前，做好数据备份是明智的。我的指导会牵扯到LVM卷的调整，这有可能导致数据丢失。\n如果你想要移除/dev/vdb并使用全新磁盘/dev/vdc，你需要进行以下步骤：\n准备/dev/vdc成为一个物理卷： pvcreate /dev/vdc 扩展你想要存储新磁盘的卷组，我假设这是klas： vgextend klas /dev/vdc 移动/dev/vdb上的所有物理区域到/dev/vdc上： pvmove /dev/vdb /dev/vdc 一旦所有数据都被成功迁移，你可以从卷组中删除旧的物理卷： vgreduce klas /dev/vdb 最后你可以完全移除旧的物理卷： pvremove /dev/vdb 注意: 所有操作必须谨慎进行，只有当你确定了你理解了每一步的意义并且在必要时进行了数据备份，你才能进行这些操作。有些步骤一旦执行，将无法回退。举例来说，pvremove指令将彻底删除物理卷，这是一个无法恢复的操作。\npvmove 是一个 Linux 命令，用于在物理卷（Physical Volumes）之间移动物理区域。物理卷是 Logical Volume Management（LVM）系统的一部分，它允许你创建逻辑卷，这些逻辑卷可以跨越一个或多个物理硬盘。\n以下是 pvmove 的一些关键特性：\n数据迁移：pvmove 可以在物理卷之间迁移数据。例如，如果你有一个物理卷（例如，/dev/sda），并且你想要迁移其数据到另一个物理卷（例如，/dev/sdb）。你可以使用 pvmove /dev/sda /dev/sdb命令来完成此操作。 ==无中断服务==：pvmove 的一个重要特性是你可以在系统正常运行时进行物理区域的迁移。这意味着，你的应用或用户不会因为数据迁移操作而中断。 迁移特定逻辑卷：你可以使用 -n 选项指定只迁移特定的逻辑卷。例如，pvmove -n /dev/klas/root /dev/vdb /dev/vdc 将会只迁移 klas/root 逻辑卷的数据。 错误处理：pvmove 命令在复制每个区域后，会更新元数据以反映这一变化。如果在迁移过程中发生错误，数据仍将保持一致。 数据同步：虽然 pvmove 可以在系统正常运行时进行，但是在数据同步期间，涉及的物理区域将被锁定，以防止写入操作。 最后，强调一点，使用 pvmove 命令时，务必要谨慎，因为错误的操作可能会导致数据丢失。一般情况下，在执行此类操作之前，建议进行数据备份。\npvmove的工作方式\npvmove的工作方式如下：\n1.创建一个临时的“ pvmove” LV，以存储所需的所有数据移动的详细信息。\n2.在VG中的每个LV上搜索需要根据命令行参数移动的连续数据。对于找到的每个数据，将新段添加到pvmove LV的末尾。该段采用临时镜像的形式，用于将数据从原始位置复制到新分配的位置。原始LV已更新为在pvmove LV中使用新的临时镜像段，而不是直接访问数据。\nVG元数据在磁盘上更新。\npvmove LV的第一段被激活，并开始镜像数据的第一部分。一次只能镜像一个段，因为通常这样会更有效。\n5.守护程序按指定的时间间隔重复检查进度。当它检测到第一个临时镜像处于同步状态时，它将断开该镜像，以便仅使用该数据的新位置，并将检查点写入磁盘上的VG元数据中。然后，它为pvmove LV的下一个段激活镜像。\n6.当没有更多要镜像的段时，将删除临时LV，并更新VG元数据，以便LV反映新的数据位置。\n请注意，此新过程不能支持磁盘元数据的原始LVM1类型。可以使用vgconvert（8）转换元数据 。\n如果使用\u0026ndash;atomic选项，则使用稍微不同的方法进行移动。同样，将创建一个临时的“ pvmove” LV以存储所需的所有数据移动的详细信息。该临时LV包含需要移动的各个LV的所有段。但是，在这种情况下，分配的相同LV包含相同数量的段，并且创建了镜像以将内容从第一个临时LV复制到第二个临时LV。完整复制后，将删除临时LV，并在目标PV上留下分段。如果在移动过程中发出中止命令，则所有被移动的LV将保留在源PV上。\n操作步骤\n先扩容, 将/dev/vdc1 初始化为物理卷, pvcreate /dev/vdc1\n把pv 加入到 卷组vg中, vgextend\npvmove 会处理一切,\nvgreduce\n检查结果 vgs\n再扩充逻辑卷, lvextend -L +350G /dev/vdc1\n查看扩充后的大小, lvs, lsdisplay\n更新文件系统 resize2fs /dev/vdc1\n查看更新后的文件系统 df -hT\n准备开始缩容\n==卸载已经挂载的逻辑卷 /dev/mapper/==\nLInux 系统下, 现在我需要把 vdc 磁盘给回收, 我加入了一块新的磁盘 vdb, 然后通过 pvcreate 和 vgextend 把这块 vdc 加入了卷组中, 我有一个疑问, 我使用 lsblk 查看, 为什么我的 格式跟 原来的 /vdc2 不一致? 不是 lvm 类型?\n我是否需要将/vdb1格式化文件系统? 还是说这个应该是 LV 逻辑卷去操作的, 现在我只需要执行 pvmove /dev/vdc1 /dev/vdb1 就可以了? [root@app-01 xcsj_service]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT vda 253:0 0 130G 0 disk ├─vda1 253:1 0 200M 0 part /boot/efi ├─vda2 253:2 0 2G 0 part /boot ├─vda3 253:3 0 16G 0 part [SWAP] └─vda4 253:4 0 100G 0 part └─klas-root 252:0 0 350G 0 lvm / vdb 253:16 0 360G 0 disk └─vdb1 253:17 0 360G 0 part vdc 253:32 0 450G 0 disk ├─vdc1 253:33 0 200G 0 part └─vdc2 253:34 0 250G 0 part └─klas-root 252:0 0 350G 0 lvm /\n==可以的, 还可以不停机运行! 够强大!==\n","date":"2024-09-18T11:23:54+08:00","permalink":"https://mikeLing-qx.github.io/p/linux%E5%91%BD%E4%BB%A4%E5%AE%9E%E6%88%98/","title":"Linux命令实战"},{"content":"1. wsl安装k3s 遇到问题\n[System has not been booted with systemd as init system (PID 1). Can\u0026rsquo;t operate]\n参考资料\nhttps://askubuntu.com/questions/1379425/system-has-not-been-booted-with-systemd-as-init-system-pid-1-cant-operate\nKubernetes介绍：https://www.cnblogs.com/weicunqi/p/14892207.html 资源管理：https://www.cnblogs.com/weicunqi/p/14894122.html 实战入门：https://www.cnblogs.com/weicunqi/p/14894249.html\n2. minikube window安装 New-Item -Path 'c:\\' -Name 'minikube' -ItemType Directory -Force Invoke-WebRequest -OutFile 'c:\\minikube\\minikube.exe' -Uri 'https://github.com/kubernetes/minikube/releases/latest/download/minikube-windows-amd64.exe' -UseBasicParsing $oldPath = [Environment]::GetEnvironmentVariable('Path', [EnvironmentVariableTarget]::Machine) if ($oldPath.Split(';') -inotcontains 'C:\\minikube'){ [Environment]::SetEnvironmentVariable('Path', $('{0};C:\\minikube' -f $oldPath), [EnvironmentVariableTarget]::Machine) } 控制台\nminikube dashboard 测试pod\nkubectl create deployment hello-minikube --image=kicbase/echo-server:1.0 kubectl expose deployment hello-minikube --type=NodePort --port=8080 kubectl get services hello-minikube minikube service hello-minikube kubectl port-forward service/hello-minikube 7080:8080 3. kube 命令 nginx-pod\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.12 ports: - containerPort: 80 副本数量\n==spec.replicas==是可以选字段，指定期望的pod数量，默认是1。\n==.spec.selector==是可选字段，用来==指定 label selector ，圈定Deployment管理的pod范围==。如果被\n指定， .spec.selector 必须匹配 .spec.template.metadata.labels，否则它将被API拒绝。如果\n.spec.selector 没有被指定， .spec.selector.matchLabels 默认是.spec.template.metadata.labels。\n==在Pod的template跟.spec.template不同或者数量超过了.spec.replicas规定的数量的情况下, Deployment会杀掉label跟selector不同的Pod==\nPod模板，.spec.template 是 .spec中唯一要求的字段\n.spec.template.spec.restartPolicy 可以设置为 Always , 如果不指定的话这就是默认配置。\n创建pod kubectl get pods -o wide -w kubectl apply -f nginx-pod.yml 删除pod kubectl delete pod nginx-deployment-f77774fc5-cgs82 # 查看Pod详情 kubectl get pods -o wide 它会重新拉起来 查看创建过程 kubectl describe pod nginx-deployment 查看pod 的详细信息 kubectl describe pod nginx 4. pod 0. 特点 ​\tPod中封装着应用的容器（有的情况下是好几个容器），==存储、独立的网络IP==，管理容器如何运行\n的策略选项。Pod代表着部署的一个单位：kubernetes中应用的一个实例，可能由一个或者多个容器组\n合在一起共享资源。\n==网络==\n==每一个Pod都会被指派一个唯一的Ip地址==，在Pod中的==每一个容器共享网络命名空间==，包括==Ip地址和 网络端口。在同一个Pod中的容器可以同locahost进行互相通信==。当Pod中的容器需要与Pod外的实体进 行通信时，则需要通过端口等共享的网络资源。\n==存储==\nPod能够被指定共享存储卷的集合，在Pod中所有的容器能够访问共享存储卷，允许这些容器共享 数据。存储卷也允许在一个Pod持久化数据，以防止其中的容器需要被重启。\n1. 镜像拉取策略 pod的镜像拉取策略分为三种：\nalways（总是从官方下载镜像）\nnever（从不下载镜像）\nifnotpresent（如果本地没有镜像就从官方下载镜像）。\n2. 生命周期行为 1. 初始化容器（ init container ） 即应用程序的主容器启动之前要运行的容器，常用于为主容器 执行一些预置操作，它们具有两种典型特征。\n初始化容器必须运行完成直至结束，若某初始化容器运行失败，那么 kubernetes 需要重启它直到 成功完成。（注意：如果 pod 的 spec.restartPolicy 字段值为“ Never ”，那么运行失败的初始化 容器不会被重启。）\n每个初始化容器都==必须按定义的顺序串行运行==。\n2. 容器探测 容器探测（ container probe ）是 Pod 对象生命周期中的一项重要的日常任务，它是 kubelet 对\n容器周期性执行的健康状态诊断，诊断操作由容器的处理器（ handler ）进行定义。\nKubernetes 支持三种处理器用于 Pod 探测：\nExecAction ：在容器内执行指定命令，并根据其返回的状态码进行诊断的操作称为 Exec 探测， 状态码为 0 表示成功，否则即为不健康状态。\nTCPSocketAction ：通过与容器的某 TCP 端口尝试建立连接进行诊断，端口能够成功打开即为正 常，否则为不健康状态。\nHTTPGetAction ：通过向容器 IP 地址的某指定端口的指定 path 发起 HTTP GET 请求进行诊断， 响应码为 2xx 或 3xx 时即为成功，否则为失败。\n任何一种探测方式都可能存在三种结果： “Success”（成功） 、 “Failure”（失败） 、 “Unknown”\n（未知） ，只有 success 表示成功通过检测。\n3. 健康检查 存活探测 ==应用是否活着== 就绪探测 ==用是否准备好为请求提供服务== Liveness探测和Readiness探测是两种Health Check机制,如果不特意配置,Kubernetes将对两种探 测采取相同的默认行为,即通过判断容器启动进程的返回值是否为零来判断探测是否成功。\n两种探测的配置方法完全一样,支持的配置参数也一样。\n不同之处在于探测失败后的行为:Liveness 探测是重启容器;Readiness探测则是将容器设置为不可用,不接收Service转发的请求。\nLiveness探测和Readiness探测是独立执行的,二者之间没有依赖,所以可以单独使用,也可以同时使用。\n用Liveness探测判断容器是否需要重启以实现自愈;用Readiness探测判断容器是否已经准备好 对外提供服务\n5. pod 控制器 ​\tPod控制器是用于实现管理pod的中间层，确保pod资源符合预期的状态，pod的资源出现故障时， 会尝试 进行重启，当根据重启策略无效，则会重新新建pod的资源。 创建为具体的控制器对象之后，每个控制器均通过 API Server 提供的接口持续监控相关资源对象 的当前状态，并在因故障、更新或其他原因导致系统状态发生变化时，尝试让资源的当前状态想期望状 态迁移和逼近。\n一个 Pod 控制器资源至少应该包含三个基本的组成部分：\n标签选择器：匹配并关联 Pod 资源对象，并据此完成受其管控的 Pod 资源计数。 期望的副本数：期望在集群中精确运行着的 Pod 资源的对象量。 Pod模板：用于新建 Pod 资源对象的 Pod 模板资源。 常见的控制器\nReplicaSet: 代用户创建指定数量的 pod 副本数量，确保 pod 副本数量符合预期状态，并且支持滚动式自动扩容和缩容功能 Deployment: 工作在 ReplicaSet 之上，用于管理无状态应用，目前来说最好的控制器。支持滚动更新和回滚功能，还提供声明式配置。 DaemonSet: 用于确保集群中的每一个节点只运行特定的 pod 副本，常用于实现系统级后台任务。比如 ELK 服务 1. ReplicaSet控制器 示例\nvi nginx-rs.yml apiVersion: apps/v1 # api版本定义 kind: ReplicaSet # 定义资源类型为ReplicaSet metadata: name: nginx-rs # ReplicaSet的名称 namespace: default # 命名空间 spec: # ReplicaSet的规格定义 replicas: 2 # 定义副本数量为2个 selector: # 标签选择器，定义匹配Pod的标签 matchLabels: app: nginx template: # Pod的模板定义 metadata: # Pod的元数据定义 name: nginx-pod # 自定义Pod的名称 labels: # 定义Pod的标签 app: nginx #定义Pod的标签，需要和上面的标签选择器内匹配规则中定义的标签一致，可以多出其他标签 spec: # Pod的规格定义 containers: # 容器定义 - name: nginx # 容器名称 image: nginx:1.12 # 容器镜像 imagePullPolicy: IfNotPresent # 拉取镜像的规则 ports: # 暴露端口 - name: http # 端口名称 containerPort: 80 # 容器端口 创建rs 控制器\nkubectl apply -f nginx-rs.yaml 查看 rs 控制器\nkubectl get rs ==更新控制器一般来说都是修改配置文件 然后再apply==\n扩缩容, 使用scale 命令, (==用于临时的处理==), 永久的还是要使用配置文件\nkubectl scale replicasets nginx-rs --replicas=4 删除rs 控制器, 数 cascade=false 设置不删除pod\nkubectl delete replicasets nginx-rs --cascade=false 2. Deployment 控制器 vi nginx-deployment.yml yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 更新策略\n滚动更新: 滚动更新是==默认的更新策略==，==它在删除一些旧版本的Pod的同时补充创建一些新的Pod，更新期间 服务不会中断==。 滚动更新期间，应用升级期间还要确保可用的Pod对象数量不低于某些阈值。确保可以持续处理客 户端请求。变动的方式和Pod对象的数量范围将通过==spec.strategy.roollingUpdata.maxSurge和 spec.strategy.roollingUpdata.maxunavailable两个属性协同进行定义。两个参数用法如下：\nmaxSurge：指定升级期间存在的总Pod对象数量最多以超出期望值的个数，其值可以为0或者正整 数，也可以是一个期望值的百分比：例如如果期望值是3，当前的属性值为1，则表示Pod对象的总 数不能超过4个。\nmaxUnavailable：升级期间正常可用的Pod副本数（包括新旧版本）最多不能低于期望的个数、其 值可以是0或者正整数。也可以是一个期望值的百分比，默认值为1；该值意味着如果期望值是3， 那么在升级期间至少要有两个Pod对象处于正常提供服务的状态\n==查看pod的版本号==\nkubectl get pods -o custom-columns=Name:metadata.name,Image:spec.containers[0].image 重建更新: 先删除所有的Pod再根据新的模板创建新的Pod，中间会导致服务的不可用，用户要 么使用的是新版本，要么就是旧版本 执行回滚\nkubectl rollout undo deployment/nginx-deployment ==回滚只能在最近的两个版本之间回滚, 如果想要回滚到指定的节点需要加版本==\nkubectl rollout history deployment/nginx-deployment --revision=1 6. 数据存储 通过命令 kubectl explain pod.spec 可以查看 当前 kubernetes 版本支持的存储卷类型\n1. emptydir emptyDir 存储卷是 Pod 对象生命周期中的一个临时目录，类似于 Docker 上的 “docker 挂载\n卷” ，在 Pod 对象启动时即被创建，而在 Pod 对象被移除时会被一并删除（永久删除）。\n==一个容器崩溃了不会导致数据的丢失，因为容器的崩溃并不移除 Pod==。\n2. hostpath 半持久化, 需要配合nfs 使用\n7. service Kubernetes service 定义了这样一种抽象：一个 Pod 的逻辑分组，一种可以访问它们的策略 ——\n通常称为微服务。这一组 pod 能够被 Service 访问到，通常是通过 Label selector\nService 资源基于标签选择器将一组 Pod 定义成一个逻辑组合，并通过自己的 IP 地址和端口调度代理请求至组内的 Pod 对象之上，如下图所示，它向客户端隐藏了真实的、处理用户请求的 Pod 资源， 使得客户端的请求看上去就像是由 Service 直接处理并响应一样。 service 的 ip 也被称作为 cluster-ip\nService能够提供负载均衡的能力，但是在使用上有以下限制：\n只提供4层负载均衡能力，而没有7层功能，但有时我们可能需要更多的匹配规则来转发请求，这点上4层负载均衡是不支持的\n客户端只需要访问 Service 的 IP，==Kubernetes 则负责建立和维护 Service 与 Pod 的映射关系==。 无论后端 Pod 如何变化，对客户端不会有任何影响，因为 Service 没有变 service 的 类型\nClusterlp：默认类型，自动分配一个仅 Cluster 内部可以访问的虚拟 IP\nNodePort：在 ClusterIP 基础上为 Service 在每台机器上绑定一个端口，这样就可以通过： NodePort 来访问该服务\nLoadBalancer：在 NodePort 的基础上，借助 cloud provider 创建一个外部负载均衡器，并将请求转发到：NodePort\nExternalName：把集群外部的服务引入到集群内部来，在集群内部直接使用。没有任何类型代理\n被创建，这只有kubernetes 1.7或更高版本的 kube-dns 才支持\n1. clusterIp 类型为ClusterIP的service，这个service有一个Cluster-IP，其实就一个VIP，具体实现原理依靠\nkubeproxy组件，通过iptables或是ipvs实现。\n==注意：这种类型的service只能在集群内访问==\n2. NodePort NodePort的最主要功能是进行外网访问，我们是不能通过访问 8000 端口访问的，他是内网端\n口，外网访问需要访问映射出来的端口 8000:31337/TCP 这里映射出来的 nodeport 是 31337 ，\n还可以通过指定 nodePort 来指定端口，要求端口必须大于30000\n","date":"2024-07-14T17:44:10+08:00","permalink":"https://mikeLing-qx.github.io/p/k8s%E5%85%A5%E9%97%A8/","title":"K8s入门"},{"content":"1. RocketMq 简介 1. 特点 支持发布/订阅（Pub/Sub）和点对点（P2P）消息模型\n能够保证严格的消息顺序，在一个队列中可靠的先进先出（FIFO）和严格的顺序传递\n提供丰富的消息拉取模式，支持拉（pull）和推（push）两种消息模式\n推模式：在推模式下，==消息从服务器“推动”到消费者==。这意味着一旦有新的消息进入服务器，服务器都会将它们推送给消费者，消费者只需要接收和处理消息即可。这种模式下，消费者较为被动，主要关注点是如何处理推送过来的消息，适合消息处理速度较快，对实时性要求较高的场景。\n拉模式：在拉模式下，==消费者从服务器“拉取”消息==。这样，消费者可以决定何时获取消息，并可根据自身需求和能力去主动获取消息，因此可以更好地控制消息处理的速度和节奏。它适合处理的消息并不那么紧急，允许有一定延迟的场景，或者处理能力有限需要自我控制拉取速度的场景。\n值得注意的是，RocketMQ==默认采用的是推模式进行消息的消费==，但==实质上其内部对于消息的获取其实是拉模式==，区别在于RocketMQ的==客户端封装了详尽的拉取调用，使得对于终端使用者来说展示出的是推送模式==。\n单一队列百万消息的堆积能力，亿级消息堆积能力\n支持多种消息协议，如 JMS、MQTT 等\n分布式高可用的部署架构,满足至少一次消息传递语义\n原理:\nhttps://www.jianshu.com/p/2838890f3284\n主要优势\n支持==事务型消息==（消息发送和 DB 操作保持两方的最终一致性，RabbitMQ 和 Kafka 不支持）\n支持结合 RocketMQ 的多个系统之间==数据最终一致性==（==多方事务==，二方事务是前提）\n支持 18 个级别的==延迟消息==（RabbitMQ 和 Kafka 不支持）\n支持==指定次数和时间间隔的失败消息重发==（Kafka 不支持，RabbitMQ 需要手动确认）\n支持 ==Consumer 端 Tag 过滤==，减少不必要的网络传输（RabbitMQ 和 Kafka 不支持）\n==支持重复消费==（RabbitMQ 不支持，Kafka 支持）\n2. 运行架构 一个正常工作的RocketMQ包括四个部分。\nNameServer ：基于高可用设计产生的，用于服务发现和路由。正式应用时通常采用集群部署。==就相当于与祖册中心==, ==NameServer之间没有通信==, ==broker 需要到 所有的nameServer上去注册==\nBroker：实现==队列机制==，负责消息存储和转发。正式应用时也采用集群部署. 也就是==消息存储中心==, 它还==存储与消息相关的元数据==, Broker 有 Master 和 Slave 两种类型，Master 既可以写又可以读，Slave不可以写只可以读。\nProducer：消息生产者，生成消息并发送到RocketMQ中，生产者通常是我们自己实现的应用程序。\nConsumer：消息消费者，从RocketMQ中接收消息并进行业务处理。这部分也通常是我们自己实现的。\n运转流程\n1. NameServer启动\r2. Broker启动向NameServer注册\r3. 生产者再发送某个主题的消息前 先从 NameServer 获取Broker服务器地址列表(可能回是集群), 然后根据负载均衡算法, 从列表汇总选择一台Broker 进行消息发送\r4. NameServer 与每台broker 服务器保持长链接, 每隔30s 监测是否存活, 如果检测到\rBroker 宕机（使用心跳机制， 如果检测超过120S），则从路由注册表中将其移除。\r5. 消费者在订阅某个主题的消息之前从 NamerServer 获取 Broker 服务器地址列表（有可能是集群），但是消费者选择从 Broker 中 订阅消息，订阅规则由 Broker 配置决定\rMessage -\u0026gt; 要传输的信息\nTopic -\u0026gt; 一条消息必须要有一个Topic, ==可以看做是你的信件要邮寄的地址==\nTag -\u0026gt; 一条消息也可以拥有一个==可选的标签（Tag）和额外的键值对==，它们可以用于设置\n==一个业务 key 并在 Broker 上查找此消息==以便在开发期间查找问题。\n3. 基础概念 Producer Group: 只是一个概念性的, 配不配都没有什么影响\n生产者组：是一类生产者的集合，通常发送同一类消息并且发送逻辑一致\nProducer:\n三种消息发送方式\n1: 同步\n2: 异步\n3: 单向发送\nConsumer Group\n消费者组：是一类消费者的集合，通常消费同一类消息并且消费逻辑一致\nConsumer\n两种消费模式:\n拉取型 推送型: 实际上也是拉取消息, 但是对拉取逻辑进行了封装, 将消息达到时执行的回调接口留给用户来实现 Broker (存储消息);\n每个Broker与Name Server集群中的所有节点建立长连接，定时(每隔30s)==注册Topic信息到所有Name Server==。Name Server定时(每隔10s)扫描所有存活broker的连接，如果Name Server超过2分钟没有收到心跳，则Name Server断开与Broker的连接。\nNameServer\n作用和zookeeper类似, 用来保存broker相关元素, 并给producer 和comsumer 查找broker 消息, ==Producer 在发送消息前会根据 Topic 到 NameServer 获取到 Broker 的路由信息，Consumer 也会定时获取 Topic 的路由信息==。\nname-server充当==路由消息的提供者==。生产者或消费者能够通过name-server查找各主题相应的 Broker IP 列表。多个 Namesrv 实例组成集群，但相互独立，没有信息交换\ntopic 消息主题 (==理解为rebbit 的 exchange==); 可以看做消息的规类\ntag 消费标签; (==理解为 rabbit 的 routing key==)\n标签, Topic的二级分类, 可以理解为==消息队列==, ==一个Topic 可以设置多个Tag==, 发送消息时执行该消息的Topic, RockerMq ==会轮询该Topic 下所有的队列将消息发出去==\nmessage queue 消息队列\u0026ndash;\u0026gt; 主题被划分为一个或多个子主题. 就是消息队列\nTopic 和 Tags 使用示例\n以天猫交易平台为例，订单消息，支付消息属于不同业务类型的消息，分别创建 Topic_Order 和 Topic_Pay，其中订单消息根据商品品类以不同的 Tag 再进行细分，如电器类、男装类、女装类、化妆品类，最后他们都被各个不同的系统所接收。\n4. windows 开发搭建 1. rocketMq 参考资料 https://blog.csdn.net/zxl646801924/article/details/105637278\n注意: ==java环境为1.8==\nnameserver 启动\nstart mqnamesrv.cmd\rbroker启动\nstart mqbroker.cmd -n 127.0.0.1:9876 autoCreateTopicEnable=true\r发送消息失败, 空间不足, 修改启动脚本\n参考资料 https://blog.csdn.net/muriyue6/article/details/130607492\nrunbroker.sh文件添加\rset \u0026quot;JAVA_OPT=%JAVA_OPT% -server -Xms512m -Xmx512m -Xmn128m -Drocketmq.broker.diskSpaceWarningLevelRatio=0.99\u0026quot;，使磁盘使用超过99%再报错，根本的解决办法是保证磁盘空间永远不超过90%\r验证Mq服务\n启动consumer set NAMESRV_ADDR=127.0.0.1:9876\rtools.cmd org.apache.rocketmq.example.quickstart.Consumer\r启动producer set NAMESRV_ADDR=127.0.0.1:9876\rtools.cmd org.apache.rocketmq.example.quickstart.Producer\r2. rocketmq-dashboard https://github.com/apache/rocketmq-dashboard\nmvn clean package -Dmaven.test.skip=true\rjava -jar target/rocketmq-dashboard-1.0.1-SNAPSHOT.jar\r5. 检测mq服务是否正常 \u0026gt; export NAMESRV_ADDR=localhost:9876\r\u0026gt; sh bin/tools.sh org.apache.rocketmq.example.quickstart.Producer\rSendResult [sendStatus=SEND_OK, msgId= ...\r如下所示官方提供这个例子属于生产者，==用于发送消息，运行之后会发送大量的消息==，之后就会退出\n接收消息\nsh bin/tools.sh org.apache.rocketmq.example.quickstart.Consumer\rConsumeMessageThread_%d Receive New Messages: [MessageExt...\r上面的消息发送完毕之后就会退出，在同一窗口中可以使用消费者类来进行接收消息, 是多线程的消费\n2. 问题 rocketMq 发送消息的时候, 默认的队列是什么? 不需要单独创建和指定吗?\n默认是四个 rocketMq dashboard 要如何使用?\nrocketMq 的==消费策略不可以随意更改, 可能会造成重复消费和 队列没有消费者消费==\n为什么? rocketMq 的顺序消息是怎么实现的? 顺序消息有几种类型\nrocketMq 有哪些消息投递策略\n3. 顺序消息 1. 概念总结 ==rocketMq 里面的局部顺序, 依靠的是broker-queue 中的分段锁实现的, queue 中的消息一定是先入先出的==\n局部顺序有一个很关键的点，在==一个消费者集群的情况下==\n消费者1先去Queue拿消息，它拿到了北京订单1，它拿完后，\n消费者2去queue拿到的是 北京订单2。\n拿的顺序是没毛病了，==但关键是先拿到不代表先消费完它==。\n会存在虽然你消费者1先拿到北京订单 但由于网络等原因，消费者2比你真正的先消费消息。这是不是很尴尬了。\n实际上\n==Broker采用的是分段锁，它不是锁整个Broker而是锁里面的单个Queue==，因为只要锁单个 Queue就可以保证局部顺序消费了。\n所以最终的消费者这边的逻辑就是\n消费者1去Queue拿 北京订单1，==它就锁住了整个Queue，只有它消费完成并返回成功后，这个锁才会释放==。\n然后下一个消费者去拿到 北京订单2 同样锁住当前Queue,这样的一个过程来真正保证对同一个 Queue能够真正意义上的顺序消费，而不仅仅是顺序取出\n注意事项\n顺序消息暂不支持广播模式。\n顺序消息不支持异步发送方式，否则将无法严格保证顺序。\n==建议同一个 Group ID 只对应一种类型的 Topic，即不同时用于顺序消息和无序消息的收发==。\n对于全局顺序消息，建议创建broker个数 \u0026gt;=2。\n2. 使用 1. 队列选择器 public class SelectorFactory {\r/**\r* 工厂模式获取MessageQueueSelector\r*\r* @param value\r* @return\r*/\rpublic static MessageQueueSelector getMessageQueueSelector(String value) {\r//如果value不为空使用hash选择器\rif (StringUtils.isNotEmpty(value)) {\rreturn new SelectMessageQueueByHash();\r}\r//如果value为空使用随机选择器\rreturn new SelectMessageQueueByRandom();\r}\r}\r2. 消息生产者 @Component\rpublic class MQProducer {\r@Autowired\rDefaultMQProducer defaultMQProducer;\r/**\r* 同步发送消息\r*\r* @param taxiBO\r*/\rpublic void send(TaxiBO taxiBO) {\rif (null == taxiBO) {\rreturn;\r}\rSendResult sendResult = null;\rtry {\r//获取消息对象\rMessage message =\rRocketMQHelper.buildMessage(DispatchConstant.SEQ_TOPIC, taxiBO);\r//根据区域编码获取队列选择器\rMessageQueueSelector selector =\rSelectorFactory.getMessageQueueSelector(taxiBO.getAreaCode());\r//发送同步消息\rsendResult = defaultMQProducer.send(message, selector,\rtaxiBO.getAreaCode(), 10000);\r} catch (Exception e) {\re.printStackTrace();\r}\rif (null != sendResult) {\rSystem.out.println(sendResult.toString());\r}\r}\r}\r4. 消息投递策略 ==一个队列只能被一个消费者订阅 (可以保证不被重复消费), 一个消费者可以订阅多个队列==\n集群消费 clustering (默认); 就是分组\n模式下一个消费者集群共同消费一个主题的多个队列，一个队列只会被\r一个消费者消费，如果某个消费者挂掉，分组内其它消费者会接替挂掉的消费者继续消费\r广播消费 broadcasting\n发给消费者组中的每一个消费者进行消费\r==commitlog 有文件过期时间, 默认是72小时, 当超过这个时间文件没有任何没有更新的话, 文件会被删除, 无法找回==\n==超过超时时间, 消息发送失败的, 是不会进行重试的==\n==集群部署需要禁止自动创建主题==\n消息重试\nrabbitMq 是会回到队列的头部, 重新发送 rocketMq 则是会新开一个新的队列, 已当前消费组为单位, 与具体的主题无关, 不会立马重试, 大概5小时内,默认会重试16次, 如果全部失败的话就会到死信队列里面 1. 生产者投递策略 轮询投递 顺序投递 RockeMQ 采用了这种实现方案：对于相同订单号的消息，通过一定的策略，将其 放置在一个queue队列中 ，然后 消费者 再采用一定的策略(一个线程独立处理一个queue ,保证处理消息的顺序性)，能够保证消费的顺序性\npackage org.apache.rocketmq.client.producer;\rimport java.util.List;\rimport org.apache.rocketmq.common.message.Message;\rimport org.apache.rocketmq.common.message.MessageQueue;\rpublic interface MessageQueueSelector {\rMessageQueue select(List\u0026lt;MessageQueue\u0026gt; var1, Message var2, Object var3);\r}\r投递策略 策略实现类 随机分配策略 SelectMessageQueueByRandom 使用了简单的随机数选择算法 基于Hash分配 SelectMessageQueueByHash 根据附加参数的Hash值，按照消 2. 消费者分配队列 BROADCASTING :广播式消费，这种模式下，一个消息会被通知到每一个 消费者 CLUSTERING : 集群式消费，这种模式下，一个消息最多只会被投递到一个 消费者 上进行消费 平局分配算法, 并不是指的严格意义上的完全平均，如上面的例子中，==10个queue，而 消费者只有4个==，无法是整除关系，除了整除之外的==多出来的queue,将依次根据消费者的顺序均摊==。\n一致性hash分配算法, 会将 consumer消费者 作为Node节点构造成一个hash环，然后 queue队列 通过这个\nhash环来决定被分配给哪个 consumer消费者 。\n默认消费者使用使用了 AllocateMessageQueueAveragely 平均分配策略\n//创建一个消息消费者，并设置一个消息消费者组，并指定使用一致性hash算法的分配策略\rDefaultMQPushConsumer consumer = new\rDefaultMQPushConsumer(null,\u0026quot;rocket_test_consumer_group\u0026quot;,null,new\rAllocateMessageQueueConsistentHash());\r5. rocketMq 消息存储 CommitLog：这是消息存储的核心文件，所有的消息都先写入 CommitLog。它是一个顺序写的日志文件，写入性能高。 ConsumeQueue：这是消息的逻辑队列，存储了消息在 CommitLog 中的物理偏移量、消息大小和消息的 Tag 哈希值。每个 Topic 下的每个队列都有一个对应的 ConsumeQueue。 IndexFile：这是消息的索引文件，存储了消息的索引信息，可以通过消息的 Key 快速查找消息。 这些文件都存储在磁盘上， Broker 会定期进行刷盘操作，将内存中的数据写入磁盘，保证消息的持久性。 6. 消息保障 1. 生产端保障 使用可靠的消息发送方式, 同步发送比异步发送可靠, ==两者都可以对消息的响应结果做处理==, 只有one-way(单向模式) 的时候才会不需要处理, 而且==只有同步发送才有重试==, ==并且超时是不会进行重试的==\n注意生产端重试\n生产禁止自动创建topic\n发送重试源码\n/**\r* 说明 抽取部分代码\r*/\rprivate SendResult sendDefaultImpl(Message msg, final CommunicationMode\rcommunicationMode, final SendCallback sendCallback, final long timeout) {\r//1、获取当前时间\rlong beginTimestampFirst = System.currentTimeMillis();\rlong beginTimestampPrev ;\r//2、去服务器看下有没有主题消息\rTopicPublishInfo topicPublishInfo =\rthis.tryToFindTopicPublishInfo(msg.getTopic());\rif (topicPublishInfo != null \u0026amp;\u0026amp; topicPublishInfo.ok()) {\rboolean callTimeout = false;\r//3、通过这里可以很明显看出 如果不是同步发送消息 那么消息重试只有1次\rint timesTotal = communicationMode == CommunicationMode.SYNC ? 1 +\rthis.defaultMQProducer.getRetryTimesWhenSendFailed() : 1;\r//4、根据设置的重试次数，循环再去获取服务器主题消息\rfor (times = 0; times \u0026lt; timesTotal; times++) {\rMessageQueue mqSelected =\rthis.selectOneMessageQueue(topicPublishInfo, lastBrokerName);\rbeginTimestampPrev = System.currentTimeMillis();\rlong costTime = beginTimestampPrev - beginTimestampFirst;\r//5、前后时间对比 如果前后时间差 大于 设置的等待时间 那么直接跳出for循环了 这就\r说明连接超时是不进行多次连接重试的\rif (timeout \u0026lt; costTime) {\rcallTimeout = true;\rbreak\r}\r//6、如果超时直接报错\rif (callTimeout) {\rthrow new RemotingTooMuchRequestException(\u0026quot;sendDefaultImpl call\rtimeout\u0026quot;);\r}\r}\r}\r}\r==自动创建topic==\nautoCreateTopicEnable 设置为true 标识开启自动创建topic\n消息发送时如果根据topic没有获取到 路由信息，则会根据默认的topic去获取，获取到路由信息后 选择一个队列进行发送，发送时报文会带上默认的topic以及默认的队列数量。\n消息到达broker后，broker检测没有topic的路由信息，则查找默认topic的路由信息，查到表示开 启了自动创建topic，则会根据消息内容中的默认的队列数量在本broker上创建topic，然后进行消\n息存储。\nbroker创建topic后并不会马上同步给namesrv，而是每30进行汇报一次，更新namesrv上的 topic路由信息，producer会每30s进行拉取一次topic的路由信息，更新完成后就可以正常发送消 息。更新之前一直都是按照默认的topic查找路由信息。 2. 消费端保障 1. 消息确认 业务实现消费回调的时候，当且仅当此回调函数返回 ==ConsumeConcurrentlyStatus.CONSUME_SUCCESS== ，RocketMQ才会认为这批消息（默认是1 条）是消费完成的。\n2. 消费异常 数据库异常，余额不足扣款失败等一切业务认为消息需要重试的场 景，只要返回 ==ConsumeConcurrentlyStatus.RECONSUME_LATER== ，RocketMQ就会认为这批消息 消费失败了。\n为了保证消息是肯定被至少消费成功一次，RocketMQ会把这批消息重发回Broker（==topic不是原 topic而是这个消费租的RETRY topic==），在延迟的某个时间点（默认是10秒，业务可设置）后，==再次投 递到这个ConsumerGroup==。而如果一直这样重复消费都持续失败到一定次数（默认16次），就会==投递 到DLQ死信队列==。应用可以监控死信队列来做人工干预。\n3. 死信队列 当一条消息初次消费失败，消息队列 RocketMQ 会自动进行消息重试；\n达到最大重试次 数后，若 消费依然失败，则表明消费者在正常情况下无法正确地消费该消息，\n此时，消息队列 RocketMQ 不会立 刻将消息丢弃，而是将其发送到该消费者对应的特殊队列中。 在消息队列 RocketMQ 中，这种正常情况 下无法被消费的消息称为死信消息，存储死信消息的特殊队列称为死信队列\n==不会再被消费者正常消费==\n有效期与正常消息相同，均为 3 天，3 天后会被自动删除。故死信消息应在产生的 3 天内及时处理\n==一个死信队列对应一个消费者组，而不是对应单个消费者实例==\n一个死信队列包含了对应的 ==Group ID所产生的所有死信消息，不论该消息属于哪个Topic若一个 Group ID 没有产生过死信消息，则 RocketMQ 不会为其创建相应的死信队列==\n7. 应用 1. spring cloud stream 整合 以统一的一套API来进行消息的发送和消费, 底层消息中间件的实现细节由各消息中间件的 binder 完成\nhttps://www.jianshu.com/p/7f8fd90564ca?utm_campaign=hugo\ngitee: https://gitee.com/jbcode/GitRepository.git\n1. 依赖 \u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;UTF-8\u0026quot;?\u0026gt;\r\u0026lt;project xmlns=\u0026quot;http://maven.apache.org/POM/4.0.0\u0026quot;\rxmlns:xsi=\u0026quot;http://www.w3.org/2001/XMLSchema-instance\u0026quot;\rxsi:schemaLocation=\u0026quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026quot;\u0026gt;\r\u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt;\r\u0026lt;groupId\u0026gt;com.erbadagang.springcloud.stream\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;sc-stream-rocketmq-producer\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;0.0.1\u0026lt;/version\u0026gt;\r\u0026lt;properties\u0026gt;\r\u0026lt;maven.compiler.target\u0026gt;1.8\u0026lt;/maven.compiler.target\u0026gt;\r\u0026lt;maven.compiler.source\u0026gt;1.8\u0026lt;/maven.compiler.source\u0026gt;\r\u0026lt;spring.boot.version\u0026gt;2.2.4.RELEASE\u0026lt;/spring.boot.version\u0026gt;\r\u0026lt;spring.cloud.version\u0026gt;Hoxton.SR1\u0026lt;/spring.cloud.version\u0026gt;\r\u0026lt;spring.cloud.alibaba.version\u0026gt;2.2.0.RELEASE\u0026lt;/spring.cloud.alibaba.version\u0026gt;\r\u0026lt;/properties\u0026gt;\r\u0026lt;!--\r引入 Spring Boot、Spring Cloud、Spring Cloud Alibaba 三者 BOM 文件，进行依赖版本的管理，防止不兼容。\r在 https://dwz.cn/mcLIfNKt 文章中，Spring Cloud Alibaba 开发团队推荐了三者的依赖关系\r--\u0026gt;\r\u0026lt;dependencyManagement\u0026gt;\r\u0026lt;dependencies\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;${spring.boot.version}\u0026lt;/version\u0026gt;\r\u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt;\r\u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-cloud-dependencies\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;${spring.cloud.version}\u0026lt;/version\u0026gt;\r\u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt;\r\u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;com.alibaba.cloud\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-cloud-alibaba-dependencies\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;${spring.cloud.alibaba.version}\u0026lt;/version\u0026gt;\r\u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt;\r\u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;/dependencies\u0026gt;\r\u0026lt;/dependencyManagement\u0026gt;\r\u0026lt;dependencies\u0026gt;\r\u0026lt;!-- 引入 SpringMVC 相关依赖，并实现对其的自动配置 --\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;!-- 引入 Spring Cloud Alibaba Stream RocketMQ 相关依赖，将 RocketMQ 作为消息队列，并实现对其的自动配置 --\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;com.alibaba.cloud\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-cloud-starter-stream-rocketmq\u0026lt;/artifactId\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;/dependencies\u0026gt;\r\u0026lt;/project\u0026gt;\r2. 生产端配置文件 spring:\rapplication:\rname: stream-rocketmq-producer-application\rcloud:\r# Spring Cloud Stream 配置项，对应 BindingServiceProperties 类\rstream:\r# Binding 配置项，对应 BindingProperties Map\rbindings:\rerbadagang-output:\rdestination: ERBADAGANG-TOPIC-01 # 目的地。这里使用 RocketMQ Topic\rcontent-type: application/json # 内容格式。这里使用 JSON\rtrek-output:\rdestination: TREK-TOPIC-01 # 目的地。这里使用 RocketMQ Topic\rcontent-type: application/json # 内容格式。这里使用 JSON\r# Spring Cloud Stream RocketMQ 配置项\rrocketmq:\r# RocketMQ Binder 配置项，对应 RocketMQBinderConfigurationProperties 类\rbinder:\rname-server: 101.133.227.13:9876 # RocketMQ Namesrv 地址\r# RocketMQ 自定义 Binding 配置项，对应 RocketMQBindingProperties Map\rbindings:\rerbadagang-output:\r# RocketMQ Producer 配置项，对应 RocketMQProducerProperties 类\rproducer:\rgroup: test # 生产者分组\rsync: true # 是否同步发送消息，默认为 false 异步。\rserver:\rport: 18080\r# 同时我们设置了2个binding，模拟2个topic情形。\r3. 消费端配置文件 spring:\rapplication:\rname: erbadagang-consumer-application\rcloud:\r# Spring Cloud Stream 配置项，对应 BindingServiceProperties 类\rstream:\r# Binding 配置项，对应 BindingProperties Map\rbindings:\rerbadagang-input:\rdestination: ERBADAGANG-TOPIC-01 # 目的地。这里使用 RocketMQ Topic\rcontent-type: application/json # 内容格式。这里使用 JSON\rgroup: erbadagang-consumer-group-ERBADAGANG-TOPIC-01 # 消费者分组,命名规则：组名+topic名\rtrek-input:\rdestination: TREK-TOPIC-01 # 目的地。这里使用 RocketMQ Topic\rcontent-type: application/json # 内容格式。这里使用 JSON\rgroup: trek-consumer-group-TREK-TOPIC-01 # 消费者分组,命名规则：组名+topic名\r# Spring Cloud Stream RocketMQ 配置项\rrocketmq:\r# RocketMQ Binder 配置项，对应 RocketMQBinderConfigurationProperties 类\rbinder:\rname-server: 101.133.227.13:9876 # RocketMQ Namesrv 地址\r# RocketMQ 自定义 Binding 配置项，对应 RocketMQBindingProperties Map\rbindings:\rerbadagang-input:\r# RocketMQ Consumer 配置项，对应 RocketMQConsumerProperties 类\rconsumer:\renabled: true # 是否开启消费，默认为 true\rbroadcasting: false # 是否使用广播消费，默认为 false 使用集群消费，如果要使用广播消费值设成true。\rserver:\rport: ${random.int[10000,19999]} # 随机端口，方便启动多个消费者\r发送消息 @OutPut 接受消息 @ Input 2. springboot 集成 rocketMq 1. 知识点: 泛型方法 private static \u0026lt;T\u0026gt; void getConsumer() {\r}\rhttps://blog.csdn.net/zxl646801924/article/details/105659481\n2. rocketmq-spring-boot-starter https://blog.csdn.net/m0_37968982/article/details/109066088\n1. 引入依赖 \u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.apache.rocketmq\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;rocketmq-spring-boot-starter\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;2.1.0\u0026lt;/version\u0026gt;\r\u0026lt;/dependency\u0026gt;\r2. 配置rocketMq rocketmq:\rname-server: 127.0.0.1:9876\rproducer:\rgroup: my-producer-group\r3. 构建生产者 @RestController\rpublic class Producer {\r@Autowired\rprivate RocketMQTemplate rocketMQTemplate;\r@GetMapping(\u0026quot;send\u0026quot;)\rpublic void send(){\rrocketMQTemplate.convertAndSend(\u0026quot;first-topic\u0026quot;,\u0026quot;你好,Java旅途\u0026quot;);\r}\r}\r4. 消费者 @Component\r@RocketMQMessageListener(topic = \u0026quot;first-topic\u0026quot;,consumerGroup = \u0026quot;my-consumer-group\u0026quot;)\r@Slf4j\rpublic class Consumer implements RocketMQListener\u0026lt;String\u0026gt; {\r@Override\rpublic void onMessage(String message) {\rlog.info(\u0026quot;我收到消息了！消息内容为：\u0026quot;+message);\r}\r}\r3. 打车服务 1. 车辆调度 2. 司机自动接单 3. 用户下车 4. 总体流程 4. RocketMQPushConsumerLifecycleListener 接口 RocketMQPushConsumerLifecycleListener 是一个用于管理 RocketMQ 推送消费者生命周期的接口。实现这个接口的类可以在==消费者启动前和启动后执行一些自定义逻辑==。\n示例\npublic class MyConsumerLifecycleListener implements RocketMQPushConsumerLifecycleListener {\r@Override\rpublic void prepareStart(DefaultMQPushConsumer consumer) {\r// 在消费者启动前执行一些初始化操作\rconsumer.setConsumeThreadMax(10); // 设置最大消费线程数\rSystem.out.println(\u0026quot;Consumer is about to start.\u0026quot;);\r}\r@Override\rpublic void prepareShutdown(DefaultMQPushConsumer consumer) {\r// 在消费者关闭前执行一些清理操作\rSystem.out.println(\u0026quot;Consumer is about to shut down.\u0026quot;);\r}\r}\r8. 任务 复习文档 整理面试题 查看代码使用 复述 rocket 打车 搭建一个本机的 rocketMq kafka 进行学习 9. RocketMq 的事务消息 面试官：什么是RocketMQ的事务消息\n事务消息就是MQ提供的类似XA的分布式事务能力，通过事务消息可以达到分布式事务的最终一致性。\n半事务消息就是MQ收到了生产者的消息，但是没有收到二次确认，不能投递的消息。\n4.1 实现原理\n生产者先发送一条半事务消息到MQ\nMQ收到消息后返回ack确认\n生产者开始执行本地事务\n如果事务执行成功发送commit到MQ，失败发送rollback\n如果MQ长时间未收到生产者的二次确认commit或者rollback，MQ对生产者发起消息回查\n生产者查询事务执行最终状态\n根据查询事务状态再次提交二次确认\n最终，如果MQ收到二次确认commit，就可以把消息投递给消费者，反之如果是rollback，消息会保存下来并且在==3天后被删除==。\n10. 消费者闲置 在使用 RocketMQ 时，如果存在多个消费者，但某些消费者闲置不接收消息\n消费者分组在 RocketMQ 中起着重要作用。一个消费者分组中的所有消费者共同消费主题中的消息。消息队列（queue）会在同一个分组的消费者之间均匀分配：\n消费者分组内部的消费者不均衡：==如果消费者数量大于消息队列的数量==，则多出的消费者会闲置。例如，如果一个主题有4个消息队列，但有5个消费者在同一个分组内，那么多出的一个消费者将不会分配到任何队列，从而闲置。 ==类似于kafka 中的分区数量少于 消费者数量== ","date":"2024-07-14T16:23:18+08:00","permalink":"https://mikeLing-qx.github.io/p/rocketmq/","title":"RocketMq"},{"content":"1. 基础组件 1. 角色 broker：节点，就是你看到的机器\nprovider：生产者，发消息的\nconsumer：消费者，读消息的\nzookeeper：信息中心，记录kafka的各种信息的地方\n==controller==：其中的一个broker，作为leader身份来负责管理整个集群。如果挂掉，借助zk重新选\n主\ncontroller_epoch: leader身份变更的次数 ​\n2. 副本集合 topic：主题，一个消息的通道，收发总得知道消息往哪投\npartition：分区，==每个主题可以有多个分区分担数据的传递==，多条路并行，吞吐量大\nReplicas：副本，==每个分区可以设置多个副本==，副本之间数据一致。相当于备份，有备胎更可靠\nleader \u0026amp; follower：主从，上面的这些副本里有1个身份为leader，其他的为follower。leader处理\npartition的所有读写请求\nAR（Assigned Repllicas）：所有副本的统称，AR=ISR+OSR\nISR（In-Sync Replicas）：同步中的副本，可以参与leader选主。一旦落后太多（数量滞后和时间\n滞后两个维度）会被踢到OSR。\nOSR（Out-Sync Relipcas）：踢出同步的副本，一直追赶leader，追上后会进入ISR\n3. 消息标记 ==offset== 每条消息在Kafka==分区中都有一个唯一的offset==，它被用来==标记消息在分区（Partition）中的位置==。这是一个递增的==长整数值==。消费者使用这个offset来跟踪他们在每个分区已经读到的位置。当消费者消费了一条消息之后，它需要更新保存的offset值。下次消费时，它会从保存的offset值开始消费。\n这是Kafka中消费者消费消息的核心机制，因为Kafka本身并不追踪哪些消息已经被消费，这个工作是由消费者自己完成的。\n==需要注意的是==，Kafka允许消费者提交他们已经处理到的offset。这样，如果消费者崩溃了并再次启动，它可以从上次提交的offset开始消费，避免重复处理相同的消息。然而，这也意味着如果你的==消费者崩溃在提交offset和处理消息之间的时间窗口，你可能会重复消费一些消息==。这就需要你的处理逻辑具备幂等性，即重复处理相同的消息不会导致问题。\nHigh Watermark）和 LEO（最后日志偏移量，Log End Offset）是两个非常重要的概念，它们主要用于消息的处理和数据的同步。\n==HW（High Watermark）==： 高水位是代表了==消费者能够消费到的最大的 offset==。也就是说，高于这个标记的消息对消费者是不可见的。当 follower 副本将 leader 副本中的消息复制到本地之后，它就会把自己的 HW 更新到和 leader 副本的 HW 一致。只有当所有的 follower 副本的 HW 都已经达到消息的 offset 时，这条消息才认为是 \u0026ldquo;已提交\u0026rdquo;（committed），也只有 \u0026ldquo;已提交\u0026rdquo; 的消息才可以被消费者消费。 ==LEO（Log End Offset）==：LEO 是每个副本（包括 leader 副本和 follower 副本）最新消息的 offset + 1。换句话说，LEO 代表了下一条消息将要写入的位置。在正常情况下，leader 副本的 LEO 是大于等于 follower 副本的 LEO 的，当 follower 成功从 leader 复制消息之后，它的 LEO 会增大。 2. kafka 搭建 使用wsl\n启动命令 #docker-compose.yml #注意hostname问题，ip地址：192.168.10.30，换成你自己服务器的 #docker-compose up -d 启动 version: '3' services: zookeeper: image: zookeeper:3.4.13 kafka-1: container_name: kafka-1 image: wurstmeister/kafka:2.12-2.2.2 ports: - 10903:9092 environment: KAFKA_BROKER_ID: 1 HOST_IP: kafka-1 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 #docker部署必须设置外部可访问ip和端口，否则注册进zk的地址将不可达造成外部无法连接 KAFKA_ADVERTISED_HOST_NAME: kafka-1 KAFKA_ADVERTISED_PORT: 9092 volumes: - /etc/localtime:/etc/localtime depends_on: - zookeeper kafka-2: container_name: kafka-2 image: wurstmeister/kafka:2.12-2.2.2 ports: - 10904:9092 environment: KAFKA_BROKER_ID: 2 HOST_IP: kafka-2 KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_ADVERTISED_HOST_NAME: kafka-2 KAFKA_ADVERTISED_PORT: 9092 volumes: - /etc/localtime:/etc/localtime depends_on: - zookeeper 2024-06-19 17:09:01 [2024-06-19 09:09:01,482] WARN [Controller id=2, targetBrokerId=2] Connection to node 2 (/127.0.0.1:10904) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient) 3. Kafka命令行工具 消费者数量如果大于分区数量可能会出现, 有消费者为闲置的情况\n1. 主题创建 #进入容器 docker exec -it kafka-1 bash #进入bin目录 cd /opt/kafka/bin #创建 kafka-topics.sh --zookeeper zookeeper:2181 --create --topic test --partitions 2 --replication-factor 1 2. 查看主题 kafka-topics.sh --zookeeper zookeeper:2181 --list kafka-topics.sh --zookeeper zookeeper:2181 --describe --topic test 3. 消息收发 #使用docker连接任意集群中的一个容器 docker exec -it kafka-1 sh #进入kafka的容器内目录 cd /opt/kafka/bin #客户端监听 kafka-console-consumer.sh --bootstrap-server kafka-1:9092,kafka-2:9092 --topic test #另起一个终端，验证发送 kafka-console-producer.sh --broker-list kafka-1:9092,kafka-2:9092 --topic test 4. 分组消费 5. 消息同步 ","date":"2024-05-15T16:13:29+08:00","permalink":"https://mikeLing-qx.github.io/p/kafka/","title":"Kafka"},{"content":"0. nginx 配置 nginx配置文件主要分成四个部分：\nmain，全局设置，影响其它部分所有设置 server，主机服务相关设置，主要用于指定虚拟主机域名、IP和端口 location，URL匹配特定位置后的设置，反向代理、内容篡改相关设置 upstream，上游服务器设置，负载均衡相关配置 server继承main，location继承server；upstream既不会继承指令也不会被继承。\n​\t在这四个部分当中，每个部分都包含若干指令，这些指令主要包含Nginx的主模块指令、事件模块 指令、HTTP核心模块指令，同时每个部分还可以使用其他HTTP模块指令，例如Http SSL模块、 HttpGzip Static模块和Http Addition模块等。\nnginx 在 linux 的安装位置 一般为 /etc/nginx\n示例配置\n# 全局块配置 user administrator administrators; # 配置用户或者组，默认为nobody nobody。 worker_processes 2; # 允许生成的进程数，默认为1 pid /nginx/pid/nginx.pid; # 指定nginx进程运行文件存放地址 events { accept_mutex on; # 设置网路连接序列化，防止惊群现象发生，默认为on multi_accept on; # 设置一个进程是否同时接受多个网络连接，默认为off worker_connections 1024; # 最大连接数，默认为512 # use epoll; # 事件驱动模型，可选select|poll|kqueue|epoll等 } http { include mime.types; # 文件扩展名与文件类型映射表 default_type application/octet-stream; # 默认文件类型，默认为text/plain # access_log off; # 取消服务日志 log_format myFormat '$remote_addr–$remote_user [$time_local] $request ' '$status $body_bytes_sent $http_referer $http_user_agent ' '$http_x_forwarded_for'; # 自定义格式 access_log log/access.log myFormat; # combined为日志格式的默认值 sendfile on; # 允许sendfile方式传输文件，默认为off sendfile_max_chunk 100k; # 每个进程每次调用传输数量不能大于设定的值，默认为0 keepalive_timeout 65; # 连接超时时间，默认为75s upstream mysvr { server 127.0.0.1:7878; server 192.168.10.121:3333 backup; # 热备 } error_page 404 https://www.baidu.com; # 错误页重定向 server { keepalive_requests 120; # 单连接请求上限次数 listen 4545; # 监听端口 server_name 127.0.0.1; # 监听地址 location ~*^.+$ { # 请求的url过滤，正则匹配 # root path; # 根目录 # index vv.txt; # 设置默认页 proxy_pass http://mysvr; # 请求转向mysvr 定义的服务器列表 deny 127.0.0.1; # 拒绝的ip allow 172.18.5.54; # 允许的ip } } } error_log log/error.log debug; # 指定日志路径和级别 1. upstream 负载均衡策略 #动态服务器组 upstream dynamicserver { server 192.168.64.1:9001; #tomcat 1 server 192.168.64.1:9002; #tomcat 2 server 192.168.64.1:9003; #tomcat 3 server 192.168.64.1:9004; #tomcat 4 } 在轮询中，==如果服务器down掉了，会自动剔除该服务器==。\n默认配置就是轮询策略。\n此策略适合服务器配置相当，无状态且短平快的服务使用\n==weight权重方式==\n#动态服务器组 upstream dynamicserver { server 192.168.64.1:9001 weight=2; #tomcat 1 server 192.168.64.1:9002; #tomcat 2 server 192.168.64.1:9003 backup; #tomcat 3 server 192.168.64.1:9004 max_fails=3 fail_timeout=20s; #tomcat 4 } 权重越高分配到需要处理的请求越多。 此策略可以与least_conn和ip_hash结合使用。 此策略比较适合服务器的硬件配置差别比较大的情况。 ==ip_hash==\n指定负载均衡器按照基于客户端IP的分配方式，这个方法确保了相同的客户端的请求一直发送到相\n同的服务器，以保证session会话。这样每个访客都固定访问一个后端服务器，可以解决session不\n能跨服务器的问题。\nupstream dynamicserver { ip_hash; #保证每个访客固定访问一个后端服务器 server 192.168.64.1:9001 weight=2; #tomcat 1 server 192.168.64.1:9002; #tomcat 2 server 192.168.64.1:9003 backup; #tomcat 3 server 192.168.64.1:9004 max_fails=3 fail_timeout=20s; #tomcat 4 } 在nginx版本1.3.1之前，不能在ip_hash中使用权重（weight）。 ip_hash不能与backup同时使用。 此策略适合有状态服务，比如session。 当有服务器需要剔除，必须手动down掉。 least_conn\n把==请求转发给连接数较少的后端服务器==。轮询算法是把请求平均的转发给各个后端，使它们的负载\n大致相同；但是，有些请求占用的时间很长，会导致其所在的后端负载较高。这种情况下， least_conn这种方式就可以达到更好的负载均衡效\nupstream dynamicserver { least_conn; #把请求转发给连接数较少的后端服务器 server 192.168.64.1:9001 weight=2; #tomcat 1 server 192.168.64.1:9002; #tomcat 2 server 192.168.64.1:9003 backup; #tomcat 3 server 192.168.64.1:9004 max_fails=3 fail_timeout=20s; #tomcat 4 } ==重试策略==\nupstream dynamicserver { server 192.168.64.1:9001 fail_timeout=60s max_fails=3; #Server A server 192.168.64.1:9002 fail_timeout=60s max_fails=3; #Server B } 可以配置nginx 能够到其它的服务器进行重试, 并且可以配置指定的错误码\n官方说明\nSyntax: proxy_next_upstream error | timeout | invalid_header | http_500 | http_502 | http_503 | http_504 | http_403 | http_404 | off ...; Default: proxy_next_upstream error timeout; Context: http, server, location upstream dynamicserver { server 192.168.64.1:9001 fail_timeout=60s max_fails=3; #tomcat 1 server 192.168.64.1:9002 fail_timeout=60s max_fails=3; #tomcat 2 } server { server_name www.itcast.com; default_type text/html; charset utf-8; location ~ .*$ { index index.jsp index.html; proxy_pass http://dynamicserver; #下一节点重试的错误状态 proxy_next_upstream error timeout http_500 http_502 http_503 http_504; } 2. 跨域配置 server { listen 80; server_name test.cross.com; if ($host ~ (.*).cross.com) { set $domain $1; ##记录二级域名值 } # 是否允许请求带有验证信息 add_header Access-Control-Allow-Credentials true; # 允许跨域访问的域名,可以是一个域的列表，也可以是通配符* add_header Access-Control-Allow-Origin http://static.enjoy.com; # 允许脚本访问的返回头 add_header Access-Control-Allow-Headers 'x-requested-with,content-type,Cache-Control,Pragma,Date,x-timestamp'; # 允许使用的请求方法，以逗号隔开 add_header Access-Control-Allow-Methods 'POST,GET,OPTIONS,PUT,DELETE'; # 允许自定义的头部，以逗号隔开,大小写不敏感 add_header Access-Control-Expose-Headers 'WWW-Authenticate,Server-Authorization'; # P3P支持跨域cookie操作 add_header P3P 'policyref=\u0026quot;/w3c/p3p.xml\u0026quot;, CP=\u0026quot;NOI DSP PSAa OUR BUS IND ONL UNI COM NAV INT LOC\u0026quot;'; if ($request_method = 'OPTIONS') { ##OPTIONS类的请求，是跨域先验请求 return 204; ##204代表ok } } 1. nginx 安装 # 下载nginx wget http://nginx.org/download/nginx-1.21.1.tar.gz # 解压nginx tar -zxvf 进入auto目录\n​\tcc是用于编译的，对所有的操作系统的判断在os里面，其他所有文件都是为了辅助configure文件在 执行的时候去判定支持哪些模块，当前的操作系统有哪些特性可以供nginx使用 然后我们在看图1中，conf是配置文件的示例文件，方便我们在安装完以后可以直接把conf里面的 配置文件复制到安装目录下面, CHANGES这个文件里面描述了nginx的哪些特性,CHANGES.ru是一个俄罗 斯版本的描述，因为nginx的作者是一个俄罗斯人，configure是一个用来生成中间文件进行编译前的一 个必备动作 接下来我们通过 ./configure \u0026ndash;help | more 命令来查看一下\n2. nginx 常用命令 nginx -s reload #重新加载配置文件 nginx -t -c /路径/nginx.conf # 检查配置文件是否正确 nginx -v 3. openresty 1. yum 安装 sudo yum install yum-utils sudo yum-config-manager --add-repo https://openresty.org/package/centos/openresty.repo sudo yum install openresty 环境设置\nvi /etc/profile ##加入path路径 export PATH=$PATH:/usr/local/openresty/nginx/sbin source /etc/profile ##生效配置 查看\nnginx -v 2. 常用命令 3. 商品详情页\u0026ndash;架构 把商品详情页直接做成一个静态页面， 然后这样子每次全量的更新，把数据全部静态放 到 redis 里面， 每次数据变化的时候，我们就通过一个Java服务去渲染这个数据 然后把这个静态页面 推送到到文件服务器 缺点\n这种方案的缺点，如果商品很多，那么渲染的时间会很长，达不到实时的效果\n文件服务器性能高，tomcat性能差，压力都在Tomcat服务器了\n只能处理一些静态的东西，如果动态数据很多，比如有库存的，你不可能说每次去渲染，然后推送\n到文件服务器，那不是更加慢？\n通过openresty 改造的\n生成静态页 添加修改页面的时候生成静态页，这个地方生成的是一个通用的静态页，敏感数据比如 价格，商品\n名称等，通过占位符来替换，然后将生成的静态页的链接，以及敏感数据同步到redis中，如果只修改价\n格不需要重新生成静态页，只需要修改redis敏感数据即可。\n推送到文件服务器 这个的文件服务器泛指能够提供静态文件处理的文件服务器，nginx代理静态文件，tomcat，以及\nOSS等都算静态文件服务器，生成完静态文件后将文件推送到文件服务器，==并将请求连接存放进redis中==\n布隆过滤器过滤请求 Redis和nginx的速度很快，但是如果有人恶意请求不存在的请求会造成redis很大的开销，那么可以\n采用布隆过滤器将不存在的请求过滤出去。\nlua直连Redis读取数据 因为java连接Reids进行操作并发性能很弱，相对于OpenResty来说性能差距很大，这里使用\nOpenResty，读取Redis中存放的URL以及敏感数据。\nOpenResty渲染数据 从Redis获取到URL后lua脚本抓取模板页面内容，==然后通过redis里面的敏感数据进行渲染==然后返回\n前端，因为都是lua脚本操作性能会很高\n4. redis布隆过滤器 ==它用于高效地判断一个元素是否在一个集合中，特别适用于需要快速判断大量数据的场景==\n布隆过滤器的原理\n布隆过滤器其==本质就是一个只包含0和1的数组==。具体操作当一个元素被加入到集合里面后，该元素\n通过K个Hash函数运算得到K个hash后的值，然后将K个值映射到这个位数组对应的位置，把对应位置的\n值设置为1。查询是否存在时，我们就看对应的映射点位置如果全是1，他就很可能存在（跟hash函数的\n个数和hash函数的设计有关），如果有一个位置是0，那这个元素就一定不存在\n1. 包安装 创建目录\nmkdir -p /tmp/etc/redis/ mkdir -p /tmp/data/redis/node{1..6} redis 配置文件\ncat \u0026gt; /tmp/etc/redis/redis.conf\u0026lt;\u0026lt; EOF protected-mode no port 6379 tcp-backlog 511 timeout 0 tcp-keepalive 300 daemonize no supervised no pidfile /var/run/redis_6379.pid loglevel notice logfile \u0026quot;\u0026quot; databases 1 always-show-logo yes save 900 1 save 300 10 save 60 10000 stop-writes-on-bgsave-error yes rdbcompression yes rdbchecksum yes dbfilename dump.rdb rdb-del-sync-files no dir ./ replica-serve-stale-data yes replica-read-only yes repl-diskless-sync no repl-diskless-sync-delay 5 repl-diskless-load disabled repl-disable-tcp-nodelay no replica-priority 100 acllog-max-len 128 lazyfree-lazy-eviction no lazyfree-lazy-expire no lazyfree-lazy-server-del no replica-lazy-flush no lazyfree-lazy-user-del no oom-score-adj no oom-score-adj-values 0 200 800 appendonly yes appendfilename \u0026quot;appendonly.aof\u0026quot; appendfsync everysec no-appendfsync-on-rewrite no auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb aof-load-truncated yes aof-use-rdb-preamble yes lua-time-limit 5000 cluster-enabled yes cluster-config-file nodes-6379.conf cluster-node-timeout 15000 slowlog-log-slower-than 10000 slowlog-max-len 128 latency-monitor-threshold 0 notify-keyspace-events \u0026quot;\u0026quot; hash-max-ziplist-entries 512 hash-max-ziplist-value 64 list-max-ziplist-size -2 list-compress-depth 0 set-max-intset-entries 512 zset-max-ziplist-entries 128 zset-max-ziplist-value 64 hll-sparse-max-bytes 3000 stream-node-max-bytes 4096 stream-node-max-entries 100 activerehashing yes client-output-buffer-limit normal 0 0 0 client-output-buffer-limit replica 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 hz 10 dynamic-hz yes aof-rewrite-incremental-fsync yes rdb-save-incremental-fsync yes jemalloc-bg-thread yes loadmodule /etc/redis/redisbloom.so EOF docker-compose.yml\nversion: '2' services: redis01: image: redis hostname: redis01 container_name: redis01 networks: docker-network: ipv4_address: 172.31.0.2 ports: - \u0026quot;7001:6379\u0026quot; volumes: - \u0026quot;/tmp/etc/redis/redis.conf:/etc/redis/redis.conf\u0026quot; - \u0026quot;/tmp/data/redis/node1:/data\u0026quot; - \u0026quot;/tmp/etc/redis/redisbloom.so:/etc/redis/redisbloom.so\u0026quot; command: redis-server /etc/redis/redis.conf redis02: image: redis hostname: redis02 container_name: redis02 networks: docker-network: ipv4_address: 172.31.0.3 ports: - \u0026quot;7002:6379\u0026quot; volumes: - \u0026quot;/tmp/etc/redis/redis.conf:/etc/redis/redis.conf\u0026quot; - \u0026quot;/tmp/data/redis/node2:/data\u0026quot; - \u0026quot;/tmp/etc/redis/redisbloom.so:/etc/redis/redisbloom.so\u0026quot; command: redis-server /etc/redis/redis.conf redis03: image: redis hostname: redis03 container_name: redis03 networks: docker-network: ipv4_address: 172.31.0.4 ports: - \u0026quot;7003:6379\u0026quot; volumes: - \u0026quot;/tmp/etc/redis/redis.conf:/etc/redis/redis.conf\u0026quot; - \u0026quot;/tmp/data/redis/node3:/data\u0026quot; - \u0026quot;/tmp/etc/redis/redisbloom.so:/etc/redis/redisbloom.so\u0026quot; command: redis-server /etc/redis/redis.conf redis04: image: redis hostname: redis04 container_name: redis04 networks: docker-network: ipv4_address: 172.31.0.5 ports: - \u0026quot;7004:6379\u0026quot; volumes: - \u0026quot;/tmp/etc/redis/redis.conf:/etc/redis/redis.conf\u0026quot; - \u0026quot;/tmp/data/redis/node4:/data\u0026quot; - \u0026quot;/tmp/etc/redis/redisbloom.so:/etc/redis/redisbloom.so\u0026quot; command: redis-server /etc/redis/redis.conf redis05: image: redis hostname: redis05 container_name: redis05 networks: docker-network: ipv4_address: 172.31.0.6 ports: - \u0026quot;7005:6379\u0026quot; volumes: - \u0026quot;/tmp/etc/redis/redis.conf:/etc/redis/redis.conf\u0026quot; - \u0026quot;/tmp/data/redis/node5:/data\u0026quot; - \u0026quot;/tmp/etc/redis/redisbloom.so:/etc/redis/redisbloom.so\u0026quot; command: redis-server /etc/redis/redis.conf redis06: image: redis hostname: redis06 container_name: redis06 networks: docker-network: ipv4_address: 172.31.0.7 ports: - \u0026quot;7006:6379\u0026quot; volumes: - \u0026quot;/tmp/etc/redis/redis.conf:/etc/redis/redis.conf\u0026quot; - \u0026quot;/tmp/data/redis/node6:/data\u0026quot; - \u0026quot;/tmp/etc/redis/redisbloom.so:/etc/redis/redisbloom.so\u0026quot; command: redis-server /etc/redis/redis.conf networks: docker-network: ipam: config: - subnet: 172.31.0.0/16 gateway: 172.31.0.1 2. docker 安装 docker run -d -p 6379:6379 --name redis-redisbloom redislabs/rebloom:latest 3. 常用命令 ","date":"2024-04-27T15:39:00+08:00","permalink":"https://mikeLing-qx.github.io/p/nginx/","title":"Nginx"},{"content":"目录 * Tomcat功能需求分析\r* Tomcat套娃式架构设计（Connector层次架构、容器层次结构） * Tomcat源码构建 * Tomcat源码剖析-链式初始化过程 * Tomcat流程剖析-Servlet请求处理链路追踪\r1. Tomcat架构设计 Http服务器和具体业务类是解耦的, Http服务器把请求交给Servlet容器来处理, 容器通过Servlet 接口调用业务类\nTomcat两个非常重要的功能（身份)\nHttp服务器功能：Socket通信（TCP/IP）、解析Http报文\nServlet容器功能：有很多Servlet（自带系统级Servlet+自定义Servlet），Servlet处理具体的业务逻辑\n1.1 设计实现 1. 基础架构 核心功能\n处理 Socket 连接，负责网络字节流与 Request 和 Response 对象的转化。 加载和管理 Servlet，以及具体处理Request请求 所以Tomcat 设计了两个核心组件\n==连接器负责对外交流, 容器负责内部处理==\nTomcat中一个容器可能对接多个连接器，每一个连接器都对应某种协议某种IO模型,Tomcat将多个连接器和单个容器组成一个service组件，一个tomcat中可能存在多个Service组件\nConnector：将不同协议不同IO模型的请求转换为标准的标准的 ServletRequest 对象交给容器处理。\nContainer：Container本质上是一个Servlet容器,负责servelt的加载和管理，处理请求ServletRequest，并返回标准的 ServletResponse 对象给连接器。\n2. 连接器是如何设计的 2.1 铺垫：支持协议\u0026amp;IO模型 Tomcat 是支持多种I/O模型和应用层协议的\n==Tomcat 支持的 I/O 模型有==：\nNIO：非阻塞 I/O，采用 Java NIO 类库实现\nNIO.2：异步 I/O，采用 JDK 7 最新的 NIO.2 类库实现\nAPR：采用 Apache 可移植运行库实现，是 C/C++ 编写的本地库\n==Tomcat 支持的应用层协议有==：\nHTTP/1.1：这是大部分 Web 应用采用的访问协议 AJP：用于和 Web 服务器集成（如 Apache） HTTP/2：HTTP 2.0 大幅度的提升了 Web 性能 2.2 连接器结构分析 Tomcat 为了实现支持多种 I/O 模型和应用层协议，一个容器可能对接多个连接器，就==好比一个房间有多个门==。但是单独的连接器或者容器都不能对外提供服务，需要把它们==组装起来才能工作==，组装后这个整体叫作 Service 组件。这里请注意，Service 本身没有做什么重要的事情，只是在连接器和容器外面多包了一层，把它们组装在一起。==Tomcat 内可能有多个 Service==，这样的设计也是出于灵活性的考虑。通过在==Tomcat 中配置多个 Service，可以实现通过不同的端口号来访问同一台机器上部署的不同应用==\n3. 核心功能 ==连接器对 Servlet 容器屏蔽了协议及 I/O 模型等的区别==，无论是 HTTP 还是 AJP，在容器中获取到的都是一个标准的 ==ServletRequest 对象==。我们可以把连接器的功能需求进一步细化，比如：\n监听网络端口。\n接受网络连接请求。读取网络请求字节流。\n根据具体应用层协议（HTTP/AJP）解析字节流，生成统一的 Tomcat Request 对象。\n将 Tomcat Request 对象转成标准的 ServletRequest。\n调用 Servlet 容器，得到 ServletResponse。\n将 ServletResponse 转成 Tomcat Response 对象。\n将 Tomcat Response 转成网络字节流。\n将响应字节流写回给浏览器。\n1.2 连接器 Endpoint内部Acceptor组件用于监听Socket 连接请求，当发送客户端连接到服务端Acceptor组件负责与客户端建立连接创建Socket,==每当连接客户端发起请求，Endpoint会创建一个SocketProcessor对象SocketProcessor 用于处理接收到的 Socket 请求，它实现 Runnable 接口==，在 run 方法里调用协议处理组件 Processor 进行处理。为了提高处理能力，SocketProcessor 被提交到线程池来执行。而这个线程池叫作执行器（Executor)\nProcessor 接收来自 Endpoint 的 Socket，读取字节流解析成 Tomcat Request 和 Response 对象，接着会调用 Adapter 的 Service 方法。并通过 Adapter 将其提交到容器处理\n连接器调用 CoyoteAdapter 的 sevice 方法，传入的是 Tomcat Request 对象，CoyoteAdapter 负责将 Tomcat Request 转成 ServletRequest，再调用容器的 service 方法。\n1.3 容器 Container 本质上市一个Servlet 容器, 负责servlet的加载和管理, 处理请求Servlertrequest, 返回标准的ServletResponse d对象给连接器\n1.3.1 容器的工作流程 当客户请求某个资源时，HTTP 服务器会用一个 ServletRequest 对象把客户的请求信息封装起来，然后调用 Servlet 容器的 service 方法，==Servlet 容器拿到请求后，根据请求的 URL 和 Servlet 的映射关系，找到相应的 Servlet==，如果 Servlet ==还没有被加载==，就用==反射机制创建==这个 Servlet，并调用 Servlet 的\n==init 方法来完成初始化==，接着调用 Servlet 的 service 方法来处理请求，把 ServletResponse 对象返回给HTTP 服务器，HTTP 服务器会把响应发送给客户端\n1.3.2 容器层次结构 ​\tTomcat 设计了 4 种容器组件，分别是 ==Engine、Host、Context 和 Wrapper==。这 4 种容器不是平行关系，而是父子关系。\nWrapper:表示一个 Servlet\nContext:表示一个 Web 应用程序，一个 Web 应用程序中可能会有多个 Servlet\nHost:表示的是一个虚拟主机，或者说一个站点，可以给 Tomcat 配置多个虚拟主机地址，而一个虚拟主机下可以部署多个 Web 应用程序\nEngine:表示引擎，用来管理多个虚拟站点，一个 Service 最多只能有一个 Engine。\n","date":"2024-04-09T19:19:15+08:00","permalink":"https://mikeLing-qx.github.io/p/tomcat/","title":"Tomcat"},{"content":"入门\n图解http 图解tcp/ip 网络是怎样连接的 https://www.bilibili.com/video/BV1c4411d7jb/?p=1 资料:\nTCP 协议的 RFC文档: https://datatracker.ietf.org/doc/rfc1644/\n如何判断一个IP是内网还是外网\nIP地址分为3类：A类、B类和C类。这三类地址中的第一个字节，分别是1126、128191、192~223\n通常情况下，我们认为的内网IP地址都是从以下三个网段中获得：\n10.0.0.0 ~ 10.255.255.255 172.16.0.0 ~ 172.31.255.255 192.168.0.0 ~ 192.168.255.255\n1. 浏览器网址 当在浏览器输入一个网址后发生了什么?\nDNS解析获取IP\nHTTP的传输工作就交给了操作系统中的==协议栈==, 调用操作系统API, Socket库, 委托协议栈工作,\n协议栈上半有两部分 TCP 和UDP, 它们会接受应用层委托, ==执行收发数据的操作==\n下半 是用IP协议, 控制网络包收发操作, 互联网上传数据时，数据会被切分成一块块的\n网络包，而将网络包发送给对方的操作就是由 IP 负责的,==IP协议还包括ICMP 协议和 ARP 协议==\n​\t==ICMP==用于告知网络包传送过程中==产生的错误以及各种控制信息== ​ ==ARP== 用于根据 IP 地址查询相应的以太网 MAC 地址 IP下面的==网卡驱动程序==负责控制 网卡硬件, ==物理硬件网卡==负责实际的收发操作, 也就是对网线中的信号执行发送和接收操作\n封装IP报文, 包括: 源IP地址, 目标IP地址\n生成了IP头部之后, 接下来网络包还需要在==IP头部的前面加上 MAC头部==\n发送方, MAC 地址是在网卡生产时写入到 ROM 里的，只要将这个值读\n取出来写入到 MAC 头部就可以了\n接收方的MAC地址\n发包时：先查询 ARP 缓存，如果其中已经保存了对方的 MAC 地址，就不需要发送 ARP 查询，直接使用ARP 缓存中的地址。 而当 ARP 缓存中不存在对方 MAC 地址时，则发送 ARP 广播查询。 协议类型\n然后通过交换机, 路由器, 层层转发, 源IP和目标IP是不变的, 但是MAC地址会不断转化\n数据包抵达服务器之后, 服务器会查看数据包的 ==MAC 头部, 是否与自己的mac地址符合==, 匹配上就把包收起来, 接着继续查看 ==数据包的IP头, 发现目标地址是自己==, 然后查看==IP头中的协议项==, 知道是TCP协议, 打开TCP的头部, 里面有==序列号==, 如果是需要的就放入缓存中, 然后就返回一个ACK, 不是就丢弃, TCP头部里面还有==端口号==, HTTP的服务器正在监听这个端口号, 于是服务器就把 报发给HTTP 进程, 服务器的HTTP 进程看到这个请求要访问一个页面, 于是就把这个网页封装到 HTTP响应报文里面;\n2. TCP 0. 概述 TCP 是一个面向连接的, 可靠的 基于字节流的传输层协议, 它能确保==接收端==的网络包是==无损坏, 无间隔, 非冗余和按序的;\n建立一个TCP连接需要客户端和服务端达成三个消息的共识\nSocket: 有IP地址和端口号组成 序列号: 用来解决乱序问题 窗口大小: 用来做流量控制 1. TCP 和 UDP的区别 1. ==连接==\nTCP 是面向连接的传输层协议，传输数据前先要建立连接。 UDP 是不需要连接，即刻传输数据。 2. ==服务对象==\nTCP 是一对一的两点服务，即一条连接只有两个端点。 UDP 支持一对一、一对多、多对多的交互通信 3. ==可靠性==\nTCP 是可靠交付数据的，数据可以无差错、不丢失、不重复、按需到达。 UDP 是尽最大努力交付，不保证可靠交付数据。 4. ==拥塞控制、流量控制==\nTCP 有拥塞控制和流量控制机制，保证数据传输的安全性。 UDP 则没有，即使网络非常拥堵了，也不会影响 UDP 的发送速率。 5. ==首部开销==\nTCP 首部长度较长，会有一定的开销，首部在没有使用「选项」字段时是 20 个字节，如果使\n用了「选项」字段则会变长的。\nUDP 首部只有 8 个字节，并且是固定不变的，开销较小。\n6. ==传输方式==\nTCP 是流式传输，没有边界，但保证顺序和可靠。\nUDP 是一个包一个包的发送，是有边界的，但可能会丢包和乱序。\n7. ==分片不同==\nTCP 的数据大小如果大于 MSS 大小，则会在传输层进行分片，目标主机收到后，也同样在传输\n层组装 TCP 数据包，如果中途丢失了一个分片，只需要传输丢失的这个分片。\nUDP 的数据大小如果大于 MTU 大小，则会在 IP 层进行分片，目标主机收到后，在 IP 层组装完\n数据，接着再传给传输层，但是如果中途丢了一个分片，则就需要重传所有的数据包，这样传输\n效率非常差，所以通常 UDP 的报文应该小于 MTU。\n==应用场景的区分==\n由于 TCP 是面向连接，能保证数据的可靠性交付，因此经常用于：\nFTP 文件传输 HTTP / HTTPS 由于 UDP 面向无连接，它可以随时发送数据，再加上UDP本身的处理既简单又高效，因此经常用于：\n包总量较少的通信，如 DNS 、 SNMP 等 视频、音频等多媒体通信 广播通信 2. 三次握手 客户端会随机初始化序号（ client_isn ），将此序号置于 TCP 首部的「序号」字段中，同时把SYN 标志位置为 1 ，表示 SYN 报文。接着第一个 SYN 报文发送给服务端，表示向服务端发起连接，该报文不包含应用层数据，之后客户端处于 SYN-SENT 状态。 服务端收到客户端的 SYN 报文后，首先服务端也随机初始化自己的序号（ server_isn ），将此序号填入 TCP 首部的「序号」字段中，其次把 TCP 首部的「确认应答号」字段填入 client_isn +1 , 接着把 SYN 和 ACK 标志位置为 1 。最后把该报文发给客户端，该报文也不包含应用层数据，之后服务端处于 SYN-RCVD 状态 客户端收到服务端报文后，还要向服务端回应最后一个应答报文，首先该应答报文 TCP 首部ACK 标志位置为 1 ，其次「确认应答号」字段填入 server_isn + 1 ，最后把报文发送给服务端，这次报文可以携带客户到服务器的数据，之后客户端处于 ESTABLISHED 状态。\n服务器收到客户端的应答报文后，也进入 ESTABLISHED 状态。\nTCP 三次握手建立连接的过程, ==目的是保证双方都有发送和接收的能力==\n一开始，客户端和服务端都处于 CLOSED 状态。先是服务端主动监听某个端口，处于 LISTEN状态。 然后客户端主动发起连接 SYN ，之后处于 SYN-SENT 状态。 服务端收到发起的连接，返回 SYN ，并且 ACK 客户端的 SYN ，之后处于 SYN-RCVD 状态。 客户端收到服务端发送的 SYN 和 ACK 之后，发送 ACK 的 ACK ，之后处于ESTABLISHED 状态，因为它一发一收成功了。 服务端收到 ACK 的 ACK 之后，处于 ESTABLISHED 状态，因为它也一发一收了。 查看TCP的连接状态\nnetstat -napt 最主要的原因是 三次握手才可以 ==初始化Socket, 序列号, 和窗口大小, 并建立TCP连接==\n三次握手才可以==阻止重复历史连接的初始化==（主要原因） 三次握手才可以==同步双方的初始序列号== 三次握手才可以==避免资源浪费== ==原因一==: ==阻止重复历史连接的初始化==\n客户端连续发送多次 SYN 建立连接的报文，在网络拥堵情况下：\n一个「旧 SYN 报文」比「最新的 SYN 」 报文早到达了服务端； 那么此时服务端就会回一个 SYN + ACK 报文给客户端； 客户端收到后可以根据自身的上下文，判断这是一个历史连接（序列号过期或超时），那么客户 端就会发送 RST 报文给服务端，表示中止这一次连接。 如果是两次握手连接，就不能判断当前连接是否是历史连接，三次握手则可以在客户端（发送方）准备\n发送第三次报文时，==客户端因有足够的上下文==来判断当前连接是否是历史连接：\n如果是历史连接（序列号过期或超时），则第三次握手发送的报文是 RST 报文，==以此中止历史连接==； 如果不是历史连接，则==第三次发送的报文是 ACK 报文，通信双方就会成功建立连接==； 所以，TCP 使用三次握手建立连接的最主要原因是==防止历史连接初始化了连接。==\n==原因二:== 同步双方初始序列号\nTCP 协议的通信双方， ==都必须维护一个「序列号」==， 序列号是可靠传输的一个关键因素，\n它的作用：\n==接收方可以去除重复的数据==；\n接收方可以根据数据包的序列号按序接收；\n可以标识发送出去的数据包中， 哪些是已经被对方收到的；\n​\t可见，序列号在 TCP 连接中占据着非常重要的作用，所以当客户端发送携带「初始序列号」的 SYN\n报文的时候，需要服务端回一个 ACK 应答报文，表示客户端的 SYN 报文已被服务端成功接收，那当\n服务端发送==「初始序列号」==给客户端的时候，依然也要得到客户端的应答回应，==这样一来一回==，才能确\n保双方的==初始序列号能被可靠的同步==\n==原因三: 避免资源的浪费==\n3. 总结 TCP 建立连接时，通过三次握手能防止历史连接的建立，能减少双方不必要的资源开销，能帮助双方同\n步初始化序列号。序列号能够保证数据包不重复、不丢弃和按序传输。\n不使用「两次握手」和「四次握手」的原因：\n「两次握手」：无法防止历史连接的建立，会造成双方资源的浪费，也无法可靠的同步双方序列号；\n「四次握手」：三次握手就已经理论上最少可靠连接建立，所以不需要使用更多的通信次数。\n4. 四次挥手 双方都可以主动断开连接, 断开连接后主机中的\u0026rsquo;资源\u0026rsquo; 将会被释放\n客户端打算关闭连接，此时会发送一个 TCP 首部 FIN 标志位被置为 1 的报文，也即 FIN\n报文，之后客户端进入 FIN_WAIT_1 状态。\n服务端收到该报文后，就向客户端发送 ACK 应答报文，接着服务端进入 CLOSED_WAIT 状\n态。\n客户端收到服务端的 ACK 应答报文后，之后进入 FIN_WAIT_2 状态。\n等待服务端处理完数据后，也向客户端发送 FIN 报文，之后服务端进入 LAST_ACK 状态。\n客户端收到服务端的 FIN 报文后，回一个 ACK 应答报文，之后进入 TIME_WAIT 状态\n服务器收到了 ACK 应答报文后，就进入了 CLOSED 状态，至此服务端已经完成连接的关闭。\n客户端在经过 2MSL 一段时间后，自动进入 CLOSED 状态，至此客户端也完成连接的关闭。\n==每个方向都需要一个 FIN 和一个 ACK，因此通常被称为四次挥手, 主动关闭连接的，才有 TIME_WAIT 状态。==\n1. ==为什么是四次挥手== 关闭连接时，客户端向服务端发送 FIN 时，仅仅表示==客户端不再发送数据了但是还能接收数据==。\n服务器收到客户端的 FIN 报文时，==先回一个 ACK 应答报文==，而==服务端可能还有数据需要处理和发送，等服务端不再发送数据时，才发送 FIN 报文给客户端来表示同意现在关闭连接==。\n从上面过程可知，服务端通常需要等待完成数据的发送和处理，所以服务端的 ACK 和 FIN 一般都会分开发送，从而比三次握手导致多了一次\n2. 为什么TIME_WAIT 等待的时间是2MSL ​\tMSL 是 Maximum Segment Lifetime，报文最大生存时间，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。因为 TCP 报文基于是 ==IP 协议的==，而 ==IP 头中有一个 TTL 字段，是 IP 数据报可以经过的最大路由数==，==每经过一个处理他的路由器此值就减 1，当此值为 0 则数据报将被丢弃，同时发送 ICMP 报文通知源主机==。\n​\tMSL 与 TTL 的区别： MSL 的单位是时间，而 TTL 是经过路由跳数。所以 MSL 应该要大于等于 TTL****消耗为 0 的时间，以确保报文已被自然消亡。\n​\tTIME_WAIT 等待 2 倍的 MSL，比较合理的解释是： ==网络中可能存在来自发送方的数据包，当这些发送方的数据包被接收方处理后又会向对方发送响应，所以一来一回需要等待 2 倍的时间==。\n​ 比如如果==被动关闭方没有收到断开连接的最后的 ACK 报文==，就会触发超时==重发 Fin 报文==，另一方接收到 ==FIN 后，会重发 ACK 给被动关闭方， 一来一去正好 2 个 MSL==。\n​\t2MSL 的时间是从客户端接收到 FIN 后发送 ACK 开始计时的。如果在 TIME-WAIT 时间内，因为客户端的 ACK 没有传输到服务端，客户端又接收到了服务端重发的 FIN 报文，那么 2MSL 时间将重新计****时。\n在 Linux 系统里 ==2MSL 默认是 60 秒==，那么一个 MSL 也就是 30 秒。Linux 系统停留在****TIME_WAIT 的时间为固定的 60 秒。\n其定义在 Linux 内核代码里的名称为 TCP_TIMEWAIT_LEN： \\#define TCP_TIMEWAIT_LEN (60*HZ) /* how long to wait to destroy TIME-WAIT state, about 60 seconds */ 如果要修改 TIME_WAIT 的时间长度，只能修改 Linux 内核代码里 TCP_TIMEWAIT_LEN 的值，并重 新编译 Linux 内核 3. 如果已建立连接, 客户端故障了怎么办? ​\tTCP 有一个机制是保活机制。这个机制的原理是这样的：\n​\t==定义一个时间段，在这个时间段内，如果没有任何连接相关的活动，TCP 保活机制会开始作用，每隔一个时间间隔，发送一个探测报文，该探测报文包含的数据非常少，如果连续几个探测报文都没有得到响应，则认为当前的 TCP 连接已经死亡，系统内核将错误信息通知给上层应用程序==\n​\t在 Linux 内核可以有对应的参数可以设置保活时间、保活探测的次数、保活探测的时间间隔，以下都为\n默认值：\nnet.ipv4.tcp_keepalive_time=7200 net.ipv4.tcp_keepalive_intvl=75 net.ipv4.tcp_keepalive_probes=9 tcp_keepalive_time=7200：表示保活时间是 7200 秒（2小时），也就 2 小时内如果没有任何连接相关的活动，则会启动保活机制 tcp_keepalive_intvl=75：表示每次检测间隔 75 秒； tcp_keepalive_probes=9：表示检测 9 次无响应，认为对方是不可达的，从而中断本次的连接。 5. 可靠性保证 0. ACK ​\t在TCP连接建立成功之后，发送方发送数据包到接收方。当接收方接收到数据包后，会向发送方发送一个ACK确认消息，表示已经接收到了数据包;\n​\tTCP使用累积确认机制，==即ACK消息不仅确认了接收到的最新数据包，还会确认之前已经接收到的所有数据包==\n​\tACK消息中的确认序号指的是==期望收到的下一个数据包的序号==。当收到的数据包序号为n时，确认序号应该为n+1\n1. 重传机制 常见的重传机制\n超时重传\n快速重传\nSACK\nD-SACk\n1. 超时重传 发送数据时, 设定一个==定时器==, 当超过指定的时间后, 没有收到对方的 ==ACK==应答报文,就会重发该数据 出现的情况 数据包丢失, (数据包没有到达接收方) 没有收到ack应答 (接收方ack应答包没有被发送方接收到) 超时重传时间应该设置为多少 RTT(Round-trip time 往返时延) ==数据从网络一端到达另一端所需要的时间== RTO (超时重传时间), 较为合理的是==超时重传时间要略大于往返时间延迟RTT==, 实际上「报文往返 RTT 的值」是经常变化的，因为我们的网络也是时常变化的。也就因为「报文往返RTT 的值」 是经常波动变化的，所以「超时重传时间 RTO 的值」应该是一个动态变化的值。 如果超时重发的数据，再次超时的时候，又需要重传的时候，==TCP 的策略是**超时间隔加倍==。也就是每当遇到一次超时重传的时候，都会将下一次超时时间间隔设为先前值的两倍。==两次超时，就说明网络环境差，不宜频繁反复发送==。 超时触发重传==存在的问题是，超时周期可能相对较长==。那是不是可以有==更快的方式==呢？于是就可以用「==快速重传==」机制来解决超时重发的时间等待。 2. 快速重传 ==为什么会是相同的ack 确认序号?==\n​ ==不以时间为驱动, 而是以数据驱动重传==\n​\t快速重传的工作方式是==当收到三个相同的 ACK 报文时，会在定时器过期之前，重传丢失的报文段==。\n​\t快速重传机制只解决了一个问题，就是超时时间的问题，但是它依然面临着另外一个问题。就是==重传的时候，是重传之前的一个，还是重传所有的问题。==\n3. SACK ==选择性确认==, 如果要支持 SACK ，必须双方都要支持,\n需要在TCP 头部[选项] 字段里面加一个 SACK, ==TCP SACK 将一个确认消息拆分成多个端, 每个段表示一个已经成功接收的数据包范围==, 使发送方 知道\n​\tTCP SACK可以改进TCP协议在网络中的性能和可靠性，特别是在高丢包率的网络环境下，能够减少不必要的数据重传和网络带宽的浪费。然而，由于TCP SACK需要在接收方和发送方之间进行协商和交互，所以它会带来一些额外的开销和复杂性。\n例如，如果接收方成功接收了数据包1、2和4，则可以向发送方发送一个SACK确认消息，其中包含两个段：[1,3]和[4,4]，表示数据包1、2和4已经成功接收，而数据包3还没有接收到。\n4. Duplicate SACK ==要使用了 SACK 来告诉「发送方」有哪些数据被重复接收了。==\n​\t当接收方发送重复的SACK段时，会在TCP头部中添加D-SACK选项字段，标识出哪些SACK段是重复的，告诉发送方这些SACK段已经被接收方重复发送。发送方在收到这些D-SACK选项字段后，就可以根据这些信息来调整重传决策，避免不必要的数据重传和网络带宽的浪费。\n2. 滑动窗口 我们知道==TCP 每发送一个数据,, 都要进行一次确认应答, 当上一个数据包收到了应答, 再发送下一个数据包==, 就好比如, 我和你面对面聊天，你一句我一句。但这种方式的缺点是效率比较低的。\n==窗口大小就是, 无需等待确认应答, 而可以继续发送数据的最大值==, 窗口的实现实际是操作系统开辟的一个缓存空间, 如果按期收到确认应答, 那数据就可以从缓存区清除;\nTCP头里面有一个字段叫Window, 也就是窗口大小, 通常是接收方决定的\n==这个字段是接收端告诉发送端自己还有多少缓冲区可以接收数据。于是发送端就可以根据这个接收端的处理能力来发送数据，而不会导致接收端处理不过来==\n假设窗口大小为 3 个 TCP 段，那么发送方就可以「连续发送」 3 个 TCP 段，并且中途若有 ACK丢失，可以通过「下一个确认应答进行确认」。如下图：\n3. 流量控制 ==TCP提供一种机制可以让「发送方」根据「接收方」的实际接收能力控制发送的数据量，这就是所谓的流量控制==\n1. 滑动窗口与操作系统缓存 操作系统缓冲区与滑动窗口的关系;\n假定了发送窗口和接收窗口是不变的，但是实际上，发送窗口和接收窗口中所存放的字节数，都是放在操作系统内存缓冲区中的，而操作系统的缓冲区，==会被操作系统调整==\n当应用进程没办法及时读取缓冲区的内容时，也会对我们的缓冲区造成影响。\n为了防止这种情况发生，==TCP规定是不允许同时减少缓存又收缩窗口的，而是采用先收缩窗口，过段时间再减少缓存==，这样就可以避免了丢包情况。\n2. 窗口关闭 TCP 通过接收方指定 希望从发送方接收的数据大小(窗口大小), ==如果窗口大小为 0 时，就会阻止发送方给接收方传递数据，直到窗口变为非 0 为止，这就是窗口关闭。==\n​\tTCP 为每个连接设有一个持续定时器，==只要TCP连接一方收到对方的零窗口通知，就启动持续计时器==。\n如果==持续计时器超时==, 就会发送==窗口探测 ( Window probe ) 报文==, 而对方在确认这个探测报文时，给出自己现在的接收窗口大小。\n3. 糊涂窗口综合症 如果接收方腾出几个字节并告诉发送方现在有几个字节的窗口，而发送方会义无反顾地发送这几个字节，这就是糊涂窗口综合症。\n让接收方不通告小窗口 怎么让发送方避免发送小数据呢？ 4. 拥塞控制 ==在网络出现拥堵时，如果继续发送大量数据包，可能会导致数据包时延、丢失等，这时TCP就会重传数据，但是一重传就会导致网络的负担更重，于是会导致更大的延迟以及更多的丢包，这个情况就会进入恶性循环被不断地放大==\n3. 抓包 1. tcpdump # -i eth1 表示抓取eth1网口的数据包 # icmp 表示抓取icmp协议的数据包 # host 表示主机过滤, 抓取对应ip的数据包 # -nn 表示不解析ip地址和端口号的名称 tcpdump -i eth1 icmp and host 10.168.1.60 -nn #抓取到icmp数据包的输出格式: #时间戳 协议 源地址.源端口 \u0026gt; 目标地址.目标端口 网络包详细信息 2. wireshark 右键选择追踪流, 作为过滤器应用 https://blog.csdn.net/amber_o0k/article/details/80380602 参考资料\n快捷键\nCtrl+Alt+Shift+H\tHTTP 流\tHTTP 流 Ctrl+Alt+Shift+S\tSSL 流\tSSL 流 Ctrl+Alt+Shift+T\tTCP 流\tTCP 流 Ctrl+Alt+Shift+U\tUDP 流\tUDP 流 ctrl + s\t保存 ctrl + e 停止/开始 4. DHCP 电脑通常都是通过 DHCP 动态获取 IP 地址，大大省去了配IP 信息繁琐的过程。\n四个步骤\n客户端首先发起DHC发现报文（DHCP DISCOVER） 的 IP 数据报，由于客户端没有 IP 地址，也不知道 DHCP 服务器的地址，所以使用的是 UDP 广播通信，其使用的广播目的地址是255.255.255.255（端口 67） 并且使用 0.0.0.0（端口 68） 作为源 IP 地址。DHCP 客户端将该IP 数据报传递给链路层，链路层然后将帧广播到所有的网络中设备。\nDHCP 服务器收到 DHCP 发现报文时，用DHCP提供报文（DHCP OFFER） 向客户端做出响应。该报文仍然使用 IP 广播地址 255.255.255.255，该报文信息携带服务器提供可租约的 IP 地址、子网掩码、默认网关、DNS 服务器以及 IP地址租用期。\n客户端收到一个或多个服务器的 DHCP 提供报文后，从中选择一个服务器，并向选中的服务器发送 DHCP请求报文（DHCP REQUEST进行响应，回显配置的参数。\n最后，服务端用 DHCP ACK报文对 DHCP 请求报文进行响应，应答所要求的参数。\n​\t一旦==客户端收到 DHCP ACK 后，交互便完成了==，并且客户端能够在租用期内使用 DHCP 服务器分配的IP 地址。\n​\t如果租约的 DHCP IP 地址快期后，客户端会向服务器发送 DHCP 请求报文：\n服务器如果同意继续租用，则用 DHCP ACK 报文进行应答，客户端就会延长租期。\n​\t服务器如果不同意继续租用，则用 DHCP NACK 报文，客户端就要停止使用租约的 IP 地址。\n可以发现，DHCP 交互中，==全程都是使用 UDP 广播通信==。\n问题: DHCP 服务器和客户端==不是在同一个局域网内==，==路由器又不会转发广播包==，那不是每个网络都要配一个 DHCP 服务器？\n==DHCP 中继代理==。有了 DHCP 中继代理以后，对不同网段的 IP 地\n址分配也可以由一个 DHCP 服务器统一进行管理。\nDHCP 服务器即使不在同一个链路上也可以实现统一分配和管理IP地址。\nDHCP 客户端会向 DHCP 中继代理发送 DHCP 请求包，而 DHCP 中继代理在收到这个广播包以后，再以单播的形式发给 DHCP 服务器。\n服务器端收到该包以后再向 DHCP 中继代理返回应答，并由 DHCP 中继代理将此包广播给DHCP 客户端 。\n5. ICMP Internet Control Message Protocol，也就是互联网控制报文协议\n==ICMP 主要的功能包括：确认 IP 包是否成功送达目标地址、报告发送过程中 IP 包被废弃的原因和改善网络设置等。==\n6. NAT 网络地址与端口转换 NAPT\n​\t两个私有 IP 地址都转换 IP 地址为公有地址 120.229.175.121，但是以不同的端口号作为区分。\n存在的问题\n外部无法主动与 NAT 内部服务器建立连接，因为 NAPT 转换表没有转换记录。\n转换表的生成与转换操作都会产生性能开销。\n通信过程中，如果 NAT 路由器重启了，所有的 TCP 连接都将被重置。\n解决方法\n改用IPV6 , 可用范围非常大，以至于每台设备都可以配置一个公有 IP 地址\nNAT 穿透\n​\tNAT 穿越技术拥有这样的功能，它能够让网络应用程序主动发现自己位于 NAT 设备之后，并且会主动获得 NAT 设备的公有 IP，并为自己建立端口映射条目，注意这些都是 NAT设备后的应用程序自动完成的。\n​\t也就是说，在 NAT 穿透技术中，NAT设备后的应用程序处于主动地位，它已经明确地知道 NAT 设备要修改它外发的数据包，于是它主动配合 NAT 设备的操作，主动地建立好映射，这样就不像以前由 NAT设备来建立映射了。\n​\t==说人话，就是客户端主动从 NAT 设备获取公有 IP 地址，然后自己建立端口映射条目，然后用这个条目对外通信，就不需要 NAT 设备来进行转换==了。\n7. ARP 在传输一个 IP 数据报的时候，==确定了源 IP 地址和目标 IP 地址后，就会通过主机「路由表」确定 IP 数据包下一跳==。然而，网络层的下一层是数据链路层，所以我们还要知道==「下一跳」的 MAC 地址==。\n由于主机的路由表中可以找到下一跳的 IP 地址，所以可以通过 ARP 协议，求得下一跳的 MAC 地址。\nARP 是通过 ==ARP 请求和 ARP 响应==两种类型的包来确定MAC 地址的;\n主机会通过广播发送 ARP 请求，这个包中包含了想要知道的 MAC 地址的主机 IP 地址。\n当同个链路中的所有设备收到 ARP 请求时，会去拆开 ARP 请求包里的内容，如果 ARP 请求包中的目标 IP 地址与自己的 IP 地址一致，那么这个设备就将自己的 MAC 地址塞入 ARP 响应包返回给主机\n​\t操作系统通常会把第一次通过 ARP 获取的 MAC 地址缓存起来，以便下次直接从缓存中找到对应 IP 地址的 MAC 地址。\n==RARP==\nARP 协议是已知 IP 地址求 MAC 地址，那 RARP 协议正好相反，它是==已知 MAC 地址求 IP 地址==。例如将打印机服务器等==小型嵌入式设备接入到网络时就经常会用得到。==\n​\t==这需要架设一台 RARP 服务器==，在这个服务器上注册设备的 MAC 地址及其 IP 地址。然后再将这个设备接入到网络，接着：\n​\t该设备会发送一条「我的 MAC 地址是XXXX，请告诉我，我的IP地址应该是什么」的请求信息。\n​\tRARP 服务器接到这个消息后返回「MAC地址为 XXXX 的设备，IP地址为 XXXX」的信息给这个设备。\n最后，设备就根据从 RARP 服务器所收到的应答信息设置自己的 IP 地址。\n8. http 与 https 1. http 与https 的区别 HTTP 是超文本传输协议，信息是明文传输，存在安全风险的问题。HTTPS 则解决 HTTP 不安全的缺陷，在 TCP 和 HTTP 网络层之间加入了 SSL/TLS 安全协议，使得报文能够==加密传输==。\nHTTP 连接建立相对简单， TCP 三次握手之后便可进行 HTTP 的报文传输。而 ==HTTPS 在 TCP三次握手之后，还需进行 SSL/TLS 的握手过程==，才可进入加密报文传输。\n==HTTP 的端口号是 80，HTTPS 的端口号是 443==。\nHTTPS 协议需要向 CA（证书权威机构）==申请数字证书==，==来保证服务器的身份是可信的==。\n解决了http的哪些问题\n窃听风险，比如通信链路上可以获取通信内容，用户号容易没。 篡改风险，比如强制植入垃圾广告，视觉污染，用户眼容易瞎。 冒充风险，比如冒充淘宝网站，用户钱容易没。 通过SSL/TSL协议\n信息加密：交互信息无法被窃取，但你的号会因为「自身忘记」账号而没。\n校验机制：无法篡改通信内容，篡改了就不能正常显示，但百度「竞价排名」依然可以搜索垃圾广告。 ==通过摘要算法==\n身份证书：证明淘宝是真的淘宝网，但你的钱还是会因为「剁手」而没, ==通过数字证书== , ==客户端先向服务器端索要公钥，然后用公钥加密信息，服务器收到密文后，用自己的私钥解密。==\nHTTPS 采用的是对称加密和非对称加密结合的「混合加密」方式：\n在通信建立前采用非对称加密的方式交换「会话秘钥」，后续就不再使用非对称加密。\n在通信过程中全部使用对称加密的「会话秘钥」的方式加密明文数据。\n采用「混合加密」的方式的原因：\n对称加密只使用一个密钥，运算速度快，密钥必须保密，无法做到安全的密钥交换。\n非对称加密使用两个密钥：公钥和私钥，公钥可以任意分发而私钥保密，解决了密钥交换问题但速度慢。\n==为了保证公钥不被篡改和信任度==\n所以这里就需要借助第三方权威机构 CA （数字证书认证机构），将服务器公钥放在数字证书（由数字证书认证机构颁发）中，只要证书是可信的，公钥就是可信的\n==SSL/TLS 协议基本流程：==\n客户端向服务器索要并验证服务器的公钥。\n双方协商生产「会话秘钥」。\n双方采用「会话秘钥」进行加密通信\n==会有四次通信==\n","date":"2024-03-25T16:10:33+08:00","permalink":"https://mikeLing-qx.github.io/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/","title":"计算机网络"},{"content":"1. 核心组件 1. Bean 组件 Spring的org.springframework.beans 包下的所有类主要解决了三件事：\nBean的定义 ==BeanDefinition==\nBean的创建 ==反射创建bean对象 . user{id=\u0026quot;\u0026quot;, name=\u0026quot;\u0026quot;, 还未赋值}==\nBean的解析 赋值和初始化\n1.1 BeanDefinition public class DefaultListableBeanFactory {\r// 我们的bean 信息, 就被封装成了beanDefinition, 保存到这个map 中\rprivate final Map\u0026lt;String, BeanDefinition\u0026gt; beanDefinitionMap;\r}\r2. Context 组件 Context在Spring的org.springframework.context包下\nContext模块构建于Core和Beans模块基础之上\nApplicationContext是Context的顶级父类\n上面的是懒加载的, 不会创建bean\n下面的 会创建所有非懒加载的bean\nBeanFactory 和 ApplicationContext 的区别 ApplicationContext 继承自 BeanFactory\n==Beanfactory:== ==容器对象==, 提供了对bean的==基本操作==, 读和写, 创建==管理bean的生命周期==, 工厂模式 ==ApplicationContext:== ==具备BeanFactory 的功能, 还有一些拓展==; 国际化操作, 后置处理器的创建\n2.1 后置处理器 后置处理器是一种拓展机制, 贯穿Spring bean的生命周期\n分为两种\n1. BeanFactory 后置处理器: BeanFactoryPostProcessor 实现改接口, 可以再spring 的==bean 创建之前==, ==修改bean的定义属性, 修改BeanDefinition==\npublic interface BeanFactoryPostProcessor { /** 该接口只有一个方法postProcessBeanFactory，\r方法参数是 ConfigurableListableBeanFactory，\r通过该 参数，可以获取BeanDefinition */\rvoid postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException; }\r场景\nspring 配置文件\ndriverClassName = \u0026ldquo;${jdbc.Driver}\u0026rdquo; 在 BeanDefinition 中 是原样读到的数据, 需要 把 它改成配置文件中的数据, 就可以通过 后置处理器, BeanPostProcessor 去完成\n2. Bean 后置处理器: BeanPostProcessor BeanPostProcessor是Spring IOC容器给我们提供的一个扩展接口\n实现该接口，可以在spring容器==实例化bean之后== (此时是没有设置值的, 或者是默认值)，在执行==bean的初始化方法前后，添加一些处理逻辑==\npublic interface BeanPostProcessor { //bean初始化方法调用前被调用 Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException; //bean初始化方法调用后被调用 Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException; }\r==spring aop 代理对象就是 在after 方法进行修改==\n可以实现Ordered, PriorityOrdered 接口进行执行排序\n如果没有实现会按照在 xml 整配置的顺序执行\n2. IOC 流程 IOC容器\u0026ndash;\u0026gt; DefaultListableBeanFactory map集合(单例池)\nFactoryBean 创建复杂的bean对象的时候使用, getObejct 方法\n循环依赖\r\u0026lt;bean id=\u0026quot;a\u0026quot; class=\u0026quot;a\u0026quot;\u0026gt; init-method = \u0026quot;init\u0026quot; 初始化方法\r\u0026lt;property name = \u0026quot;b\u0026quot; ref=\u0026quot;b\u0026quot;\u0026gt;\r\u0026lt;/bean\u0026gt;\r\u0026lt;bean id=\u0026quot;b\u0026quot; class=\u0026quot;b\u0026quot;\u0026gt;\r\u0026lt;property name = \u0026quot;a\u0026quot; ref =\u0026quot;a\u0026quot;\u0026gt;\r\u0026lt;/bean\u0026gt;\r==流程总结==\n创建BeanFactory ==容器对象== IOC容器\n创建BeanDefinitionReader, 加载解析Bean定义信息, 封装BeanDefinition\n执行BeanfactoryPostProcessor\n通过反射实例化对象\n==spring事件驱动准备工作==: 创建 BeanPostProcessor, 创建多播器, 监听器\n初始化操作\n完整对象, 添加到容器中\n3. Bean的生命周期 Bean 生命周期的整个执行过程描述如下。 1）根据配置情况调用 Bean 构造方法或工厂方法实例化 Bean。\n2）利用依赖注入完成 Bean 中所有属性值的配置注入。\n3）如果 Bean 实现了 BeanNameAware 接口，则 Spring 调用 Bean 的 setBeanName() 方法传入当前Bean 的 id 值。\n4）如果 Bean 实现了 BeanFactoryAware 接口，则 Spring 调用 setBeanFactory() 方法传入当前工厂实例的引用。\n5）如果 Bean 实现了 ApplicationContextAware 接口，则 Spring 调用setApplicationContext() 方法传入当前 ApplicationContext 实例的引用。\n6）如果 BeanPostProcessor 和 Bean 关联，则 Spring 将调用该接口的预初始化方法 postProcessBeforeInitialzation() 对 Bean 进行加工操作\n7）如果 Bean 实现了 InitializingBean 接口，则 Spring 将调用 afterPropertiesSet() 方法。\n8）如果在配置文件中通过 init-method 属性==指定了初始化方法，则调用该初始化方法==。\n9）如果 BeanPostProcessor 和 Bean 关联，则 Spring 将调用该接口的初始化方法 postProcessAfterInitialization()。此时，Bean 已经可以被应用系统使用了。\n10）如果在 中指定了该 Bean 的作用范围为 scope=\u0026ldquo;singleton\u0026rdquo;，则将该 Bean 放入 Spring IoC 的缓存池中，将触发 Spring 对该 Bean 的生命周期管理；如果在 中指定了该 Bean 的作用范围为scope=\u0026ldquo;prototype\u0026rdquo;，则将该 Bean 交给调用者，调用者管理该 Bean 的生命周期，Spring 不再管理该Bean。\n11）如果 Bean 实现了 DisposableBean 接口，则 Spring 会调用 destory() 方法将 Spring 中的 Bean 销毁；如果在配置文件中通过 destory-method 属性指定了 Bean 的销毁方法，则 Spring 将调用该方法。\n4. 源码 1. 如何创建的BeanFactory? (真实类型) obtainFreshBeanFactory\n如果存在旧的BeanFactory 就销毁 创建新的BeanFactory (DefauListableBeanFactory) 解析xml/ 加载Bean定义, 注册Bean定义到beanFactory (不初始化) 返回新的Beanfactory 2. 如何解析的配置文件, 封装BeanDefinition 解析bean 标签 再解析子标签 最后会注册到 BeanFactory 的 BeanDefinitionMap 中 3. 实例化过程 DefaultListableBeanFactory 中的\ngetBean方法 \u0026mdash; doGetBean(从缓存中拿)\u0026ndash;createBean \u0026mdash; doCreateBean(会执行一系列的流程 BeanPostProcesser)\n4. Factorybean 参考资料: https://juejin.cn/post/6844903954615107597\nFactoryBean和BeanFactory虽然名字很像，但是这两者是完全不同的两个概念\nbeanFactory \u0026mdash; spring IOC 容器\nFactorybean 是一个特殊的bean\n1. 用法 FactoryBean的特殊之处在于它可以==向容器中注册两个Bean==，一个是==它本身==，一个是==FactoryBean.getObject()方法返回值所代表的Bean==。\n代码示例\npublic void preInstantiateSingletons() throws BeansException {\r// 从容器中获取到所有的beanName\rList\u0026lt;String\u0026gt; beanNames = new ArrayList\u0026lt;\u0026gt;(this.beanDefinitionNames);\rfor (String beanName : beanNames) {\rRootBeanDefinition bd = getMergedLocalBeanDefinition(beanName);\rif (!bd.isAbstract() \u0026amp;\u0026amp; bd.isSingleton() \u0026amp;\u0026amp; !bd.isLazyInit()) {\r// 在此处会根据beanName判断bean是不是一个FactoryBean，实现了FactoryBean接口的bean，会返回true\r// 此时当beanName为customerFactoryBean时，会返回true，会进入到if语句中\rif (isFactoryBean(beanName)) {\r// 然后通过getBean()方法去获取或者创建单例对象\r// 注意：在此处为beanName拼接了一个前缀：FACTORY_BEAN_PREFIX\r// FACTORY_BEAN_PREFIX是一个常量字符串，即：\u0026amp;\r// 所以在此时容器启动阶段，对于customerFactoryBean，应该是：getBean(\u0026quot;\u0026amp;customerFactoryBean\u0026quot;)\rObject bean = getBean(FACTORY_BEAN_PREFIX + beanName);\r// 下面这一段逻辑，是判断是否需要在容器启动阶段，就去实例化getObject()返回的对象，即是否调用FactoryBean的getObject()方法\rif (bean instanceof FactoryBean) {\rfinal FactoryBean\u0026lt;?\u0026gt; factory = (FactoryBean\u0026lt;?\u0026gt;) bean;\rboolean isEagerInit;\rif (System.getSecurityManager() != null \u0026amp;\u0026amp; factory instanceof SmartFactoryBean) {\risEagerInit = AccessController.doPrivileged((PrivilegedAction\u0026lt;Boolean\u0026gt;)\r((SmartFactoryBean\u0026lt;?\u0026gt;) factory)::isEagerInit,\rgetAccessControlContext());\r}\relse {\risEagerInit = (factory instanceof SmartFactoryBean \u0026amp;\u0026amp;\r((SmartFactoryBean\u0026lt;?\u0026gt;) factory).isEagerInit());\r}\rif (isEagerInit) {\rgetBean(beanName);\r}\r}\r}\r}\r}\r}\r在容器启动阶段，会先通过==getBean()方法来创建CustomerFactoryBean的实例对象==。如果实现了==SmartFactoryBean接口，且isEagerInit()方法返回的是true==，那么在容器==启动阶段==，就会调用getObject()方法，向容器中注册getObject()方法返回值的对象。==否则，只有当第一次获取getObject()返回值的对象时，才会去回调getObject()方法==。\n在==getBean()==中会调用到==doGetBean()==方法，下面为doGetBean()精简后的源码。从源码中我们发现，最终都会调用==getObjectForBeanInstance()==方法\nprotected Object getObjectForBeanInstance(\rObject beanInstance, String name, String beanName, @Nullable RootBeanDefinition mbd) {\rif (BeanFactoryUtils.isFactoryDereference(name)) {\rif (beanInstance instanceof NullBean) {\rreturn beanInstance;\r}\rif (!(beanInstance instanceof FactoryBean)) {\rthrow new BeanIsNotAFactoryException(beanName, beanInstance.getClass());\r}\r}\r// 如果bean不是factoryBean，那么会直接返回Bean\r// 或者bean是factoryBean但name是以\u0026amp;特殊符号开头的,此时表示要获取FactoryBean的原生对象。\r// 例如：如果name = \u0026amp;customerFactoryBean，那么此时会返回CustomerFactoryBean类型的bean\rif (!(beanInstance instanceof FactoryBean) || BeanFactoryUtils.isFactoryDereference(name)) {\rreturn beanInstance;\r}\r// 如果是FactoryBean，那么先从cache中获取，如果缓存不存在，则会去调用FactoryBean的getObject()方法。\rObject object = null;\rif (mbd == null) {\r// 从缓存中获取。什么时候放入缓存的呢？在第一次调用getObject()方法时，会将返回值放入到缓存。\robject = getCachedObjectForFactoryBean(beanName);\r}\rif (object == null) {\rFactoryBean\u0026lt;?\u0026gt; factory = (FactoryBean\u0026lt;?\u0026gt;) beanInstance;\rif (mbd == null \u0026amp;\u0026amp; containsBeanDefinition(beanName)) {\rmbd = getMergedLocalBeanDefinition(beanName);\r}\rboolean synthetic = (mbd != null \u0026amp;\u0026amp; mbd.isSynthetic());\r// 在getObjectFromFactoryBean()方法中最终会调用到getObject()方法\robject = getObjectFromFactoryBean(factory, beanName, !synthetic);\r}\rreturn object;\r}\r==FactoryBean的创建流程==\n5. Spring事件驱动 基于观察者模式基础上改良\nApplicationEventPublisher\n6. Spring 循环依赖 循环依赖：一个或多个对象实例之间存在直接或间接的依赖关系，这种依赖关系构成了构成一个环形调用(闭环)\nSpring 循环依赖场景\n构造器的循环依赖 field 属性的循环依赖 ==构造器引起的循环依赖 不能解决==\n单例bean的 Setter 注入产生的循环依赖 \u0026ndash;可以解决\n==Spring怎么解决循环依赖==\nSpring的循环依赖的理论依据其实是基于==Java的引用传递==，当我们==获取到对象的引用时，对象的field或者属性是可以延后设置==的(但是构造器必须是在获取引用之前)。\n1. 三级缓存 /** Cache of singleton objects: bean name --\u0026gt; bean instance */ private final Map\u0026lt;String, Object\u0026gt; singletonObjects = new ConcurrentHashMap\u0026lt;String, Object\u0026gt;(256); /** Cache of early singleton objects: bean name --\u0026gt; bean instance */ private final Map\u0026lt;String, Object\u0026gt; earlySingletonObjects = new HashMap\u0026lt;String, Object\u0026gt;(16); /** Cache of singleton factories: bean name --\u0026gt; ObjectFactory */\rprivate final Map\u0026lt;String, ObjectFactory\u0026lt;?\u0026gt;\u0026gt; singletonFactories = new HashMap\u0026lt;String, ObjectFactory\u0026lt;?\u0026gt;\u0026gt;(16);\r一级缓存: spring的容器, 也就是存放完整的bean实例, 已经实例化和初始化好的实例\n二级缓存: bean是否被AOP 切面代理\n否: 保存半成品的bean 实例, 属性未填充 是: 保存的是代理的bean 实例beanProxy, 目标bean 还是半成品的 三件缓存: 存放的是 ObjectFactory, 传入的是一个匿名内部类, objectFactory.getObject 最终调用的是==getEarlyBeanReference方法==\n如果bean 被代理, getEarlyBeanReference则返回bean的代理对象, 如果bean未被代理, 则返回原bean 实例 2. 解决循环依赖的流程图 只使用一级缓存能不能解决循环依赖? 半成品, 属性注入未完成\n不使用二级缓存能否解决循环依赖? 可以,但是在没有AOP的情况下, 可以直接使用 一级缓存和三级缓存解决循环依赖, 但是有AOP的话则需要二级缓存来保证, 获取到的代理对象是同一个\n构造注入为什么不能解决循环依赖? 解决循环依赖的本质: 将实例化对象和 初始化对象是 分割来的, 实例化不了\n==我们先用构造函数创建一个不完整 的 bean== 实例，从这句话\n可以看出，构造器循环依赖是无法解决的，因为当构造器出现循环依赖，我们连 “不完整” 的 bean 实例\n都构建不出来。\n3. 源码 第一步: getBean\n使用getBean(java.lang.Class)从IOC中获取bean信息，实际上在IOC容器通过扫描包或加载XML后也会循环调用getBean(\u0026hellip;)进行Bean的首轮实例化。\n第二步: ==doGetBean大致步骤==：\n1.尝试根据beanName从缓存中获取获取bean对象\n2.若获取到缓存对象则执行getObjectForBeanInstance(\u0026hellip;)后返回bean信息\n3.若没有获取到缓存对象(首次创建)，则根据bean的作用域类型来采取不同方式创建bean(这里默认为单例模式)，然后再执行getObjectForBeanInstance(\u0026hellip;)后返回bean信息\n7. Spring Aop 概述\nAOP: 面向切面编程\n用途: 日志管理, 事务管理\n实现: 利用==代理模式==, 通过代理对象对被代理的对象增加功能\n所以, 关键在于AOP 框架自动创建AOP代理对象, 代理模式分为静态代理和动态代理\n框架\n==AspectJ 使用静态代理==, 编译时增强, 在编译器生成代理对象 ==SpringAop 使用动态代理==, 运行时增强, 在运行时, 动态生成代理对象 实现机制:\n​\tSpring AOP 底层实现机制目前有两种：JDK 动态代理、CGLIB 动态字节码生成。在阅读源码前对这两种机制的使用有个认识，有利于更好的理解源码。\n1. Jdk 代理和 Cglib 代理 2. Aop 哪个后置处理器 AnnotationAeareAspectAutoProxyCreator 代理对象创建的细节 BeanpostProcess的after方法中 AOP 中责任链 \u0026lt;aop: aspecj-autoproxy proxy-target-class = true/\u0026gt; 配置了AOP才生效;\n会往容器中加一个BeanPostProcessor ==proxy-target-class = true== 则 spring生成代理对象将强制使用cgli; false 就是接口使用jdk, 普通类使用cglib\n3. 责任链模式 \u0026ndash; AOP中的体现 ==一个对象被多个拦截器拦截处理时, 这样干的设计模式就是为责任链模式==\n4. 流程总结 源码剖析创建 AOP 代理\nAnnotationAwareAspectJAutoProxyCreator****类图\nAnnotationAwareAspectJAutoProxyCreator 实现了几个重要的扩展接口（可能是在父类中实现）：\n1）实现了 BeanPostProcessor 接口：实现了 postProcessAfterInitialization 方法。\n2）实现了 InstantiationAwareBeanPostProcessor 接口：实现了postProcessBeforeInstantiation 方法。\n3）实现了 SmartInstantiationAwareBeanPostProcessor 接口：实现了 predictBeanType 方法、getEarlyBeanReference 方法。\n4）实现了 BeanFactoryAware 接口，实现了 setBeanFactory 方法。对于 AOP 来说，postProcessAfterInitialization 是我们重点分析的内容，因为在该方法中，会对 bean\n进行代理，该方法由父类 AbstractAutoProxyCreator 实现。\n8. Spring mvc 流程 问题1: Springmvc 和 Spring混合使用的时候, 会创建几个容器 (父子容器)\n问题2: DispatcherServlet 初始化过程中做了什么?\ninit() \u0026ndash;\u0026gt; 创建子容器, setParent (根容器), 解析springmvc.xml, 生成了一些bean 对象: 保存地址和handlerMethod 的映射关系 mappingLookUp (mapping, handleMethod) ; urlLookUp\n例子: (\u0026quot;[GET /test/user]\u0026quot; , getUser)\n初始化9大组件 (进行一些赋值操作)\n问题3: 请求的执行流程是怎样的\n怎么根据请求url 找到controller 里面的方法 获取请求地址的uri, 根据uri 获取初始化过程中保存的映射关系(RequestMapiingInfo), 根据RequestMappingInfo 获取handleMethod 怎么设置的参数 根据method 里面的参数名称, 通过reuest获取到参数值Object[], 反射调用目标方法handler, invoke(bean, args[]); 怎么向model 设置值 底层就是像Request域中设置的值, request.setAttribute() 怎么完成视图渲染和跳转 通过视图解析器 解析逻辑视图 (拼接前缀后缀), 进行请求转发 程序的入口:\nFrameworkServlet#service 方法\ndoService 关键方法\n1. Servlet 生命周期: 默认情况下: 单实例 多线程的, 客户端发送一个请求到达客户端的时候才会进行创建 \u0026lt;load-on-startup\u0026gt;4\u0026lt;/\u0026gt; 配置了该标签的servlet 将在服务器启动时完成实例化及初始化操作\rpublic interface Servlet { // 只会执行一次\rpublic void init(ServletConfig config) throws ServletException; public ServletConfig getServletConfig(); // init-param --\u0026gt; 指定springmvc.xml 配置文件\r// 处理业务逻辑\rpublic void service(ServletRequest req, ServletResponse res) throws ServletException, IOException; public String getServletInfo(); public void destroy(); // 销毁\r}\rinvocableMethod\nreturnValueHandlers 返回值处理器\nresolvers\n​\targumentResolvers 参数处理器 2. 父子容器 问题 BeanFactory 和 ApplicationContext 的区别 ApplicationContext 继承自 BeanFactory\rBeanfactory: 提供了对bean的基本操作, 读和写\rApplicationContext: 具备BeanFactory 的功能, 还有一些拓展; 国际化操作, 后置处理器的创建\rBeanFactory 后置处理器: BeanFactoryPostProcessor 实现改接口, 可以再spring 的 bean 创建之前, 修改bean的定义属性\nbean的生命周期\n循环依赖怎么解决\n代理对象 Bean 的后置处理器\nallowBeanDefinitionOverriding = true 是否允许bean覆盖\n同一个配置文件会报错 不同配置文件, 后加载的bean会把前面加载的bean给覆盖 AllowCircleRefer 是否允许循环引用\nApplicationContextAwareProcessor 在papareBeanfactory 的时候会进行设置\n什么是Aspectj ?\n构建bean有哪些方法\nxml 配置 注解 实现BeanDefinitionRegistryPostProcess 接口, 实现方法, 构建beanDefinition 添加到BeanFactory 中 Beanfactory 后置处理器 可以通过 beanFactoryPostProcessor 和 BeanDefinitionRegistryPostProcess 接口\n初始化事件广播器 initApplicationEnentMulticaster(); spring的事件驱动, 用到了观察者模式\ngetBean 方法 获取FactoryBean 需要在 BeanName 前面加上\u0026quot;\u0026amp;\u0026quot;\nError:Kotlin: [Internal Error] java.lang.LinkageError: loader constraint violation: loader (instance of org/jetbrains/kotlin/cli/jvm/plugins/PluginURLClassLoader$SelfThenParentURLClassLoader) previously initiated loading for a different type with name \u0026quot;kotlin/sequences/Sequence\u0026quot;\rat java.lang.ClassLoader.defineClass1(Native Method)\rat\rlamda 表达式\n匿名内部类是局部内部类的更深入一步。 假如只创建某类的一个对象时，就不必将该类进行命名。 匿名内部类的前提是存在一个类或者接口，且匿名内部类是写在方法中的。 只针对重写一个方法时使用，需要重写多个方法时不建议使用\nnew 类名或接口名(){\r重写方法;\r}； //注意分号\r//以上就是内部类的格式，其实这整体就相当于是new出来的一个对象\r多线程\nnew Thread(){\rpublic void run(){\rSystem.out.println(getName());\r}\r}.start();\r上面例子，如果去除.start()就是一个线程的匿名内部类【创建匿名内部类的同时，也会创建一个\r对象】所以上文中的.start（）就相当于调用了Thread的start方法\r","date":"2023-11-03T11:21:43+08:00","permalink":"https://mikeLing-qx.github.io/p/spring%E6%BA%90%E7%A0%81/","title":"Spring源码"},{"content":"源项目文档: https://github.com/alibaba/jetcache/tree/master/docs/CN\n1. 简介 JetCache是一个基于==Java的缓存系统封装==，==提供统一的API和注解来简化缓存的使用==。 JetCache提供了==比SpringCache更加强大的注解==，可以原生的支持TTL、两级缓存、分布式自动刷新，还提供了Cache接口用于手工缓存操作。 当前有四个实现，RedisCache、TairCache（此部分未在github开源）、CaffeineCache(in memory)和一个简易的LinkedHashMapCache(in memory)，要添加新的实现也是非常简单的。\n全部特性:\n通过统一的API访问Cache系统 通过注解实现声明式的方法缓存，支持TTL和两级缓存 通过注解创建并配置Cache实例 针对所有Cache实例和方法缓存的自动统计 Key的生成策略和Value的序列化策略是可以配置的 分布式缓存自动刷新，分布式锁 (2.2+) 异步Cache API (2.2+，使用Redis的lettuce客户端时) Spring Boot支持 2. 使用 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alicp.jetcache\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jetcache-starter-redis-lettuce\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.7.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 启动类上加 @EnableMethodCache ， @EnableCreateCacheAnnotation 这两个注解, 分别激活 @Cached 和 @CreateCache 注解\n配置文件\n# 配置详情参考: https://github.com/alibaba/jetcache/blob/master/docs/CN/Config.md jetcache: statIntervalMinutes: 1 # 统计间隔，0表示不统计 areaInCacheName: false #hiddenPackages: zx.service.visu # @Cached和@CreateCache自动生成name的时候，为了不让name太长，hiddenPackages指定的包名前缀被截掉 # @Cached和@CreateCache的area属性 local/remote local: default: type: linkedhashmap # tair、redis为当前支持的远程缓存；linkedhashmap、caffeine为当前支持的本地缓存类型 keyConvertor: fastjson2 mike: type: caffeine keyConvertor: fastjson2 remote: default: type: redis.lettuce # tair、redis为当前支持的远程缓存；linkedhashmap、caffeine为当前支持的本地缓存类型 uri: redis://daheng@redis.daheng.co:6379/0?timeout=10s keyConvertor: fastjson2 valueEncoder: java valueDecoder: java poolConfig: minIdle: 5 maxIdle: 20 maxTotal: 50 mike: type: redis.lettuce # tair、redis为当前支持的远程缓存；linkedhashmap、caffeine为当前支持的本地缓存类型 uri: redis://daheng@redis.daheng.co:6379/3?timeout=10s keyConvertor: fastjson2 valueEncoder: java valueDecoder: java poolConfig: minIdle: 5 maxIdle: 20 maxTotal: 50 public interface UserService { // 添加缓存 @Cached(name=\u0026quot;userCache.\u0026quot;, key=\u0026quot;#userId\u0026quot;, expire = 3600) User getUserById(long userId); // 更新缓存 @CacheUpdate(name=\u0026quot;userCache.\u0026quot;, key=\u0026quot;#user.userId\u0026quot;, value=\u0026quot;#user\u0026quot;) void updateUser(User user); // 移除缓存 @CacheInvalidate(name=\u0026quot;userCache.\u0026quot;, key=\u0026quot;#userId\u0026quot;) void deleteUser(long userId); // CacheRefresh } ==@Cache 注解==\n属性 默认值 说明 area “default” 如果在配置中配置了多个缓存area，在这里指定使用哪个area name 未定义 指定缓存的唯一名称，不是必须的，如果没有指定，会使用类名+方法名。name会被用于远程缓存的key前缀。另外在统计中，一个简短有意义的名字会提高可读性。 key 未定义 使用SpEL指定key，如果没有指定会根据所有参数自动生成。 expire 未定义 超时时间。如果注解上没有定义，会使用全局配置，如果此时全局配置也没有定义，则为无穷大 timeUnit TimeUnit.SECONDS 指定expire的单位 cacheType CacheType.REMOTE 缓存的类型，包括CacheType.REMOTE、CacheType.LOCAL、CacheType.BOTH。如果定义为BOTH，会使用LOCAL和REMOTE组合成两级缓存 localLimit 未定义 如果cacheType为LOCAL或BOTH，这个参数指定本地缓存的最大元素数量，以控制内存占用。如果注解上没有定义，会使用全局配置，如果此时全局配置也没有定义，则为100 localExpire 未定义 仅当cacheType为BOTH时适用，为内存中的Cache指定一个不一样的超时时间，通常应该小于expire serialPolicy 未定义 指定远程缓存的序列化方式。可选值为SerialPolicy.JAVA和SerialPolicy.KRYO。如果注解上没有定义，会使用全局配置，如果此时全局配置也没有定义，则为SerialPolicy.JAVA keyConvertor 未定义 指定KEY的转换方式，用于将复杂的KEY类型转换为缓存实现可以接受的类型，当前支持KeyConvertor.FASTJSON和KeyConvertor.NONE。NONE表示不转换，FASTJSON可以将复杂对象KEY转换成String。如果注解上没有定义，会使用全局配置。 enabled true 是否激活缓存。例如某个dao方法上加缓存注解，由于某些调用场景下不能有缓存，所以可以设置enabled为false，正常调用不会使用缓存，在需要的地方可使用CacheContext.enableCache在回调中激活缓存，缓存激活的标记在ThreadLocal上，该标记被设置后，所有enable=false的缓存都被激活 cacheNullValue false 当方法返回值为null的时候是否要缓存 condition 未定义 使用SpEL指定条件，如果表达式返回true的时候才去缓存中查询 postCondition 未定义 使用SpEL指定条件，如果表达式返回true的时候才更新缓存，该评估在方法执行后进行，因此可以访问到#result ==@CacheRefresh注解==\n属性 默认值 说明 refresh 未定义 刷新间隔 timeUnit TimeUnit.SECONDS 时间单位 stopRefreshAfterLastAccess 未定义 指定该key多长时间没有访问就停止刷新，如果不指定会一直刷新 refreshLockTimeout 60秒 类型为BOTH/REMOTE的缓存刷新时，同时只会有一台服务器在刷新，这台服务器会在远程缓存放置一个分布式锁，此配置指定该锁的超时时间 ​\tkey使用Spring的SpEL脚本来指定。如果要使用参数名（比如这里的key=\u0026quot;#userId\u0026quot;），项目编译设置target必须为1.8格式，并且指定javac的-parameters参数，否则就要使用key=\u0026quot;args[0]\u0026quot;这样按下标访问的形式。\n3. 配置详解 jetcache: statIntervalMinutes: 15 areaInCacheName: false hidePackages: com.alibaba local: default: type: caffeine limit: 100 keyConvertor: fastjson2 #其他可选：fastjson/jackson expireAfterWriteInMillis: 100000 otherArea: type: linkedhashmap limit: 100 keyConvertor: none expireAfterWriteInMillis: 100000 remote: default: type: redis keyConvertor: fastjson2 #其他可选：fastjson/jackson broadcastChannel: projectA valueEncoder: java #其他可选：kryo/kryo5 valueDecoder: java #其他可选：kryo/kryo5 poolConfig: minIdle: 5 maxIdle: 20 maxTotal: 50 host: ${redis.host} port: ${redis.port} otherArea: type: redis keyConvertor: fastjson2 #其他可选：fastjson/jackson broadcastChannel: projectA valueEncoder: java #其他可选：kryo/kryo5 valueDecoder: java #其他可选：kryo/kryo5 poolConfig: minIdle: 5 maxIdle: 20 maxTotal: 50 host: ${redis.host} port: ${redis.port} 属性 默认值 说明 jetcache.statIntervalMinutes 0 统计间隔，0表示不统计 jetcache.areaInCacheName true(2.6-) false(2.7+) jetcache-anno把cacheName作为远程缓存key前缀，2.4.3以前的版本总是把areaName加在cacheName中，因此areaName也出现在key前缀中。2.4.4以后可以配置，为了保持远程key兼容默认值为true，但是新项目的话false更合理些，2.7默认值已改为false。 jetcache.hiddenPackages 无 @Cached和@CreateCache自动生成name的时候，为了不让name太长，hiddenPackages指定的包名前缀被截掉 jetcache.[local/remote].${area}.type 无 缓存类型。tair、redis为当前支持的远程缓存；linkedhashmap、caffeine为当前支持的本地缓存类型 jetcache.[local/remote].${area}.keyConvertor fastjson2 key转换器的全局配置，2.6.5+已经支持的keyConvertor：fastjson2/jackson； 2.6.5-只有一个已经实现的keyConvertor：fastjson。仅当使用@CreateCache且缓存类型为LOCAL时可以指定为none，此时通过equals方法来识别key。方法缓存必须指定keyConvertor jetcache.[local/remote].${area}.valueEncoder java 序列化器的全局配置。仅remote类型的缓存需要指定，2.7+可选java/kryo/kryo5；2.6-可选java/kryo jetcache.[local/remote].${area}.valueDecoder java 序列化器的全局配置。仅remote类型的缓存需要指定，2.7+可选java/kryo/kryo5；2.6-可选java/kryo jetcache.[local/remote].${area}.limit 100 每个缓存实例的最大元素的全局配置，仅local类型的缓存需要指定。注意是每个缓存实例的限制，而不是全部，比如这里指定100，然后用@CreateCache创建了两个缓存实例（并且注解上没有设置localLimit属性），那么每个缓存实例的限制都是100 jetcache.[local/remote].${area}.expireAfterWriteInMillis 无穷大 以毫秒为单位指定超时时间的全局配置(以前为defaultExpireInMillis) jetcache.remote.${area}.broadcastChannel 无 ==jetcahe2.7的两级缓存支持更新以后失效其他JVM中的local cache==，但多个服务共用redis同一个channel可能会造成广播风暴，需要在这里指定channel，你可以决定多个不同的服务是否共用同一个channel。如果没有指定则不开启。 jetcache.local.${area}.expireAfterAccessInMillis 0 需要jetcache2.2以上，以毫秒为单位，指定多长时间没有访问，就让缓存失效，当前只有本地缓存支持。0表示不使用这个功能 缓存的超时时间，有多个地方指定\nput等方法上指定了超时时间，则以此时间为准 put等方法上未指定超时时间，使用Cache实例的默认超时时间 Cache实例的默认超时时间，通过在@CreateCache和@Cached上的expire属性指定，如果没有指定，使用yml中定义的全局配置，例如@Cached(cacheType=local)使用jetcache.local.default.expireAfterWriteInMillis，如果仍未指定则是无穷大 ","date":"2023-11-02T15:08:41+08:00","permalink":"https://mikeLing-qx.github.io/p/spring_jetcache/","title":"Spring_jetcache"},{"content":"1. group_concat 统计name 相同的 user\nselect name from `user` group by name; 如果想把 name 相同的 phone 拼接在一起, 存放到一列中可以使用这个group_concat 函数\nselect name,group_concat(phone) from `user` group by name; 2. char_length 获取字符的长度，然后根据字符的长度进行排序\nselect * from brand where name like '%苏三%' order by char_length(name) asc limit 5; 3. locate LOCATE() 函数是 MySQL 中用于查找子串在一个字符串中的位置的函数。该函数返回第一次出现该子串的位置，如果该子串不存在于字符串中，则返回 0。\n该函数的语法如下：\nLOCATE(substr, str) 其中：\nsubstr：需要查找的子串。 str：包含该子串的字符串。 例如：\nSELECT LOCATE('world', 'Hello, world!'); ​\t该语句将返回 7，因为子串 \u0026lsquo;world\u0026rsquo; 第一次出现在字符串 \u0026lsquo;Hello, world!\u0026rsquo; 中的位置为 7。\n4. replace ​\tREPLACE() 函数是 MySQL 中用于在一个字符串中替换子串的函数。该函数将字符串中的所有出现的子串替换为指定的字符串，并返回替换后的字符串。\n其中：\nstr：需要替换的字符串, 字段名称 from_str：需要被替换的子串。 to_str：用于替换的字符串。 #语法 REPLACE(str, from_str, to_str) #例子 SELECT REPLACE('Hello, world!', 'world', 'friends'); 5. now MYSQL中获取当前时间，可以使用now()函数\nselect now() from brand limit 1; 6. insert into .. select 用来批量插入数据, 数据来源于其他表\n例子:\nINSERT INTO `brand`(`id`, `code`, `name`, `edit_date`) select null,code,name,now(3) from `order` where code in ('004','005'); 7. insert into \u0026hellip; ignore 场景: 在插入1000个品牌之前，需要先根据name，判断一下是否存在。如果存在，则不插入数据。如果不存在，才需要插入数据。\nbrand表的name字段创建了唯一索引，同时该表中已经有一条name等于苏三的数据了, 直接执行会报错\n可以使用 insert into \u0026hellip; select 配合 not exists 语句\nINSERT INTO `brand`(`id`, `code`, `name`, `edit_date`) select null,'108', '苏三',now(3) from dual where not exists (select * from `brand` where name='苏三'); 用 insert into \u0026hellip; ignore\n==INSERT INTO ... IGNORE 语句是 MySQL 中用于在插入数据时忽略重复数据的语句。当该语句在插入数据时发现重复的数据时，它会忽略该数据并继续执行。==\ntarget_table：接收数据的表。 column1, column2, ...：要插入数据的目标列。 value1, value2, ...：要插入的数据。 #语法 INSERT IGNORE INTO target_table (column1, column2, ...) VALUES (value1, value2, ...); #例子 INSERT IGNORE INTO employees (id, name, salary) VALUES (100, 'John Doe', 50000); INSERT ignore INTO `brand`(`id`, `code`, `name`, `edit_date`) VALUES (123, '108', '苏三', now(3)); 它会忽略异常，返回的执行结果影响行数为0，它不会重复插入数据。\n8. select \u0026hellip; for update 9. on duplicate key update ​\t插入数据之前，一般会先查询一下，该数据是否存在。如果不存在，则插入数据。如果已存在，则不插入数据，而直接返回结果\n​ 没啥并发量的场景中，这种做法是没有什么问题的。但如果插入数据的请求，有一定的==并发量==，这种做法就==可能会产生重复的数据==。\n​\t解决重复数据的做法: ==加唯一索引、加分布式锁==\n​\t这些方案，都==没法做到让第二次请求也更新数据==，它们一般会判断已经存在就直接返回了。\n​\nINSERT INTO `brand`(`id`, `code`, `name`, `edit_date`) VALUES (123, '108', '苏三', now(3)) on duplicate key update name='苏三',edit_date=now(3); ​\t==高并发的场景==下使用on duplicate key update语法，可能会==存在死锁的问题==，所以要根据实际情况酌情使用。\n10. show create table ​\t快速查看某张表的字段情况，通常会使用desc命令\ndesc `user` show create table `order`; 非常完整的建表语句，表名、字段名、字段类型、字段长度、字符集、主键、索引、执行引擎等都能看到。\n11. show processlist show processlist命令查看当前线程执行情况。\n12. mysql dump ​\tmysqldump 数据量很大的情况 导出xxx.sql 文件占用空间会很大 dump参数支持压缩导出(gzip) 还有==不要在线上运行的数据库== 使用dump 会==锁表== 在线迁移数据库请使用 mysql tool\n","date":"2023-10-07T15:01:41+08:00","permalink":"https://mikeLing-qx.github.io/p/mysql%E5%87%BD%E6%95%B0/","title":"MySql函数"},{"content":"RabbitMQ高级特性（二） 课程回顾：消息队列MQ\n1、MQ的介绍\n概念：消息队列，在消息传输过程中用来保存消息的容器。 目的：完成应用间的相互通信。 使用场景： 业务的解耦 异步通信 流量削峰 产品：ActiveMQ、RabbitMQ、RocketMQ、Kafka 2、RabbitMQ介绍以及安装\n介绍：略（erlang语言开发的） 安装：先安装erlang开发环境 + 再RabbitMQ【计算机名称不能为中文的】 配置：用户名 、 虚拟主机 3、RabbitMQ入门程序：使用的是MQ的简单模式\n4、RabbitMQ的通信方式：五种\n简单模式 工作模式 P/S模式：fanout，广播 路由模式：Direct，定向 将消息进行分类，将消息发送到各个满足条件的队列中 主题模式：Topic，主题 将消息进行分类，路由器是进行匹配规则 *：0个1个 #：0个1个多个 5、SpringBoot集成RabbitMQ【目的：减少代码开发】\n编写生产者：略 编写消费者：编写监听器 @RabbitListener 课程计划：\n1、消息的可靠性投递\n2、消费端ack【确认】机制\n3、消费端限流\n4、延时队列【TTL、DLX】\n5、RabbitMQ相关应用问题【了解】\n6、RabbitMQ集群【了解】 运维人员\n1 消息的可靠性投递 1.1 什么是可靠性投递 在使用RabbitMQ的时候，作为消息的发送方希望杜绝任何消息丢失或者投递失败的场景。如果消息投递失败，RabbitMQ为我们提供了两种模式用来控制消息的可靠投递。\nconfirm：确认模式 return：退回模式 -\n我们都知道MQ消息投递的流程，producer\u0026mdash;\u0026gt;exchange\u0026mdash;\u0026gt;routingKey\u0026mdash;\u0026gt;queue\u0026mdash;\u0026gt;consumer。如图所示：\n那么这两种模式又是如何工作的呢？\nconfirm模式：\n首先需要开启confirm模式 消息从producer到达exchange后，会执行一个confirmCallback回调函数 该回调函数的方法中有个ack参数 ack = true，则发送成功 ack = false，则发送失败 return模式：\n首先需要开启return模式 消息从exchange路由到queue后 如果投递成功，不会执行一个returnCallback回调函数 如果投递失败，则会执行一个returnCallback回调函数 1.2 confirm模式 1.2.1 创建工程 创建\u0026lt;rabbitmq-day02-demo1-reliable\u0026gt;工程并且添加依赖\npom文件：\n\u0026lt;!--起步依赖--\u0026gt;\r\u0026lt;parent\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;2.1.4.RELEASE\u0026lt;/version\u0026gt;\r\u0026lt;/parent\u0026gt;\r\u0026lt;dependencies\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-amqp\u0026lt;/artifactId\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;/dependencies\u0026gt;\r1.2.2 编写启动类 \u0026lt;com.itheima.ReliableApplication\u0026gt;编写启动类:\n@SpringBootApplication\rpublic class ReliableApplication {\rpublic static void main(String[] args) {\rSpringApplication.run(ReliableApplication.class, args);\r}\r}\r1.2.3 编写创建队列的配置类 创建配置类\u0026lt;com.itheima.config.ConfirmQueueConfig\u0026gt; 或者 我们也可以在启动类中创建队列。\n@Configuration\rpublic class ConfirmQueueConfig {\r// 创建队列\r@Bean\rpublic Queue confirmQueue(){\rreturn new Queue(\u0026quot;confirm-queue\u0026quot;, true);\r}\r// 创建交换机\r@Bean\rpublic Exchange confirmExchange(){\rreturn new DirectExchange(\u0026quot;confirm-exchange\u0026quot;, true, false);\r}\r// 队列绑定到交换机\r@Bean\rpublic Binding queueBindToExchange(Queue confirmQueue, Exchange confirmExchange){\rreturn BindingBuilder.bind(confirmQueue).to(confirmExchange).with(\u0026quot;confirm-routing-key\u0026quot;).noargs();\r}\r}\r1.2.4 添加yml文件 在resources目录下添加application.yml文件：\nspring:\rrabbitmq:\rhost: localhost\rport: 5672\rusername: guest\rpassword: guest\r# 开启confirm模式\rpublisher-confirms: true\r1.2.5 编写单元测试 在test包下编写单元测试类：\u0026lt;com.itheima.ReliableTest\u0026gt;\n@SpringBootTest\r@RunWith(SpringRunner.class)\rpublic class ReliableTest {\r@Autowired\rprivate RabbitTemplate rabbitTemplate;\r/**\r* @author 栗子\r* @Description confirm模式测试\r* @Date 13:15 2020/5/12\r* @param\r* @return void\r**/\r@Test\rpublic void testConfirm(){\r// 1、设置回调函数\rrabbitTemplate.setConfirmCallback(new RabbitTemplate.ConfirmCallback() {\r/**\r* @author 栗子\r* @Description\r* @Date 13:15 2020/5/12\r* @param correlationData 其他相关属性\r* @param ack 应答，成功或失败\r* @param cause 失败原因\r* @return void\r**/\r@Override\rpublic void confirm(CorrelationData correlationData, boolean ack, String cause) {\rif (ack){\rSystem.out.println(\u0026quot;我成功接收到消息了。。。\u0026quot;);\r}else {\rSystem.out.println(\u0026quot;我没有接收得到消息，原因是：\u0026quot; + cause);\r}\r}\r});\r// 2-1、正确发送消息\rrabbitTemplate.convertAndSend(\u0026quot;confirm-exchange\u0026quot;, \u0026quot;confirm-routing-key\u0026quot;, \u0026quot;confirm确认模式。。。\u0026quot;);\r// 2-2、错误发送消息，我们可以指定一个不存在的交换机\r// rabbitTemplate.convertAndSend(\u0026quot;confirm-exchange-001\u0026quot;, \u0026quot;confirm-routing-key\u0026quot;, \u0026quot;confirm确认模式。。。\u0026quot;);\r}\r}\r1.2.6 测试结果 发送消息成功：\n发送消息失败：【温馨提示：confirm模式只针对交换机，因此在测试过程中指定错误路由则callback回调函数中的ack依然为true。】\n1.3 return模式 1.3.1 创建队列 编写创建队列的配置类\u0026lt;com.itheima.config.ReturnQueueConfig\u0026gt;。\n@Configuration\rpublic class ReturnQueueConfig {\r// 创建队列\r@Bean\rpublic Queue returnQueue(){\rreturn new Queue(\u0026quot;return-queue\u0026quot;, true);\r}\r// 创建交换机\r@Bean\rpublic Exchange returnExchange(){\rreturn new DirectExchange(\u0026quot;return-exchange\u0026quot;, true, false);\r}\r// 队列绑定到交换机\r@Bean\rpublic Binding queueBindToExchangeByReturn(Queue returnQueue, Exchange returnExchange){\rreturn BindingBuilder.bind(returnQueue).to(returnExchange).with(\u0026quot;return-routing-key\u0026quot;).noargs();\r}\r}\r1.3.2 编写yml文件 开启return模式。\nspring:\rrabbitmq:\rhost: localhost\rport: 5672\rusername: guest\rpassword: guest\r# 开启confirm模式\rpublisher-confirms: true\r# 开启return模式\rpublisher-returns: true\r1.3.3 编写单元测试 在\u0026lt;ReliableTest\u0026gt;测试类中添加方法：\n@Test\rpublic void testReturn(){\r// 1、消息处理方式 true：消息通过交换机无法匹配到队列时会返回给生产者 false：匹配不到直接丢弃消息\r// rabbitTemplate.setMandatory(true); // 默认则为true，如果设置为false就算发送失败也不会执行callback方法，因为消息被丢弃\r// 2、设置回调函数\rrabbitTemplate.setReturnCallback(new RabbitTemplate.ReturnCallback() {\r/**\r* @author 栗子\r* @Description\r* @Date 15:08 2020/5/12\r* @param message 消息体\r* @param replyCode 响应码\r* @param replyText 响应信息\r* @param exchange\r* @param routingKey\r* @return void\r**/\r@Override\rpublic void returnedMessage(Message message, int replyCode, String replyText, String exchange, String routingKey) {\rSystem.out.println(\u0026quot;消息路由失败会执行该方法。。。\u0026quot;);\rSystem.out.println(\u0026quot;发送的消息体：\u0026quot; + new String(message.getBody()));\rSystem.out.println(\u0026quot;响应码：\u0026quot; + replyCode);\rSystem.out.println(\u0026quot;响应信息：\u0026quot; + replyText);\r}\r});\r// 3-1、正确发送消息\r// rabbitTemplate.convertAndSend(\u0026quot;test-return-exchange\u0026quot;, \u0026quot;test-return-routing-key\u0026quot;, \u0026quot;return退回模式。。。\u0026quot;);\r// 3-2、错误发送消息，我们可以指定一个不存在的路由\rrabbitTemplate.convertAndSend(\u0026quot;test-return-exchange\u0026quot;, \u0026quot;test-return-routing-key-001\u0026quot;, \u0026quot;return退回模式。。。\u0026quot;);\r}\r1.3.4 测试结果 PS：当程序中指定一个不存在的routingKey的时候，发送失败则会触发returnCallback回调方法。（温馨提示：指定错误的exchange无效。）\n1.4 总结 1、confirm确认模式：\n开启confirm模式（application.yml spring.publisher-confirms=true） 设置回调函数：rabbitTemplate.setConfirmCallback 若ack=true，消息投递成功 若ack=false，消息投递失败 2、return退回模式：\n开启return模式（application.yml spring.publisher-returns=true） 设置消息处理方式：rabbitTemplate.setMandatory(true) 【可以省略，默认为true。】 设置回调函数：rabbitTemplate.setReturnCallback 投递失败，则会执行该方法。 2 消费端ack机制 2.1 什么是ack 在第一章节中我们就生产者发送消息如何保证消息可靠性投递进行了处理，也就是说需要保证消息能够成功发送到broker中，但是如果消费者消费消息失败那又该如何处理呢？\n如果在处理消息的过程中，消费者的服务在处理消息的时候出现异常，那么可能这条正在处理的消息就没有完成消息消费，数据就会丢失。为了确保数据不会丢失，RabbitMQ支持确认机制ACK （Acknowledge）。\n消费端接收到消息后有三种ack方式：\n自动确认：ack = \u0026ldquo;none\u0026rdquo; 手动确认：ack = \u0026ldquo;manual\u0026rdquo; 根据异常确认（少，不考虑）：ack = \u0026ldquo;auto\u0026rdquo; 自动确认是指，消息一旦被consumer接收到则自动确认收到，并将相应的message从RabbitMQ的消息缓存中移除。但是在实际的业务处理中，很可能是消息被接收到了，但是业务处理出现了异常，那么消息从缓存中移除即该消息就被丢弃了。如果设置了手动确认，则需要在业务处理成功后，调用channel.basicAck()方法手动签收，如果出现了异常，则调用channel.basicNack()方法，让其自动重发消息。\n总结：ack机制，确认机制（消费者对mq中的消息进行确认）\n手动确认，获取到消息\u0026mdash;\u0026gt;消费消息【处理业务】\n消费成功：签收 channel.basicAck(id, true) 消费失败：将消息重回队列中 channel.basicNack() 2.2 ack代码实现 2.2.1 创建工程 创建工程\u0026lt;rabbitmq-day02-demo2-ack\u0026gt;并且添加依赖。\npom.xml文件：\n\u0026lt;!--起步依赖--\u0026gt;\r\u0026lt;parent\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;2.1.4.RELEASE\u0026lt;/version\u0026gt;\r\u0026lt;/parent\u0026gt;\r\u0026lt;dependencies\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-amqp\u0026lt;/artifactId\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;/dependencies\u0026gt;\r2.2.2 编写启动类 @SpringBootApplication\rpublic class AckApplication {\rpublic static void main(String[] args) {\rSpringApplication.run(AckApplication.class, args);\r}\r}\r2.2.3 添加application.yml文件 在resources目录下添加yml文件：\nspring:\rrabbitmq:\rhost: localhost\rport: 5672\rusername: guest\rpassword: guest\r# 消息确认方式\rlistener:\rsimple:\racknowledge-mode: manual # 手动确认\r2.2.4 编写监听器 在\u0026lt;com.itheima.listener\u0026gt;包下创建AckListener监听器\n@Component\r@RabbitListener(queues = {\u0026quot;confirm-queue\u0026quot;})\rpublic class AckListener {\r@RabbitHandler\rpublic void readMsg(String msg, Message message, Channel channel){\rSystem.out.println(\u0026quot;获取到的消息体：\u0026quot; + new String(message.getBody()));\rlong id = message.getMessageProperties().getDeliveryTag();\rtry {\rSystem.out.println(\u0026quot;业务处理成功...\u0026quot;);\r// 业务处理成功：手动签收\rchannel.basicAck(id, true);\r} catc h (Exception e) {\rSystem.out.println(\u0026quot;业务处理失败...\u0026quot;);\rtry {\rThread.sleep(2000); // 消息每隔2s发送一次\r// 业务处理失败：拒收，并且让消息重回队列\rchannel.basicNack(id, true, true);\r} catch (Exception e1) {\re1.printStackTrace();\r}\r}\r}\r}\r2.2.5 测试 我们可以模拟一个业务处理失败的场景。例如：\n2.3 总结 MQ消息的可靠性：\r1、持久化：exchange、queue、message都需要持久化\r2、生产者确保消息被成功发送，confirm\r3、消费者保证消息被消费，ack\r4、搭建Broker(mq服务)的高可用性\r3 消费端限流 3.1 限流介绍 如果并发访问量大的情况下，生产方不停的发送消息，消费端可能处理不了那么多消息，此时消息在队列中堆积很多，当消费端启动，瞬间就会涌入很多消息，消费端有可能瞬间垮掉，这时我们可以在消费端进行限流操作，每秒钟拉取多少个消息。这样就可以进行并发量的控制，减轻系统的负载，提供系统的可用性，这种效果往往可以在秒杀和抢购中进行使用。rabbitmq中有限流的配置。\n3.2 代码实现 3.2.1 开启限流 这里我们就不在单独的去创建工程了，我们在\u0026lt;rabbitmq-day02-demo2-ack\u0026gt;工程中application.yml中添加限流配置。\nspring:\rrabbitmq:\rhost: localhost\rport: 5672\rusername: guest\rpassword: guest\r# 确认方式\rlistener:\rsimple:\racknowledge-mode: manual\r# 每次最多处理消息的个数\rprefetch: 5\r3.2.2 单元测试 3.2.2.1 发送n条消息 在\u0026lt;rabbitmq-day02-demo1-reliable\u0026gt;工程的测试类ReliableTest中添加测试方法\n// 发送10条消息\r@Test\rpublic void testSendMsg(){\rfor (int i = 1; i \u0026lt;= 10; i++) {\rrabbitTemplate.convertAndSend(\u0026quot;confirm-exchange\u0026quot;, \u0026quot;confirm-routing-key\u0026quot;, \u0026quot;发送消息：\u0026quot; + i);\r}\r}\r3.2.2.2 修改监听器代码 修改\u0026lt;rabbitmq-day02-demo2-ack\u0026gt;工程中监听器代码\n删除异常代码\u0026lt;System.out.println(9/0)\u0026gt; 并且在try代码块让程序进行休眠，例如：休眠5s @Component\r@RabbitListener(queues = {\u0026quot;confirm-queue\u0026quot;})\rpublic class AckListener {\r@RabbitHandler\rpublic void readMsg(String msg, Message message, Channel channel){\rSystem.out.println(\u0026quot;获取到的消息体：\u0026quot; + new String(message.getBody()));\rlong id = message.getMessageProperties().getDeliveryTag();\rtry {\rThread.sleep(5000);\rSystem.out.println(\u0026quot;业务处理成功...\u0026quot;);\r// 业务处理成功：手动签收签收\rchannel.basicAck(id, true);\r} catch (Exception e) {\rSystem.out.println(\u0026quot;业务处理失败...\u0026quot;);\rtry {\rThread.sleep(2000); // 消息每隔2s发送一次\r// 业务处理失败：拒收，并且让消息重回队列\rchannel.basicNack(id, true, true);\r} catch (Exception e1) {\re1.printStackTrace();\r}\r}\r}\r}\r3.2.2.3 结果说明 生产者发送消息（没有启动消费者）：\nReady：待消费的消息总数，10条 Unacked：待应答的消息总数，0条 启动消费端后：\nReady：待消费的消息总数，5条 Unacked：待应答的消息总数，5条 4 延时队列 环境搭建：\n创建\u0026lt;rabbitmq-day02-demo3-delay\u0026gt;工程并且添加依赖。\n\u0026lt;!--起步依赖--\u0026gt;\r\u0026lt;parent\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;2.1.4.RELEASE\u0026lt;/version\u0026gt;\r\u0026lt;/parent\u0026gt;\r\u0026lt;dependencies\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-amqp\u0026lt;/artifactId\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;/dependencies\u0026gt;\r编写启动类\n@SpringBootApplication\rpublic class DelayApplication {\rpublic static void main(String[] args) {\rSpringApplication.run(DelayApplication.class, args);\r}\r}\r编写application.yml文件\nspring:\rrabbitmq:\rhost: localhost\rport: 5672\rusername: guest\rpassword: guest\r4.1 TTL 4.1.1 概念 TTL：Time To Live（存活时间/过期时间）\n当消息到达存活时间后，该消息还没有被消费，会自动被清除\nRabbitMQ可以对消息设置过期时间也可以对整个队列设置过期时间\n如果都设置了，哪个时间先到则生效 举个最常见了栗子：当我们在某个平台购买商品或者火车票、机票等，如果半个小时内没有支付，则该订单失效。\n4.1.2 代码实现 4.1.2.1 创建队列 在工程中创建队列的配置类\u0026lt;com.itheima.config.TTLConfig\u0026gt;\n@Configuration\rpublic class TTLConfig {\r// 创建队列\r@Bean\rpublic Queue ttlQueue(){\r// 创建队列，并且指定队列的过期时间\rreturn QueueBuilder.durable(\u0026quot;ttl-queue\u0026quot;).withArgument(\u0026quot;x-message-ttl\u0026quot;, 10000).build();\r}\r@Bean\rpublic Exchange ttlExchange(){\r// 注意，由于routingkey我们使用了匹配，因此我们要创建topic类型的交换机\rreturn new TopicExchange(\u0026quot;ttl-exchange\u0026quot;, true, false);\r}\r// 队列绑定到交换机\r@Bean\rpublic Binding queueBindToExchangeByTTL(Queue ttlQueue, Exchange ttlExchange){\rreturn BindingBuilder.bind(ttlQueue).to(ttlExchange).with(\u0026quot;ttl.#\u0026quot;).noargs();\r}\r}\r4.1.2.2 编写单元测试 在工程的test包下创建测试类\u0026lt;com.itheima.DelayTest\u0026gt;\n@Test\rpublic void testTTL(){\r// 可以设置消息的属性消息\rMessagePostProcessor messagePostProcessor = new MessagePostProcessor() {\r@Override\rpublic Message postProcessMessage(Message message) throws AmqpException {\r// 设置消息的过期时间 5s\rmessage.getMessageProperties().setExpiration(\u0026quot;5000\u0026quot;);\rreturn message;\r}\r};\rrabbitTemplate.convertAndSend(\u0026quot;test-ttl-exchange\u0026quot;,\u0026quot;ttl.hehe\u0026quot;, \u0026quot;ttl消息\u0026quot;, messagePostProcessor);\r}\rPS：温馨提示，如果同时设置了队列过期时间和消息的过期时间，那么时间越短的先生效。\r4.1.3 客户端创建队列【了解】 1、创建队列\n2、创建交换机\n3、队列绑定交换机\n4、发送消息【PS：Delivery mode ：2 - Persistent，指定为消息持久化】\n4.2 DLX 4.2.1 概念 DLX：Dead-Letter-Exchange，死信交换机。当消息成为Dead Message后，可以被重新发送到另一个交换机，这个交换机就称为死信交换机。\nDLX其始就是一个Exchange，和一般的Exchange没有区别，仅仅只是设置某个队列的属性而已。当这个队列中有死信时，RabbitMQ就会自动的将这个消息重新发布到设置的Exchange上去，进而被路由到另一个队列。消费端可以监听这个队列中的消息做相应的处理。\n4.2.2 处理过程 1、生成者将消息发送到交换机后，由交换机路由到指定的队列\n2、当该消息成为了死信后并且将该消息发送给DLX。PS：成为死信的三种情况\n队列消息长度达到限制 消费者拒签消息 原队列中存在消息过期设置，消息到达超时时间未被消费 3、DLX再将这个消息路由给死信队列，并且由对应的消费者消费\n4.2.3 代码实现 创建配置类：\n@Configuration\rpublic class DLXConfig {\r// 创建交换机\r@Bean\rpublic Exchange delayExchange(){\rreturn new DirectExchange(\u0026quot;delay-exchange\u0026quot;);\r}\r// 创建队列\r@Bean\rpublic Queue delayQueue(){\rMap\u0026lt;String, Object\u0026gt; args = new HashMap\u0026lt;\u0026gt;();\rargs.put(\u0026quot;x-message-ttl\u0026quot;, 20000); // 队列过期时间\rargs.put(\u0026quot;x-max-length\u0026quot;, 10000000); // 队列中消息数量\rargs.put(\u0026quot;x-dead-letter-exchange\u0026quot;, \u0026quot;dlx-exchange\u0026quot;); // 绑定死信交换机\rargs.put(\u0026quot;x-dead-letter-routing-key\u0026quot;, \u0026quot;dlx-routing-key\u0026quot;); // 绑定死信路由器\rreturn QueueBuilder.durable(\u0026quot;delay-queue\u0026quot;).withArguments(args).build();\r}\r// 将队列绑定到交换机\r@Bean\rpublic Binding delayBinding(Queue delayQueue, Exchange delayExchange){\rreturn BindingBuilder.bind(delayQueue).to(delayExchange).with(\u0026quot;delay-routing-key\u0026quot;).noargs();\r}\r// 创建死信交换机\r@Bean\rpublic Exchange dlxExhange(){\rreturn new DirectExchange(\u0026quot;dlx-exchange\u0026quot;);\r}\r// 创建死信队列\r@Bean\rpublic Queue dlxQueue(){\rreturn new Queue(\u0026quot;dlx-queue\u0026quot;);\r}\r// 将死信队列绑定到死信交换机上\r@Bean\rpublic Binding dlxBinding(Queue dlxQueue, Exchange dlxExhange){\rreturn BindingBuilder.bind(dlxQueue).to(dlxExhange).with(\u0026quot;dlx-routing-key\u0026quot;).noargs();\r}\r}\r单元测试：\n@Test\rpublic void testDLX(){\r// 发送消息\rrabbitTemplate.convertAndSend(\u0026quot;delay-exchange\u0026quot;,\u0026quot;delay-routing-key\u0026quot;, \u0026quot;死信消息\u0026quot;);\r}\r发送消息：\n20S后，我们再看：\n4.3 延时队列 4.3.1 什么是延时队列 延时队列，即消息进入队列后不会被立即消费，只有到达指定的时间后才会被消费。比较遗憾的是，RabbitMQ本身没有直接支持延迟队列的功能，但是可以通过TTL + DLX组合来实现延时队列的效果。\n4.3.2 需求 例如：用户下单后，30分钟内未完成支付，取消订单回滚库存。\n解决方案：\n定时器 延时队列 4.3.3 代码实现 在【4.2章节中】我们其始已实现了延时队列了。我们只需要去监听死信队列去消费消息即可。创建监听器，如下：\n@Component\r@RabbitListener(queues = {\u0026quot;dlx-queue\u0026quot;})\rpublic class DelayListener {\r@RabbitHandler\rpublic void readMsg(String msg){\rSimpleDateFormat sdf = new SimpleDateFormat(\u0026quot;yyyy-MM-dd hh:mm:ss\u0026quot;);\rSystem.out.println(\u0026quot;消费消息时间：\u0026quot; + sdf.format(new Date()));\rSystem.out.println(\u0026quot;获取到的消息为：\u0026quot; + msg);\r}\r}\r// 单元测试\r@Test\rpublic void testDLX(){\r// 发送消息\rrabbitTemplate.convertAndSend(\u0026quot;delay-exchange\u0026quot;,\u0026quot;delay-routing-key\u0026quot;, \u0026quot;死信消息\u0026quot;);\rSimpleDateFormat sdf = new SimpleDateFormat(\u0026quot;yyyy-MM-dd hh:mm:ss\u0026quot;);\rSystem.out.println(\u0026quot;发送消息时间：\u0026quot; + sdf.format(new Date()));\r}\r5 RabbitMQ应用 5.1 日志与监控 5.1.1 日志 Linux OS下：\nRabbitMQ默认存放日志的路径：/var/log/rabbitmq/rabbit@xxx.log\nWindows OS下，例如：\n5.1.2 管控台监控 5.2 消息补偿-面试 需求：100%保证消息发送成功\n5.3 消息幂等性保障-面试 思想一样。\nMySql的数据库引擎：InnoDB（默认） + MYISAM 建表：engines=MYISAM\n数据结构：InnoDB B+T B+T+链表 不会进行全表扫描 需求：数据 库存只有1件了。 显示10个人买。\n悲观锁：for update **乐视锁：**需要在表设计的时候添加一个额外的字段 version（1 -2） 幂等性：指一次或多次请求某一个资源，对于资源本身应该具有同样的结果。也就是说，任意多次请求对资源本身所产生的影响均与一次请求所产生的影响相同。在MQ中，消费多条相同的消息，得到与消费该消息一次相同的结果。\n6 RabbitMQ集群-了解 没有意义：docker\u0026mdash;部署集群的mq\n注意：在学习docker的时候我们在给大家演示如何搭建集群。\n总结：MQ的高级特性 1、消息的可靠性投递\nconfirm：确认模式【交换机】 return：退回模式【路由器】 2、消息的确认机制【ack】 消费方\n自动确认：略 手动确认：快递送到咱手中，本人需要签字。（买了华为P30 送了MP3：拒收，消息重回队列） 3、消费端限流：配置限流【每次处理消息的最大个数】\n4、延时队列：TTL + DLX\nTTL：过期 DLX：死信交换机 死信队列 5、RabbitMQ的应用\n日志和监控 消息补偿：要求消息100%发送到队列中 消息幂等性; 保障：乐视锁 7. Rabbit 避免重复消费; 8. 保证消息的顺序 大体来说主流的解决方案有两种：\n一种是使用单线程消费来保证消息的顺序性 对消息进行编号，消费者处理时根据编号来判断顺序 乍一看，觉得两种方案没有什么问题，但是深入了解下，觉得这两种解决方案并不能完全接保证消息的消费的顺序性问题。\n首先我们来分析下，是什么导致RabbitMQ消息的顺序性问题的，常见的有以下几种情形：\n消息生产者启用了发送确认机制，在发生超时、中断等，需要RabbitMQ补偿发送时，那么此时消息在源头就已经出现顺序混乱了，导致消息被消费时也是乱序的 另一种情况，如果消息发送时，设置了超时时间，并且采用了死信队列，模拟了延时队列的效果，那么此时消息的顺序也时不能保证的 还有一种情况，如果消息设置了优先级，那么在高并发的情况下，消息的顺序也是得不到保证的，消息的消费顺序也就不能保证了 刚才我们分析了出现消息乱序的几种情况，这里我们首先排除了实用单线程来消费，原因很简单，发送的消息的顺序（源头数据的顺序出现了异常）出现了异常，单线程消费的顺序肯定也是异常的\n其次对消息进行编号，这个可以解决消息顺序的问题，但是对加大了对业务处理的复杂性\nmessageId\n","date":"2023-08-10T16:15:39+08:00","permalink":"https://mikeLing-qx.github.io/p/rabbitmq_2/","title":"RabbitMq_2"},{"content":"RabbitMQ（一） 课程回顾：\n1、OpenFeign进行远程调用\n介绍 对RestTemplate进行了封装 通过更加优雅的方式【客户端 编写Feign相当于编写Service】进行调用 集成了负载均衡、集成了熔断器、支持日志、支持请求与响应压缩 代码实现：略 2、SpringCloudGateway网关\n场景：鉴权操作、认证操作、记录日志、限流等等 网关介绍： 路由：转发到哪个服务上 断言：匹配具体的url地址 过滤器：在本次的请求中，通过过滤器完成一些限制（业务需求） 过滤器： 局部过滤器 全局过滤器 官方自带的过滤器 自定义过滤器： 全局过滤器：实现GlobalFilter接口，还可以实现Ordered接口，指定过滤器的执行顺序的【代替该接口：@Order注解】 局部过滤器： 继承AbstractGatewayFilterFactory 自定义过滤器的名称规范：XxxGatewayFilterFactory 需要在yml文件中配置 3、SpringCloudConfig：配置中心\n配置中心：管理服务中的配置文件的 执行流程：托管平台（码云/GitHub）\u0026mdash;\u0026gt;配置中心\u0026mdash;\u0026gt;分发到具体的服务中 学习目标：MQ：Message Queue（消息队列）\n1、消息队列介绍【了解】\n2、RabbitMQ介绍以及安装【应用】\n3、编写RabbitMQ的入门程序【掌握】\n4、RabbitMQ的通信方式【重点】\n5、SpringBoot集成RabbitMQ【掌握】\n1. 消息队列概述 学习过Redis：5种数据结构 List可以作为消息队列。\n1.1. 什么是MQ 消息队列（MQ）是一种应用程序对应用程序的通信方法。应用程序通过写和检索出入列队的针对应用程序的数据（消息）来通信，而无需专用连接来链接它们。消息传递指的是程序之间通过在消息中发送数据进行通信，而不是通过直接调用彼此来通信，直接调用通常是用于诸如远程过程调用的技术。排队指的是应用程序通过队列来通信。队列的使用除去了接收和发送应用程序同时执行的要求。\nMQ全称为Message Queue，消息队列作用在（一个）应用程序和（一个或多个）应用程序之间进行通信过程中保存消息的容器。【是在消息的传输过程中保存消息的容器 】\n1.2 常见产品 场景的消息中间件（MOM）产品有：RabbitMQ ActiveMQ RocketMQ kafka\nkafka Apache下的一个子项目，使用scala实现的一个高性能分布式Publish/Subscribe消息队列系统。\r1.快速持久化：通过磁盘顺序读写与零拷贝机制，可以在O(1)的系统开销下进行消息持久化；\r2.高吞吐：在一台普通的服务器上既可以达到10W/s的吞吐速率；\r3.高堆积：支持topic下消费者较长时间离线，消息堆积量大；\r4.完全的分布式系统：Broker、Producer、Consumer都原生自动支持分布式，依赖zookeeper自动实现负载均衡；\r5.支持Hadoop数据并行加载：对于像Hadoop的一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。\r1.3 MQ的常见应用场景 1.3.1 应用解耦 双11是购物狂节,用户下单后,订单系统需要通知库存系统,传统的做法就是订单系统调用库存系统的接口.这种做法有一个缺点:当库存系统出现故障时,订单就会失败。(这样某云、某东将少赚好多好多钱)订单系统和库存系统高耦合.\n用户提交订单：同步处理，需要执行的时间：50ms * n 思考：在上述业务逻辑中，其实用户只需要关心自己的订单提交是否成功即可，至于其他的服务（业务）处理与用户是没有任何关系的，因此针对这个场景，我们是否可以这样去做呢？ 由此可以看出,引入消息队列后，用户的响应时间就等于写入数据库的时间+写入消息队列的时间(可以忽略不计),引入消息队列后处理后,响应时间提高3倍。\n订单系统:用户下单后,订单系统完成持久化处理,将消息写入消息队列,返回用户订单下单成功。库存系统:订阅下单的消息,获取下单消息,进行库操作。就算库存系统出现故障,消息队列也能保证消息的可靠投递,不会导致消息丢失。\n1.3.2 异步处理 例如：用户注册。\n用户注册，将数据写入数据库，然后等待系统发送短信，收到短信后继续等待系统发送邮件。这个过程中短信业务与邮件业务与用户也是无关的，因此也可以通过MQ去解决 MQ：用户注册过程中，只需要将手机号和email发送到MQ中即可，然后由其他服务监听MQ然后获取消息并且处理相关业务。 1.3.3 流量削峰 流量削峰一般在秒杀活动中应用广泛。场景:秒杀活动，一般会因为流量过大，导致应用挂掉,为了解决这个问题，一般在应用前端加入消息队列。 作用:\n可以控制活动人数，超过此一定阀值的订单直接丢弃(这就是为什么秒杀一次都没有成功过:cry:) 可以缓解短时间的高流量压垮应用(应用程序按自己的最大处理能力获取订单) 用户的请求,服务器收到之后,首先写入消息队列,加入消息队列长度超过最大值,则直接抛弃用户请求或跳转到错误页面 秒杀业务根据消息队列中的请求信息，再做后续业务处理 1.4 思考 所有服务间调用是否都可以使用MQ？不是。\n记住一个原则：调用方实时依赖执行结果的业务场景，使用直接调用，而不是MQ 。\n1.5 AMQP 和 JMS AMQP高级消息队列协议，是一个进程间传递异步消息的网络协议，更准确的说是一种binary wire-level protocol（链接协议）。这是其和JMS的本质差别，AMQP不从API层进行限定，而是直接定义网络交换的数据格式。\nJMS即Java消息服务（JavaMessage Service）应用程序接口，是一个Java平台中关于面向消息中间件（MOM）的API，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。\nJMS和AMQP区别\nJMS是定义了统一的接口，来对消息操作进行统一；AMQP是通过规定协议来统一数据交互的格式\nJMS限定了必须使用Java语言；AMQP只是协议，不规定实现方式，因此是跨语言的。\nJMS规定了两种消息模式（P2P、P/S）；而AMQP的消息模式更加丰富\nJMS：应用层接口 AMQP：消息传输的协议。\n2. RabbitMQ介绍以及安装 2.1 RabbitMQ介绍 RabbitMQ是由erlang语言开发，基于AMQP（Advanced Message Queue 高级消息队列协议）协议实现的消息队列，它是一种应用程序之间的通信方法，消息队列在分布式系统开发中应用非常广泛。\nRabbitMQ官方地址：http://www.rabbitmq.com/\nRabbitMQ提供了5种通信模式：简单模式，work模式，Publish/Subscribe发布与订阅模式，Routing路由模式，Topics主题模式；\n官网对应模式介绍：https://www.rabbitmq.com/getstarted.html\n2.2. 安装说明 略：详细查看 资料/软件/安装Windows RabbitMQ.pdf 文档。\n1、先安装erlang\n2、再安装mq服务\n2.3 用户以及Virtual Hosts配置 2.3.1 配置账号 RabbitMQ在安装好后，可以访问http://localhost:15672 ；其自带了guest/guest的用户名和密码；如果需要创建自定义用户；那么也可以登录管理界面后，如下操作：\n创建用户后效果如下：\n角色说明： 超级管理员(administrator)：可登陆管理控制台，可查看所有的信息，并且可以对用户，策略(policy)进行操作。 监控者(monitoring)：可登陆管理控制台，同时可以查看rabbitmq节点的相关信息(进程数，内存使用情况，磁盘使用情况等) 策略制定者(policymaker)：可登陆管理控制台, 同时可以对policy进行管理。但无法查看节点的相关信息(上图红框标识的部分)。 普通管理者(management)：仅可登陆管理控制台，无法看到节点信息，也无法对策略进行管理。 其他：无法登陆管理控制台，通常就是普通的生产者和消费者。 2.3.2 配置Virtual Hosts 像mysql拥有数据库的概念并且可以指定用户对库和表等操作的权限。RabbitMQ也有类似的权限管理；在RabbitMQ中可以虚拟消息服务器Virtual Host，每个Virtual Hosts相当于一个相对独立的RabbitMQ服务器，每个VirtualHost之间是相互隔离的。exchange、queue、message不能互通。 相当于mysql的db。Virtual Name一般以/开头。\n第一步：创建Virtual Hosts\n第二步：设置Virtual Hosts权限\n首先：点击【名称】\n权限设置：进入权限设置页面后，关联一个用户，然后确定【Set permission】。\n效果如下：\n参数说明：\nuser：用户名\rconfigure ：一个正则表达式，用户对符合该正则表达式的所有资源拥有 configure 操作的权限\rwrite：一个正则表达式，用户对符合该正则表达式的所有资源拥有 write 操作的权限\rread：一个正则表达式，用户对符合该正则表达式的所有资源拥有 read 操作的权限\r3 RabbitMQ入门程序 入门案例将使用RabbitMQ的简单模式实现。\n3.1 创建工程 创建工程：rabbitmq-day01-demo1-quickstart\n添加依赖：\n\u0026lt;!--添加mq依赖--\u0026gt;\r\u0026lt;dependencies\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;com.rabbitmq\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;amqp-client\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;5.6.0\u0026lt;/version\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;/dependencies\u0026gt;\r3.2 创建生产者 3.2.1 实现步骤 创建生产者具体步骤：\n// 1.创建链接工厂对象\r// 2.设置RabbitMQ服务主机地址,默认localhost\r// 3.设置RabbitMQ服务端口,默认5672\r// 4.设置虚拟主机名字，默认/\r// 5.设置用户连接名，默认guest\r// 6.设置链接密码，默认guest\r// 7.创建一个新链接\r// 8.创建消息通道\r// 9.创建队列\r// 10.创建消息\r// 11.消息发送\r// 12.关闭资源\r创建com.itheima.quickstart.Producer类，代码如下：\npublic class Producer {\rpublic static void main(String[] args) throws IOException, TimeoutException {\r// 1.创建链接工厂对象\rConnectionFactory connectionFactory = new ConnectionFactory();\r// 2.设置RabbitMQ服务主机地址,默认localhost\rconnectionFactory.setHost(\u0026quot;localhost\u0026quot;);\r// 3.设置RabbitMQ服务端口,默认5672\rconnectionFactory.setPort(5672);\r// 4.设置虚拟主机名字，默认/\rconnectionFactory.setVirtualHost(\u0026quot;/sz_itcast\u0026quot;);\r// 5.设置用户连接名，默认guest\rconnectionFactory.setUsername(\u0026quot;admin\u0026quot;);\r// 6.设置链接密码，默认guest\rconnectionFactory.setPassword(\u0026quot;123456\u0026quot;);\r// 7.创建链接\rConnection connection = connectionFactory.newConnection();\r// 8.创建消息通道\rChannel channel = connection.createChannel();\r// 9.创建队列\r// arg0：队列名称 arg1：是否持久化 arg2：是否排外 arg3：关闭连接时队列是否自动删除 arg4：队列其他参数\rchannel.queueDeclare(\u0026quot;simple_queue\u0026quot;, true, false, false, null);\r// 10.创建消息\rString message = \u0026quot;你好，欢迎来到程序员。\u0026quot;;\r// 11.消息发送\r// arg0：交换机名称，没有指定使用默认的Default Exchange\r// arg1：路由key，点对点模式可以使用队列名称 arg2：指定消息其他属性 arg3：消息的字节码\rchannel.basicPublish(\u0026quot;\u0026quot;, \u0026quot;simple_queue\u0026quot;, null, message.getBytes());\r// 12.关闭资源\rchannel.close();\rconnection.close();\r}\r}\rPS：参数详细说明\rarg2 boolean exclusive 是否排外\r如果不是排外的，可以使用两个消费者都访问同一个队列，没有任何问题，如果是排外的，会对当前队列加锁，其他通道channel是不能访问的，如果强制访问会报异常。\rarg4:队列其他参数设置\r- Message TTL 设置消息生命周期\r- Auto Expire 当队列在指定的时间没有被访问就会被删除\r- Max Length 限定队列的消息的最大值长度，超过指定长度将会把最早的几条删除掉\r- Max Length Bytes 限定队列最大占用的空间大小\r- DLX 当队列消息长度大于最大长度、或者过期的等，将从队列中删除的消息推送到指定的交换机中去而不是丢弃掉\r- DLK 将删除的消息推送到指定交换机的指定路由键的队列中去\r- Maximum priority 声明优先级队列\r- Lazy mode 先将消息保存到磁盘上，不放在内存中，当消费者开始消费的时候才加载到内存中\r3.2.2 效果 在执行上述的消息发送之后；可以登录rabbitMQ的管理控制台：\n如果想查看消息，可以点击队列名称-\u0026gt;Get Messages,如下图：\n3 自己实现代码 package com.itheima.simple;\rimport com.rabbitmq.client.Channel;\rimport com.rabbitmq.client.Connection;\rimport com.rabbitmq.client.ConnectionFactory;\rimport java.io.IOException;\rimport java.util.concurrent.TimeoutException;\r/**\r* @description: 简单模式 生产者 没有交换机 , 队列名称 = 路由器名称\r* @author: QIXIANG LING\r* @date: 2020/5/23 19:25\r*/\rpublic class SimpleProducer {\rpublic static void main(String[] args) throws Exception {\r// 1. 创建连接工厂\rConnectionFactory connectionFactory = new ConnectionFactory();\r// 2. 设置RabbitMq服务主机地址\rconnectionFactory.setHost(\u0026quot;localhost\u0026quot;);\r// 3, 设置RabbitMq 服务端口, 默认5672 (通信端口 TCP) (15672 是应用层的端口 http 访问rabbitMQ客户端的端口)\rconnectionFactory.setPort(5672);\r// 4.设置虚拟主机名字, 默认/\rconnectionFactory.setVirtualHost(\u0026quot;/sz_itheima88\u0026quot;);\r// 5. 设置用户连接名和 密码\rconnectionFactory.setUsername(\u0026quot;Rina\u0026quot;);\rconnectionFactory.setPassword(\u0026quot;123456\u0026quot;);\r// 7. 创建新连接\rConnection connection = connectionFactory.newConnection();\r// 8. 创建消息通道\rChannel channel = connection.createChannel();\r// 9. 创建队列\r// 需要参数 String queue, boolean durable, boolean exclusive, boolean autoDelete,Map\u0026lt;String, Object\u0026gt; arguments\r// queue 队列的名称 ; durable 是否持久化 ; exclusive 是否排外 (操作这个队列的时候是否允许别人也操作)\r// autoDelete : 是否自动删除 ; arguments (指定队列的一些额外属性 ) 队列的过期时间 , 长度\rchannel.queueDeclare(\u0026quot;simple-queue\u0026quot;,true,false,false,null);\r// 10 . 创建消息\rString message = \u0026quot;望月理奈,夏海里伽子,花鸟玲爱\u0026quot;;\r// 11 .消息发送\r// String exchange (指定交换机的名称) , String routingKey (路由器的名称) , BasicProperties props (指定消息的属性 :可指定过期时间), byte[] body\rchannel.basicPublish(\u0026quot;\u0026quot;,\u0026quot;simple-exchange\u0026quot;,null,message.getBytes(\u0026quot;UTF-8\u0026quot;));\r}\r}\r3.2.3 Queues列表说明 PS：列表说明\r- Features消息特征：\r- D：持久化\r- State：当前的状态\r- running：运行中\r- idle：空闲。 - Ready：待消费的消息总数 - Unacked：待应答的消息总数 - total：消息总数 = Ready+Unacked - incoming：消息进入的速率\r- deliver/get：消息获取的速率\r- ack：消息应答的速率\r3.3 创建消费者 消费者创建可以按照如下步骤实现：\n// 1.创建链接工厂对象\r// 2.设置RabbitMQ服务主机地址,默认localhost\r// 3.设置RabbitMQ服务端口,默认5672\r// 4.设置虚拟主机名字，默认/\r// 5.设置用户连接名，默认guest\r// 6.设置链接密码，默认guest\r// 7.创建一个新链接\r// 8.创建消息通道\r// 9.创建队列\r// 10.创建消费者，并设置消息处理\r// 11.消息监听\r// 12.关闭资源(不建议关闭，建议一直监听消息)\r按照上面的步骤创建消息消费者com.itheima.rabbitmq.simple.Consumer代码如下：\npublic class Consumer {\rpublic static void main(String[] args) throws IOException, TimeoutException {\r// 1.创建链接工厂对象\rConnectionFactory connectionFactory = new ConnectionFactory();\r// 2.设置RabbitMQ服务主机地址,默认localhost\rconnectionFactory.setHost(\u0026quot;localhost\u0026quot;);\r// 3.设置RabbitMQ服务端口,默认5672\rconnectionFactory.setPort(5672);\r// 4.设置虚拟主机名字，默认/\rconnectionFactory.setVirtualHost(\u0026quot;/sz_itcast\u0026quot;);\r// 5.设置用户连接名，默认guest\rconnectionFactory.setUsername(\u0026quot;admin\u0026quot;);\r// 6.设置链接密码，默认guest\rconnectionFactory.setPassword(\u0026quot;123456\u0026quot;);\r// 7.创建一个新链接\rConnection connection = connectionFactory.newConnection();\r// 8.创建消息通道\rChannel channel = connection.createChannel();\r// 9.创建队列\rchannel.queueDeclare(\u0026quot;simple_queue\u0026quot;, true, false, false, null);\r// 10.创建消费者，并消费消息\rDefaultConsumer consumer = new DefaultConsumer(channel){\r/**\r* @param consumerTag 消费者标签，在channel.basicConsume时候可以指定\r* @param envelope 消息包的内容，可从中获取消息id，消息routing key，交换机，消息和重发标志(收到消息失败后是否需要重新发送)\r* @param properties 消息属性信息\r* @param body 消息体\r**/\r@Override\rpublic void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException {\r// 路由\rString routingKey = envelope.getRoutingKey();\r// 交换机\rString exchange = envelope.getExchange();\r// 消息id\rlong deliveryTag = envelope.getDeliveryTag();\r// 消息体\rString message = new String(body, \u0026quot;UTF-8\u0026quot;);\rSystem.out.println(\u0026quot;路由：\u0026quot; + routingKey + \u0026quot;,交换机：\u0026quot; + exchange + \u0026quot;,消息id：\u0026quot; + deliveryTag + \u0026quot;,消息体：\u0026quot; + message);\r}\r};\r// 11.消息监听 arg0：监听的队列名称\r// arg1：是否自动应答，设置为true为表示消息接收到自动向mq回复接收到了，mq接收到回复会删除消息，设置为false则需要手动确认\r// arg2：消费者接收消息到后回调（消费消息）\rchannel.basicConsume(\u0026quot;simple_queue\u0026quot;, true, consumer);\r// 12.关闭资源(不建议关闭，建议一直监听消息)\r}\r}\rPS：同一个会话， consumerTag 是固定的 可以做此会话的名字， deliveryTag 每次接收消息+1，可以做此消息处理通道的名字\r3.3.2 console控制台 执行后，控制台输入如下：\n3.3.3 RabbitMQ控制台 3.4 代码抽取 重复代码： 无论是消费者，还是生产者，都需要创建连接，因此我们可以将这段公共的代码抽取到工具类中。创建com.itheima.rabbitmq.util.ConnectionUtil工具类对象，用于创建Connection，代码如下：\npublic class ConnectionUtils {\r// 提供一个公共的静态的访问方法\rpublic static Connection getConnection() throws Exception {\r// 1.创建链接工厂对象\rConnectionFactory connectionFactory = new ConnectionFactory();\r// 2.设置RabbitMQ服务主机地址,默认localhost\rconnectionFactory.setHost(\u0026quot;localhost\u0026quot;);\r// 3.设置RabbitMQ服务端口,默认5672\rconnectionFactory.setPort(5672);\r// 4.设置虚拟主机名字，默认/\rconnectionFactory.setVirtualHost(\u0026quot;/sz_itcast\u0026quot;);\r// 5.设置用户连接名，默认guest\rconnectionFactory.setUsername(\u0026quot;admin\u0026quot;);\r// 6.设置链接密码，默认guest\rconnectionFactory.setPassword(\u0026quot;123456\u0026quot;);\r// 7.创建链接\rConnection connection = connectionFactory.newConnection();\rreturn connection;\r}\r}\r生产者优化 修改Producer，链接对象使用上面的ConnectionUtil工具类创建，代码如下：\n//创建链接\rConnection connection = ConnectionUtil.getConnection();\r消费者优化 修改Consumer，链接对象使用上面的ConnectionUtil工具类创建，代码如下：\n//创建链接\rConnection connection = ConnectionUtil.getConnection();\r3.4 小结 上述的入门案例中使用的是如下的AMQP最简单的P2P通信方式：\n在上图的模型中，有以下概念：\nP：生产者，也就是要发送消息的程序\rC：消费者：消息的接受者，会一直等待消息到来。\rqueue：消息队列，图中红色部分。类似一个邮箱，可以缓存消息；生产者向其中投递消息，消费者从其中取出消息。\r在rabbitMQ中的消费者是一定要监听某个消息队列才能获取消息。\r4. RabbitMQ工作模式 RabbitMQ提供了6种模式：简单模式，work模式，Publish/Subscribe发布与订阅模式，Routing路由模式，Topics主题模式。\n4.1 Work queues工作队列模式 4.1.1 模式说明 Work Queues与入门程序的简单模式相比，多了一个或多个消费端，多个消费端共同消费同一个队列中的消息。\n应用场景：对于任务过重或任务较多情况使用工作队列可以提高任务处理的速度（也就是多人处理）。\n4.1.2 代码实现 Work Queues与入门程序的简单模式的代码是几乎一样的；可以完全复制，并复制多一个消费者进行多个消费者同时消费消息的测试。\n4.1.2.1 创建生产者 创建WorkProducer消息生产者对象，代码如下：\npublic class WorkProducer {\rpublic static void main(String[] args) throws IOException, TimeoutException {\r// 获取连接\rConnection connection = ConnectionUtils.getConnection();\r// 创建消息通道\rChannel channel = connection.createChannel();\rchannel.queueDeclare(\u0026quot;work_queue\u0026quot;, true, false, false, null);\rfor (int i = 0; i \u0026lt; 10; i++){\r// 创建消息\rString message = \u0026quot;你好，欢迎来到黑马程序员，学号：\u0026quot; + i;\r// 消息发送\rchannel.basicPublish(\u0026quot;\u0026quot;, \u0026quot;work_queue\u0026quot;, null, message.getBytes());\r}\r// 关闭资源\rchannel.close();\rconnection.close();\r}\r}\r4.1.2.2 创建消费者1 WorkConsumer1,代码如下：\npublic class WorkConsumer1 {\rpublic static void main(String[] args) throws IOException, TimeoutException {\r// 获取连接\rConnection connection = ConnectionUtils.getConnection();\r// 创建消息通道\rChannel channel = connection.createChannel();\r// 创建队列\rchannel.queueDeclare(\u0026quot;work_queue\u0026quot;, true, false, false, null);\r// 创建消费者，并消费消息\rDefaultConsumer consumer = new DefaultConsumer(channel){\r@Override\rpublic void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException {\r// 路由\rString routingKey = envelope.getRoutingKey();\r// 交换机\rString exchange = envelope.getExchange();\r// 消息id\rlong deliveryTag = envelope.getDeliveryTag();\r// 消息体\rString message = new String(body, \u0026quot;UTF-8\u0026quot;);\rSystem.out.println(\u0026quot;消费者1获取的数据：\u0026quot; + \u0026quot;路由：\u0026quot; + routingKey + \u0026quot;,交换机：\u0026quot; + exchange + \u0026quot;,消息id：\u0026quot; + deliveryTag + \u0026quot;,消息体：\u0026quot; + message);\r}\r};\r// 11.消息监听 arg0：监听的队列名称\rchannel.basicConsume(\u0026quot;work_queue\u0026quot;, true, consumer);\r}\r}\r4.1.2.3 创建消费者2 创建WorkConsumer2，代码如下：\npublic class WorkConsumer2 {\rpublic static void main(String[] args) throws IOException, TimeoutException {\r// 获取连接\rConnection connection = ConnectionUtils.getConnection();\r// 创建消息通道\rChannel channel = connection.createChannel();\r// 创建队列\rchannel.queueDeclare(\u0026quot;work_queue\u0026quot;, true, false, false, null);\r// 创建消费者，并消费消息\rDefaultConsumer consumer = new DefaultConsumer(channel){\r@Override\rpublic void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException {\r// 路由\rString routingKey = envelope.getRoutingKey();\r// 交换机\rString exchange = envelope.getExchange();\r// 消息id\rlong deliveryTag = envelope.getDeliveryTag();\r// 消息体\rString message = new String(body, \u0026quot;UTF-8\u0026quot;);\rSystem.out.println(\u0026quot;消费者2获取的数据：\u0026quot; + \u0026quot;路由：\u0026quot; + routingKey + \u0026quot;,交换机：\u0026quot; + exchange + \u0026quot;,消息id：\u0026quot; + deliveryTag + \u0026quot;,消息体：\u0026quot; + message);\r}\r};\r// 11.消息监听 arg0：监听的队列名称\rchannel.basicConsume(\u0026quot;work_queue\u0026quot;, true, consumer);\r}\r}\r4.1.3 测试 启动两个消费者，然后再启动生产者发送消息；到IDEA的两个消费者对应的控制台查看是否竞争性的接收到消息。\n消费者1： 消费者2： 4.1.4 小结 在一个队列中如果有多个消费者，那么消费者之间对于同一个消息的关系是竞争的关系。\n默认，RabbitMQ会一个一个的发送信息给下一个消费者(consumer)，而不考虑每个任务的时长且是一次性分配，并非一个一个分配。平均的每个消费者将会获得相等数量的消息。这种发送消息得方式叫做——轮询（round-robin）。\nP/S：消息的发布与订阅者\nFanout：广播模式 RoutingKey：路由模式 Topic：主题模式 4.2 Fanout广播模式 4.2.1 模式说明 发布订阅模式：\n1.每个消费者监听自己的队列。\r2.生产者将消息发给broker，由交换机将消息转发到绑定此交换机的每个队列，每个绑定交换机的队列都将接收\r到消息\r而在订阅模型中，多了一个exchange角色，而且过程略有变化：\nP：生产者，也就是要发送消息的程序，但是不再发送到队列中，而是发给X（交换机）\rC：消费者，消息的接受者，会一直等待消息到来。\rQueue：消息队列，接收消息、缓存消息。\rExchange：交换机，图中的X。一方面，接收生产者发送的消息。另一方面，知道如何处理消息，例如递交给某个特别队列、递交给所有队列、或是将消息丢弃。到底如何操作，取决于Exchange的类型。Exchange有常见以下3种类型：\rFanout：广播，将消息交给所有绑定到交换机的队列\rDirect：定向，把消息交给符合指定routing key 的队列\rTopic：通配符，把消息交给符合routing pattern（路由模式） 的队列\rExchange（交换机）只负责转发消息，不具备存储消息的能力，因此如果没有任何队列与Exchange绑定，或者没有符合路由规则的队列，那么消息会丢失！\n4.2.2 代码 4.2.2.1 创建生产者 生产者需要注意如下3点：\n1.声明交换机\r2.声明队列\r3.队列需要绑定指定的交换机\r创建FanoutProducer消息生产者，代码如下：\npublic class FanoutProducer {\rpublic static void main(String[] args) throws IOException, TimeoutException {\r// 获取连接\rConnection connection = ConnectionUtils.getConnection();\r// 创建消息通道\rChannel channel = connection.createChannel();\r// 创建队列\rchannel.queueDeclare(\u0026quot;fanout_queue1\u0026quot;, true, false, false, null);\rchannel.queueDeclare(\u0026quot;fanout_queue2\u0026quot;, true, false, false, null);\r// 创建交换机：arg0,交换机名称 arg1,交换机类型（广播）\rchannel.exchangeDeclare(\u0026quot;fanout_exchange\u0026quot;, BuiltinExchangeType.FANOUT);\r// 将队列绑定到交换机\rchannel.queueBind(\u0026quot;fanout_queue1\u0026quot;, \u0026quot;fanout_exchange\u0026quot;, \u0026quot;\u0026quot;);\rchannel.queueBind(\u0026quot;fanout_queue2\u0026quot;, \u0026quot;fanout_exchange\u0026quot;, \u0026quot;\u0026quot;);\rfor (int i = 0; i \u0026lt; 10; i++){\r// 创建消息\rString message = \u0026quot;你好，欢迎来到程序员，fanout：\u0026quot; + i;\r// 消息发送\rchannel.basicPublish(\u0026quot;fanout_exchange\u0026quot;, \u0026quot;\u0026quot;, null, message.getBytes());\r}\r// 关闭资源\rchannel.close();\rconnection.close();\r}\r}\r4.2.2.2 创建消费者1 创建FanoutConsumer1消费者，代码如下：\npublic class FanoutConsumer1 {\rpublic static void main(String[] args) throws IOException, TimeoutException {\r// 获取连接\rConnection connection = ConnectionUtils.getConnection();\r// 创建消息通道\rChannel channel = connection.createChannel();\r// 创建队列\rchannel.queueDeclare(\u0026quot;fanout_queue1\u0026quot;, true, false, false, null);\r// 创建消费者，并消费消息\rDefaultConsumer consumer = new DefaultConsumer(channel){\r@Override\rpublic void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException {\r// 路由\rString routingKey = envelope.getRoutingKey();\r// 交换机\rString exchange = envelope.getExchange();\r// 消息id\rlong deliveryTag = envelope.getDeliveryTag();\r// 消息体\rString message = new String(body, \u0026quot;UTF-8\u0026quot;);\rSystem.out.println(\u0026quot;消费者1获取的数据：\u0026quot; + \u0026quot;路由：\u0026quot; + routingKey + \u0026quot;,交换机：\u0026quot; + exchange + \u0026quot;,消息id：\u0026quot; + deliveryTag + \u0026quot;,消息体：\u0026quot; + message);\r}\r};\r// 11.消息监听 arg0：监听的队列名称\rchannel.basicConsume(\u0026quot;fanout_queue1\u0026quot;, true, consumer);\r}\r}\r4.2.2.3 创建消费者2 创建FanoutConsumer2消费者，代码如下：\npublic class FanoutConsumer2 {\rpublic static void main(String[] args) throws IOException, TimeoutException {\r// 获取连接\rConnection connection = ConnectionUtils.getConnection();\r// 创建消息通道\rChannel channel = connection.createChannel();\r// 创建队列\rchannel.queueDeclare(\u0026quot;fanout_queue2\u0026quot;, true, false, false, null);\r// 创建消费者，并消费消息\rDefaultConsumer consumer = new DefaultConsumer(channel){\r@Override\rpublic void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException {\r// 路由\rString routingKey = envelope.getRoutingKey();\r// 交换机\rString exchange = envelope.getExchange();\r// 消息id\rlong deliveryTag = envelope.getDeliveryTag();\r// 消息体\rString message = new String(body, \u0026quot;UTF-8\u0026quot;);\rSystem.out.println(\u0026quot;消费者2获取的数据：\u0026quot; + \u0026quot;路由：\u0026quot; + routingKey + \u0026quot;,交换机：\u0026quot; + exchange + \u0026quot;,消息id：\u0026quot; + deliveryTag + \u0026quot;,消息体：\u0026quot; + message);\r}\r};\r// 11.消息监听 arg0：监听的队列名称\rchannel.basicConsume(\u0026quot;fanout_queue2\u0026quot;, true, consumer);\r}\r}\r4.2.3 测试 启动所有消费者，然后使用生产者发送消息；在每个消费者对应的控制台可以查看到生产者发送的所有消息；到达广播的效果。\n消费者1：\n消费者2：\n在执行完测试代码后，其实到RabbitMQ的管理后台找到Exchanges选项卡，点击 fanout_exchange 的交换机，可以查看到如下的绑定：\n4.2.4 小结 交换机需要与队列进行绑定，绑定之后；一个消息可以被多个消费者都收到。\n发布订阅模式与work队列模式的区别\n1、work队列模式不用定义交换机，而发布/订阅模式需要定义交换机。 2、发布/订阅模式的生产方是面向交换机发送消息，work队列模式的生产方是面向队列发送消息(底层使用默认交换机)。\r3、发布/订阅模式需要设置队列和交换机的绑定，work队列模式不需要设置，实际上work队列模式会将队列绑 定到默认的交换机 。\r4.3 Routing路由模式 4.3.1. 模式说明 路由模式特点：\n1.队列与交换机的绑定，不能是任意绑定了，而是要指定一个RoutingKey（路由key）\r2.消息的发送方在 向 Exchange发送消息时，也必须指定消息的 RoutingKey。\r3.Exchange不再把消息交给每一个绑定的队列，而是根据消息的Routing Key进行判断，只有队列的Routingkey与消息的 Routing key完全一致，才会接收到消息\r图解：\nP：生产者，向Exchange发送消息，发送消息时，会指定一个routing key。\rX：Exchange（交换机），接收生产者的消息，然后把消息递交给 与routing key完全匹配的队列\rC1：消费者，其所在队列指定了需要routing key 为 error 的消息\rC2：消费者，其所在队列指定了需要routing key 为 info、error、warning 的消息\r4.3.2 代码 在编码上与 Publish/Subscribe发布与订阅模式 的区别是交换机的类型为：Direct，还有队列绑定交换机的时候需要指定routing key。\n4.3.2.1 创建生产者 创建RoutingKeyProducer消息生产者，代码如下：\npublic class RoutingKeyProducer {\rpublic static void main(String[] args) throws IOException, TimeoutException {\r// 获取连接\rConnection connection = ConnectionUtils.getConnection();\r// 创建消息通道\rChannel channel = connection.createChannel();\r// 创建队列\rchannel.queueDeclare(\u0026quot;routing_key_queue1\u0026quot;, true, false, false, null);\rchannel.queueDeclare(\u0026quot;routing_key_queue2\u0026quot;, true, false, false, null);\r// 创建交换机：arg0,交换机名称 arg1,交换机类型（广播）\rchannel.exchangeDeclare(\u0026quot;routing_key_exchange\u0026quot;, BuiltinExchangeType.DIRECT);\r// 将队列绑定到交换机\r// routing_key_queue1：error\r// routing_key_queue2：error、info、warning\rchannel.queueBind(\u0026quot;routing_key_queue1\u0026quot;, \u0026quot;routing_key_exchange\u0026quot;, \u0026quot;error\u0026quot;);\rchannel.queueBind(\u0026quot;routing_key_queue2\u0026quot;, \u0026quot;routing_key_exchange\u0026quot;, \u0026quot;error\u0026quot;);\rchannel.queueBind(\u0026quot;routing_key_queue2\u0026quot;, \u0026quot;routing_key_exchange\u0026quot;, \u0026quot;info\u0026quot;);\rchannel.queueBind(\u0026quot;routing_key_queue2\u0026quot;, \u0026quot;routing_key_exchange\u0026quot;, \u0026quot;warning\u0026quot;);\rfor (int i = 0; i \u0026lt; 10; i++){\r// 创建消息\rString message = \u0026quot;你好，欢迎来到黑马程序员，routing_key：\u0026quot; + i;\rString routingKey = \u0026quot;\u0026quot;;\rif (i%2 == 0){ // routing_key_queue1、routing_key_queue2 0、2、4、6、8\rroutingKey = \u0026quot;error\u0026quot;;\r}else if (i%5 == 0){ // routing_key_queue2：5\rroutingKey = \u0026quot;info\u0026quot;;\r}else { // 0、1、5\rroutingKey = \u0026quot;warning\u0026quot;;\r}\rmessage += \u0026quot;---\u0026gt;\u0026quot; + routingKey;\r// 消息发送\rchannel.basicPublish(\u0026quot;routing_key_exchange\u0026quot;, routingKey, null, message.getBytes());\r}\r// 关闭资源\rchannel.close();\rconnection.close();\r}\r}\r4.3.2.2 创建消费者1 创建RoutingKeyConsumer1，代码如下：\npublic class RoutingKeyConsumer1 {\rpublic static void main(String[] args) throws IOException, TimeoutException {\r// 获取连接\rConnection connection = ConnectionUtils.getConnection();\r// 创建消息通道\rChannel channel = connection.createChannel();\r// 创建队列\rchannel.queueDeclare(\u0026quot;routing_key_queue1\u0026quot;, true, false, false, null);\r// 创建消费者，并消费消息\rDefaultConsumer consumer = new DefaultConsumer(channel){\r@Override\rpublic void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException {\r// 路由\rString routingKey = envelope.getRoutingKey();\r// 交换机\rString exchange = envelope.getExchange();\r// 消息id\rlong deliveryTag = envelope.getDeliveryTag();\r// 消息体\rString message = new String(body, \u0026quot;UTF-8\u0026quot;);\rSystem.out.println(\u0026quot;消费者1获取的数据：\u0026quot; + \u0026quot;路由：\u0026quot; + routingKey + \u0026quot;,交换机：\u0026quot; + exchange + \u0026quot;,消息id：\u0026quot; + deliveryTag + \u0026quot;,消息体：\u0026quot; + message);\r}\r};\r// 11.消息监听 arg0：监听的队列名称\rchannel.basicConsume(\u0026quot;routing_key_queue1\u0026quot;, true, consumer);\r}\r}\r4.3.2.3 创建消费者2 创建RoutingKeyConsumer2，代码如下：\npublic class RoutingKeyConsumer2 {\rpublic static void main(String[] args) throws IOException, TimeoutException {\r// 获取连接\rConnection connection = ConnectionUtils.getConnection();\r// 创建消息通道\rChannel channel = connection.createChannel();\r// 创建队列\rchannel.queueDeclare(\u0026quot;routing_key_queue2\u0026quot;, true, false, false, null);\r// 创建消费者，并消费消息\rDefaultConsumer consumer = new DefaultConsumer(channel){\r@Override\rpublic void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException {\r// 路由\rString routingKey = envelope.getRoutingKey();\r// 交换机\rString exchange = envelope.getExchange();\r// 消息id\rlong deliveryTag = envelope.getDeliveryTag();\r// 消息体\rString message = new String(body, \u0026quot;UTF-8\u0026quot;);\rSystem.out.println(\u0026quot;消费者2获取的数据：\u0026quot; + \u0026quot;路由：\u0026quot; + routingKey + \u0026quot;,交换机：\u0026quot; + exchange + \u0026quot;,消息id：\u0026quot; + deliveryTag + \u0026quot;,消息体：\u0026quot; + message);\r}\r};\r// 11.消息监听 arg0：监听的队列名称\rchannel.basicConsume(\u0026quot;routing_key_queue2\u0026quot;, true, consumer);\r}\r}\r4.3.3 测试 启动所有消费者，然后使用生产者发送消息；在消费者对应的控制台可以查看到生产者发送对应routing key对应队列的消息；到达按照需要接收的效果。\n消费者1：\n消费者2：\n在执行完测试代码后，其实到RabbitMQ的管理后台找到Exchanges选项卡，点击 direct_exchange 的交换机，可以查看到如下的绑定：\n4.3.4 小结 Routing模式要求队列在绑定交换机时要指定routing key，消息会转发到符合routing key的队列。\n4.4 Topics通配符模式 4.4.1 模式说明 Topic类型与Direct相比，都是可以根据RoutingKey把消息路由到不同的队列。只不过Topic类型Exchange可以让队列在绑定Routing key 的时候使用通配符！\nRoutingkey 一般都是有一个或多个单词组成，多个单词之间以”.”分割，例如： item.insert\n通配符规则：\n#：匹配一个或多个词\n*：匹配不多\n举例：\nitem.#：能够匹配item.insert.abc 或者 item.insert\nitem.*：只能匹配item.insert\n图解：\n红色Queue：绑定的是usa.# ，因此凡是以 usa.开头的routing key 都会被匹配到 黄色Queue：绑定的是#.news ，因此凡是以 .news结尾的 routing key 都会被匹配 4.4.2 代码 4.4.2.1 创建生产者 创建TopicProducer实现消息生产者，代码如下：\npublic class TopicProducer {\rpublic static void main(String[] args) throws IOException, TimeoutException {\r// 获取连接\rConnection connection = ConnectionUtils.getConnection();\r// 创建消息通道\rChannel channel = connection.createChannel();\r// 创建队列\rchannel.queueDeclare(\u0026quot;topic_queue1\u0026quot;, true, false, false, null);\rchannel.queueDeclare(\u0026quot;topic_queue2\u0026quot;, true, false, false, null);\r// 创建交换机：arg0,交换机名称 arg1,交换机类型（广播）\rchannel.exchangeDeclare(\u0026quot;topic_exchange\u0026quot;, BuiltinExchangeType.TOPIC);\r// 将队列绑定到交换机\rchannel.queueBind(\u0026quot;topic_queue1\u0026quot;, \u0026quot;topic_exchange\u0026quot;, \u0026quot;*.orange.*\u0026quot;);\rchannel.queueBind(\u0026quot;topic_queue2\u0026quot;, \u0026quot;topic_exchange\u0026quot;, \u0026quot;*.*.rabbit\u0026quot;);\rchannel.queueBind(\u0026quot;topic_queue2\u0026quot;, \u0026quot;topic_exchange\u0026quot;, \u0026quot;lazy.#\u0026quot;);\r// 发送消息\rString msg = \u0026quot;欢迎来到黑马学习quick.orange.rabbit\u0026quot;;\rbyte[] body = msg.getBytes(\u0026quot;UTF-8\u0026quot;);\rchannel.basicPublish(\u0026quot;topic_exchange\u0026quot;, \u0026quot;quick.orange.rabbit\u0026quot;, null, body); // 1/2\rString msg2 = \u0026quot;欢迎来到黑马学习lazy.pink.rabbit\u0026quot;;\rbyte[] body2 = msg2.getBytes(\u0026quot;UTF-8\u0026quot;);\rchannel.basicPublish(\u0026quot;topic_exchange\u0026quot;, \u0026quot;lazy.pink.rabbit\u0026quot;, null, body2); // 1\r// 关闭资源\rchannel.close();\rconnection.close();\r}\r}\r4.4.2.2 创建消费者1 创建TopicConsumer1，代码如下：\npublic class TopicConsumer1 {\rpublic static void main(String[] args) throws IOException, TimeoutException {\r// 获取连接\rConnection connection = ConnectionUtils.getConnection();\r// 创建消息通道\rChannel channel = connection.createChannel();\r// 创建队列\rchannel.queueDeclare(\u0026quot;topic_queue1\u0026quot;, true, false, false, null);\r// 创建消费者，并消费消息\rDefaultConsumer consumer = new DefaultConsumer(channel){\r@Override\rpublic void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException {\r// 路由\rString routingKey = envelope.getRoutingKey();\r// 交换机\rString exchange = envelope.getExchange();\r// 消息id\rlong deliveryTag = envelope.getDeliveryTag();\r// 消息体\rString message = new String(body, \u0026quot;UTF-8\u0026quot;);\rSystem.out.println(\u0026quot;消费者1获取的数据：\u0026quot; + \u0026quot;路由：\u0026quot; + routingKey + \u0026quot;,交换机：\u0026quot; + exchange + \u0026quot;,消息id：\u0026quot; + deliveryTag + \u0026quot;,消息体：\u0026quot; + message);\r}\r};\r// 11.消息监听 arg0：监听的队列名称\rchannel.basicConsume(\u0026quot;topic_queue1\u0026quot;, true, consumer);\r}\r}\r4.4.2.3 创建消费者2 创建TopicConsumer2，代码如下：\npublic class TopicConsumer2 {\rpublic static void main(String[] args) throws IOException, TimeoutException {\r// 获取连接\rConnection connection = ConnectionUtils.getConnection();\r// 创建消息通道\rChannel channel = connection.createChannel();\r// 创建队列\rchannel.queueDeclare(\u0026quot;topic_queue2\u0026quot;, true, false, false, null);\r// 创建消费者，并消费消息\rDefaultConsumer consumer = new DefaultConsumer(channel){\r@Override\rpublic void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException {\r// 路由\rString routingKey = envelope.getRoutingKey();\r// 交换机\rString exchange = envelope.getExchange();\r// 消息id\rlong deliveryTag = envelope.getDeliveryTag();\r// 消息体\rString message = new String(body, \u0026quot;UTF-8\u0026quot;);\rSystem.out.println(\u0026quot;消费者2获取的数据：\u0026quot; + \u0026quot;路由：\u0026quot; + routingKey + \u0026quot;,交换机：\u0026quot; + exchange + \u0026quot;,消息id：\u0026quot; + deliveryTag + \u0026quot;,消息体：\u0026quot; + message);\r}\r};\r// 11.消息监听 arg0：监听的队列名称\rchannel.basicConsume(\u0026quot;topic_queue2\u0026quot;, true, consumer);\r}\r}\r4.4.3 测试 启动所有消费者，然后使用生产者发送消息；在消费者对应的控制台可以查看到生产者发送对应routing key对应队列的消息；到达按照需要接收的效果；并且这些routing key可以使用通配符。\n消费者1：\n消费者2：\n在执行完测试代码后，其实到RabbitMQ的管理后台找到Exchanges选项卡，点击 topic_exchange 的交换机，可以查看到如下的绑定：\n4.4.4 小结 Topic主题模式可以实现 Publish/Subscribe发布与订阅模式 和 Routing路由模式 的功能；只是Topic在配置routing key 的时候可以使用通配符，显得更加灵活。\n4.5 RabbitMQ工作模式总结 RabbitMQ工作模式： 1、简单模式 HelloWorld 一个生产者、一个消费者，不需要设置交换机（使用默认的交换机）\n2、工作队列模式 Work Queue 一个生产者、多个消费者（竞争关系），不需要设置交换机（使用默认的交换机）\n3、发布订阅模式 Publish/subscribe 需要设置类型为fanout的交换机，并且交换机和队列进行绑定，当发送消息到交换机后，交换机会将消息发送到绑定的队列\n4、路由模式 Routing 需要设置类型为direct的交换机，交换机和队列进行绑定，并且指定routing key，当发送消息到交换机后，交换机会根据routing key将消息发送到对应的队列\n5、通配符模式 Topic 需要设置类型为topic的交换机，交换机和队列进行绑定，并且指定通配符方式的routing key，当发送消息到交换机后，交换机会根据routing key将消息发送到对应的队列\n5 Spring Boot集成RabbitMQ 5.1 简介 在Spring项目中，可以使用Spring-Rabbit去操作RabbitMQ https://github.com/spring-projects/spring-amqp\n尤其是在spring boot项目中只需要引入对应的amqp启动器依赖即可，方便的使用RabbitTemplate发送消息，使用注解接收消息。\n一般在开发过程中：\n生产者工程：\napplication.yml文件配置RabbitMQ相关信息；\n在生产者工程中编写配置类，用于创建交换机和队列，并进行绑定\n注入RabbitTemplate对象，通过RabbitTemplate对象发送消息到交换机\n消费者工程：\napplication.yml文件配置RabbitMQ相关信息\n创建消息处理类，用于接收队列中的消息并进行处理\n5.2 搭建生产者工程 5.2.1 创建工程 创建生产者工程springboot-rabbitmq-producer：略。\n5.2.2 添加依赖 修改pom.xml文件内容为如下：\n\u0026lt;parent\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;2.1.4.RELEASE\u0026lt;/version\u0026gt;\r\u0026lt;relativePath/\u0026gt;\r\u0026lt;/parent\u0026gt;\r\u0026lt;dependencies\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-amqp\u0026lt;/artifactId\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt;\r\u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;/dependencies\u0026gt;\r5.2.3 启动类 创建启动类com.itheima.ProducerApplication，代码如下：\n@SpringBootApplication\rpublic class RabbitMqProviderApplication {\rpublic static void main(String[] args) {\rSpringApplication.run(RabbitMqProviderApplication.class, args);\r}\r// 创建队列\r@Bean\rpublic Queue topicQueue(){\rreturn new Queue(\u0026quot;topic_queue_spring_boot\u0026quot;);\r}\r// 创建交换机\r@Bean\rpublic Exchange topicExchange(){\rreturn new TopicExchange(\u0026quot;topic_exchange_spring_boot\u0026quot;);\r}\r// 将队列绑定到交换机\r@Bean\rpublic Binding topicQueueBind(Queue topicQueue, Exchange topicExchange){\rreturn BindingBuilder.bind(topicQueue).to(topicExchange).with(\u0026quot;item.#\u0026quot;).noargs();\r}\r}\r5.2.4 配置RabbitMQ application.yml配置文件\nspring:\rrabbitmq:\rhost: localhost\rport: 5672\rvirtual-host: /sz_itcast\rusername: admin\rpassword: 123456\r5.3 搭建消费者工程 5.3.1 创建工程 创建消费者工程springboot-rabbitmq-consumer,工程坐标如下：\n\u0026lt;groupId\u0026gt;com.itheima\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;springboot-rabbitmq-consumer\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt;\r5.3.2 添加依赖 修改pom.xml文件内容为如下：\n\u0026lt;parent\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;2.1.4.RELEASE\u0026lt;/version\u0026gt;\r\u0026lt;relativePath/\u0026gt;\r\u0026lt;/parent\u0026gt;\r\u0026lt;dependencies\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-amqp\u0026lt;/artifactId\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt;\r\u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;/dependencies\u0026gt;\r5.3.3 启动类 创建启动类com.itheima.ConsumerApplication，代码如下：\n@SpringBootApplication\rpublic class ConsumerApplication {\rpublic static void main(String[] args) {\rSpringApplication.run(ConsumerApplication.class);\r}\r}\r5.3.4 配置RabbitMQ 创建application.yml，内容如下：\nspring:\rrabbitmq:\rhost: localhost\rport: 5672\rvirtual-host: /sz_itcast\rusername: admin\rpassword: 123456\r5.3.5 消息监听处理类 编写消息监听器com.itheima.listener.MessageListener，代码如下：\n@Component\rpublic class MessageListener {\r/**\r* 监听某个队列的消息\r* @param message 接收到的消息\r*/\r@RabbitListener(queues = \u0026quot;topic_queue_spring_boot\u0026quot;)\rpublic void myListener1(String message){\rSystem.out.println(\u0026quot;消费者接收到的消息为：\u0026quot; + message);\r}\r}\r5.4 测试 在生产者工程rabbitmq-producer中创建测试类com.itheima.test.RabbitMQTest，发送消息：\n@RunWith(SpringRunner.class)\r@SpringBootTest\rpublic class RabbitMqProviderApplicationTests {\r@Autowired\rprivate RabbitTemplate rabbitTemplate;\r@Test\rpublic void contextLoads() {\rrabbitTemplate.convertAndSend(\u0026quot;topic_exchange_spring_boot\u0026quot;,\u0026quot;item.update\u0026quot;, \u0026quot;更新\u0026quot;);\rrabbitTemplate.convertAndSend(\u0026quot;topic_exchange_spring_boot\u0026quot;,\u0026quot;item.brand.insert\u0026quot;, \u0026quot;插入\u0026quot;);\r}\r}\r先运行上述测试程序（交换机和队列才能先被声明和绑定），然后启动消费者；在消费者工程springboot-rabbitmq-consumer中控制台查看是否接收到对应消息。\n另外；也可以在RabbitMQ的管理控制台中查看到交换机与队列的绑定：\n6 安装问题 计算机名称是中文无法启动RabbitMQ解决方案：\n1、管理员运行cmd然后打开RabbitMQ安装目录\r2、rabbitmq-service.bat remove\r3、set RABBITMQ_BASE=D:\\rabbitmq_server\\data\r4、rabbitmq-service.bat install\r5、rabbitmq-plugins enable rabbitmq_management\r总结 1、对MQ介绍以及使用场景\nMQ：在消息传输过程中用来保存消息的容器 Message Queue 使用场景： 业务解耦 异步通信 流量削峰 MQ的产品有很多：略 2、RabbitMQ\n介绍：略 安装：略 客户端工具：配置用户 + 配置虚拟主机【数据库】 3、编写RabbitMQ程序：5种通信方式【重点】\n简单模式 工作模式 广播模式 路由模式 主题模式 4、SpringBoot集成RabbitMQ：代码简化了。\n","date":"2023-08-08T16:15:32+08:00","permalink":"https://mikeLing-qx.github.io/p/rabbitmq_1/","title":"RabbitMq_1"},{"content":"官方文档: https://seata.io/zh-cn/docs/overview/what-is-seata.html\n1. AT 模式 1. AT 一阶段提交 第一阶段包括提交业务数据和回滚日志（undoLog）\n==全局事务的状态和操作决策的关键仍在 TC 中==\n@GlobalTransactional的方法通过AOP实现了，开启全局事务和提交全局事务两个操作，与Spring 事务机制类似，当 GlobalTransactionalInterceptor 在事务执行过程中捕获到Throwable时，会发起全局事务回滚\n0.1 步骤中会生成一个全局事务ID\n0.2 所有事务参与者执行结束后，一阶段事务提交\nundo log 结构 // 省略了相关方法 public class SQLUndoLog { // insert, update ... private SQLType sqlType; private String tableName; // 执行前 private TableRecords beforeImage; // 执行后 private TableRecords afterImage; } 2. AT 二阶段 二阶段是完全异步化的并且完全由Seata控制，Seata根据所有事务参与者的提交情况决定二阶段如何处理\n如果所有事务提交成功，则二阶段的任务就是删除一阶段生成 的undoLog，修改分支事务的状态, 并释放全局锁 如果部分事务参与者提交失败，则需要根据undoLog对已经注册的事务分支进行回滚，并释放全局锁 3. 流程思考 问题1. Seata如何做到==无侵入的分析业务SQL生成undoLog==，注册事务分支等操作？\n==Seata 代理了DataSource==，我们可以通过在代码注入一个DataSource来验证我的说法，目前的DataSource 是 io.seata.rm.datasource.DataSourceProxy\n问题2. ConnectionProxy 如何判断当前事务是全局事务，还是本地事务？\n==前线程是否绑定了全局事务id==，在进行全局事务之前，需要调用RootContext.bind(xid);\n问题3. 全局事务并发更新\n订单扣减库存的场景为例，如果TX1和TX2同时扣减product_id为1的库存，这时Seata会不会生成相同的beforeImage？\n举个例子，TX1读库存为100，TX1扣减库存1，此时BeforeImage为100 紧接着 如果TX2读库存也为100，那么就有问题了，不管TX2扣减多少库存，如果TX1回滚那么相当于覆盖了TX2扣减的库存，出现了脏写\nio.seata.rm.datasource.exec.AbstractDMLBaseExecutor::executeAutoCommitFalse\nbeforeImage()，这是一个抽象方法，看一下他的子类UpdateExecutor是如何实现的\n直接通过 select for update\n问题4. 全局事务外的更新\n我们现在可以确认在Seata的保证下，全局事务，不会造成数据的脏写，但是全局事务外会！\n什么意思呢？\n还以库存为例\n用户正在抢购，用户A完成了1阶段的库存扣减，这个时候库存为99。 此时库存管理员上线了，他查了一下库存为99。嗯\u0026hellip;太少了，我加100个，库存管理员把库存更新为200。 而此时seata给用户A生成beforeImage为100，如果此时用户A的全局事务失败了，发生了回滚，再次将库存更新为100\u0026hellip; 再次出现脏写 Seata 针对这个问题，提供了@GlobalLock注解，标记该注解时，会像全局事务一样进行SQL分析，竞争全局锁，就不会出现上述问题了\n问题5. @GlobalTransactional 和 @Transactional 同时使用会怎么样\n我们上文中已经说过了 @GlobalTransactional 的作用了，他是负责开启全局事务/提交事务1阶段，说白了@GlobalTransactional 只和Seata-server 交互，而 @Transactional 管理的是本地数据库的事务，所以二者不发生冲突。\n但是需要注意 @GlobalTransactional AOP 覆盖范围一定要大于 @Transactional\n事务分组说明。 1.事务分组是什么？ 事务分组是seata的资源逻辑，类似于服务实例。在file.conf中的my_test_tx_group就是一个事务分组。 2.通过事务分组如何找到后端集群？ 首先程序中配置了事务分组（GlobalTransactionScanner 构造方法的txServiceGroup参数），程序会通过用户配置的配置中心去寻找service.vgroup_mapping.事务分组配置项，取得配置项的值就是TC集群的名称。拿到集群名称程序通过一定的前后缀+集群名称去构造服务名，各配置中心的服务名实现不同。拿到服务名去相应的注册中心去拉取相应服务名的服务列表，获得后端真实的TC服务列表。 3.为什么这么设计，不直接取服务名？ 这里多了一层获取事务分组到映射集群的配置。这样设计后，事务分组可以作为资源的逻辑隔离单位，当发生故障时可以快速failover。 4. 注意事项 1. AT 模式注意事项 必须使用代理数据源，有 3 种形式可以代理数据源： 依赖 seata-spring-boot-starter 时，自动代理数据源，无需额外处理。 依赖 seata-all 时，使用 @EnableAutoDataSourceProxy (since 1.1.0) 注解，注解参数可选择 jdk 代理或者 cglib 代理。 依赖 seata-all 时，也可以手动使用 DatasourceProxy 来包装 DataSource。 配置 GlobalTransactionScanner，使用 seata-all 时需要手动配置，使用 seata-spring-boot-starter 时无需额外处理。 业务表中必须包含单列主键，若存在复合主键，请参考问题 13 。 每个业务库中必须包含 undo_log 表，若与分库分表组件联用，分库不分表。 跨微服务链路的事务需要对相应 RPC 框架支持，目前 seata-all 中已经支持：Apache Dubbo、Alibaba Dubbo、sofa-RPC、Motan、gRpc、httpClient，对于 Spring Cloud 的支持，请大家引用 spring-cloud-alibaba-seata。其他自研框架、异步模型、消息消费事务模型请结合 API 自行支持。 目前AT模式支持的数据库有：MySQL、Oracle、PostgreSQL和 TiDB。 使用注解开启分布式事务时，若默认服务 provider 端加入 consumer 端的事务，provider 可不标注注解。但是，provider 同样需要相应的依赖和配置，仅可省略注解。 使用注解开启分布式事务时，若要求事务回滚，必须将异常抛出到事务的发起方，被事务发起方的 @GlobalTransactional 注解感知到。provide 直接抛出异常 或 定义错误码由 consumer 判断再抛出异常。 2. 事务上下文 ​\tSeata 的事务上下文由 RootContetxt 来管理\n​\t1. 应用开启一个全局事务后，RootContext 会自动绑定该事务的 XID，事务结束（提交或回滚完成），RootContext 会自动解绑 XID。\n// 绑定 XID RootContext.bind(xid); // 解绑 XID String xid = RootContext.unbind(); 应用可以通过 RootContext 的 API 接口来获取当前运行时的全局事务 XID // 获取 XID String xid = RootContext.getXID(); 应用是否运行在一个全局事务的上下文中，就是通过 RootContext 是否绑定 XID 来判定的 public static boolean inGlobalTransaction() { return CONTEXT_HOLDER.get(KEY_XID) != null; } 1. 事务传播 ​\tSeata 全局事务的传播机制就是指==事务上下文的传播，根本上，就是 XID 的应用运行时的传播方式==。\n1. 服务内部的事务传播 默认的，RootContext 的实现是基于 ==ThreadLocal 的，即 XID 绑定在当前线程上下文中==。\npublic class ThreadLocalContextCore implements ContextCore { private ThreadLocal\u0026lt;Map\u0026lt;String, String\u0026gt;\u0026gt; threadLocal = new ThreadLocal\u0026lt;Map\u0026lt;String, String\u0026gt;\u0026gt;() { @Override protected Map\u0026lt;String, String\u0026gt; initialValue() { return new HashMap\u0026lt;String, String\u0026gt;(); } }; @Override public String put(String key, String value) { return threadLocal.get().put(key, value); } @Override public String get(String key) { return threadLocal.get().get(key); } @Override public String remove(String key) { return threadLocal.get().remove(key); } } 所以服务内部的 XID 传播通常是天然的通过同一个线程的调用链路串连起来的。默认不做任何处理，事务的上下文就是传播下去的。\n如果希望挂起事务上下文，则需要通过 RootContext 提供的 API 来实现：\n// 挂起（暂停） String xid = RootContext.unbind(); // TODO: 运行在全局事务外的业务逻辑 // 恢复全局事务上下文 RootContext.bind(xid); 2. 跨服务调用的事务传播 跨服务调用场景下的事务传播，本质上就是要把 XID 通过==服务调用传递到服务提供方==，并绑定到 RootContext 中去\n对Dubbo 的支持的机制的解读\n/** * The type Transaction propagation filter. */ @Activate(group = { Constants.PROVIDER, Constants.CONSUMER }, order = 100) public class TransactionPropagationFilter implements Filter { private static final Logger LOGGER = LoggerFactory.getLogger(TransactionPropagationFilter.class); @Override public Result invoke(Invoker\u0026lt;?\u0026gt; invoker, Invocation invocation) throws RpcException { String xid = RootContext.getXID(); // 获取当前事务 XID String rpcXid = RpcContext.getContext().getAttachment(RootContext.KEY_XID); // 获取 RPC 调用传递过来的 XID if (LOGGER.isDebugEnabled()) { LOGGER.debug(\u0026quot;xid in RootContext[\u0026quot; + xid + \u0026quot;] xid in RpcContext[\u0026quot; + rpcXid + \u0026quot;]\u0026quot;); } boolean bind = false; if (xid != null) { // Consumer：把 XID 置入 RPC 的 attachment 中 RpcContext.getContext().setAttachment(RootContext.KEY_XID, xid); } else { if (rpcXid != null) { // Provider：把 RPC 调用传递来的 XID 绑定到当前运行时 RootContext.bind(rpcXid); bind = true; if (LOGGER.isDebugEnabled()) { LOGGER.debug(\u0026quot;bind[\u0026quot; + rpcXid + \u0026quot;] to RootContext\u0026quot;); } } } try { return invoker.invoke(invocation); // 业务方法的调用 } finally { if (bind) { // Provider：调用完成后，对 XID 的清理 String unbindXid = RootContext.unbind(); if (LOGGER.isDebugEnabled()) { LOGGER.debug(\u0026quot;unbind[\u0026quot; + unbindXid + \u0026quot;] from RootContext\u0026quot;); } if (!rpcXid.equalsIgnoreCase(unbindXid)) { LOGGER.warn(\u0026quot;xid in change during RPC from \u0026quot; + rpcXid + \u0026quot; to \u0026quot; + unbindXid); if (unbindXid != null) { // 调用过程有新的事务上下文开启，则不能清除 RootContext.bind(unbindXid); LOGGER.warn(\u0026quot;bind [\u0026quot; + unbindXid + \u0026quot;] back to RootContext\u0026quot;); } } } } } } 3. TCC 模式 Try（尝试执行）、Confirm（确认执行）、Cancel（取消执行）。它要求业务服务提供者定义这些阶段的具体实现。\nTry 阶段：进行业务操作的预留资源操作。例如，扣款、锁库存等操作。\nConfirm 阶段：在所有服务都成功执行 Try 后，正式执行操作（如，扣款成功并提交）。\nCancel 阶段：如果任何服务失败，回滚所有已做的操作（如，取消扣款、释放库存等）。\n4. SAGA 长事务模式 SAGA 模式是一种**基于补偿机制**的分布式事务模型，通过将**长事务分解为多个子事务（也叫局部事务）**，每个局部事务都可以独立提交。每当一个子事务失败时，SAGA 会触发补偿事务，回滚之前的事务，从而实现最终一致性。 事务模式 适用场景 优缺点 AT 模式 简单、易用，适合无需精细控制的场景，但不适合多数据库 TCC 模式 适用于需要精确控制的复杂业务流程 高度灵活，但需要开发者实现 Try、Confirm、Cancel 方法 SAGA 模式 长时间的业务流程、需要逐步执行的事务 适合长事务，支持补偿，但实现复杂，性能相对较低 XA 模式 跨数据库、分布式消息队列等全局事务场景 保证全局一致性，但实现复杂、性能较低 5. XA XA模式使用两阶段提交来保证所有资源同时提交或回滚任何特定的事务。第一阶段，事务协调者通知每个事务参与者执行本地事务，本地事务执行完成后报告事务执行状态给事务协调者，此时事务不提交，继续持有数据库锁。第二阶段，根据第一阶段的执行结果而决定，==如果一阶段都成功，则通知所有事务参与者提交事务；如果一阶段任意一个参与者失败，则通知所有事务参与者回滚事务==\nTM: 事务管理器\nRM: 资源管理器\n6. 二阶段提交和三阶段提交 特性 两阶段提交（2PC） 三阶段提交（3PC） 事务一致性 提供强一致性 提供强一致性 容错性 如果协调器宕机或网络分区，可能造成死锁 增加超时回滚机制，降低死锁风险 性能开销 较低（两轮通信） 较高（三轮通信） 资源锁定 一阶段锁定资源直到二阶段完成 仅在预提交阶段锁定资源 ==二阶段提交流程==\n第一阶段（Prepare 阶段）：\n事务管理器（TM）向所有参与的资源管理器（RM）发送 Prepare 请求，询问它们是否可以提交事务。 参与者（RM）会执行事务操作但不提交，并将操作的锁定资源、事务状态等持久化。 如果所有参与者都返回成功，第一阶段完成。 第二阶段（Commit 或 Rollback 阶段）：\n如果第一阶段所有参与者都成功，TM 向所有 RM 发送 Commit 请求，要求它们正式提交事务。 如果任何参与者在第一阶段失败，TM 会发送 Rollback 请求，要求所有参与者回滚已执行的事务操作。 ==三阶段提交流程==\nCanCommit（询问阶段）：\n协调者询问所有参与者是否可以提交事务，参与者仅反馈“是”或“否”，并不锁定资源。 PreCommit（预提交阶段）：\n如果所有参与者答复“是”，协调者发送“预提交”请求，==参与者锁定资源并将事务状态设置为“准备提交”==，但尚未正式提交。 如果任何参与者答复“否”，协调者发送“回滚”指令，所有参与者释放资源。 DoCommit（正式提交阶段）：\n协调者在预提交阶段成功后，发送“提交”指令，所有参与者正式完成事务。 如果协调者未能发送指令，参与者会超时自动回滚（进一步提高容错能力）。 Seata 简介\n2019 年 1 月，阿里巴巴中间件团队发起了开源项目 Fescar（Fast \u0026amp; Easy Commit And Rollback），和社区一起共建开源分布式事务解决方案。Fescar 的愿景是让分布式事务的使用像本地事务的使用一样，简单和高效，并逐步解决开发者们遇到的分布式事务方面的所有难题。\nFescar 开源后，蚂蚁金服加入 Fescar 社区参与共建，并在 Fescar 0.4.0 版本中贡献了 TCC 模式。\n为了打造更中立、更开放、生态更加丰富的分布式事务开源社区，经过社区核心成员的投票，大家决定对 Fescar 进行品牌升级，并更名为 Seata，意为：Simple Extensible Autonomous Transaction Architecture，是一套一站式分布式事务解决方案。\nSeata 融合了阿里巴巴和蚂蚁金服在分布式事务技术上的积累，并沉淀了新零售、云计算和新金融等场景下丰富的实践经验。\n核心组件：\nTransaction Coordinator (TC)： 事务协调器，维护全局事务的运行状态，负责协调并驱动全局事务的提交或回滚。 Transaction Manager (TM)： 控制全局事务的边界，负责开启一个全局事务，并最终发起全局提交或全局回滚的决议。 Resource Manager (RM)： 控制分支事务，负责分支注册、状态汇报，并接收事务协调器的指令，驱动分支（本地）事务的提交和回滚。 工作流程：\nTM 向 TC 申请开启一个全局事务，全局事务创建成功并生成一个全局唯一的事务ID（XID），XID 在微服务调用链路的上下文中传播。 RM 向 TC 注册分支事务，接着执行这个分支事务并提交事务（==重点：RM在此阶段就已经执行了本地事务的提交/回滚==），最后将执行结果汇报给TC。 TM 根据 TC 中所有的分支事务的执行情况，发起全局提交或回滚决议。 TC 调度 XID 下管辖的全部分支事务完成提交或回滚请求。 2.6.2 Seata支持的模式 seata中有两种常见分布式事务实现方案，AT及TCC\nAT模式：赖于RM拥有本地数据库事务的能力，对于客户业务无侵入性\nTCC 模式\n2.6.3 Seata的优点 对业务无侵入：即减少技术架构上的微服务化所带来的分布式事务问题对业务的侵入 高性能：减少分布式事务解决方案所带来的性能消耗(2PC)\n2.6.4 AT模式 Seata AT模式是基于XA事务演进而来的一个分布式事务中间件，XA是一个基于数据库实现的分布式事务协议，本质上和两阶段提交一样，需要数据库支持，Mysql5.6以上版本支持XA协议，其他数据库如Oracle，DB2也实现了XA接口。\nAT模式分为两个阶段，如下：\n第一阶段：本地数据备份阶段 Seata 的 JDBC 数据源代理通过对业务 SQL 的解析，把==业务数据在变化前后的数据镜像组织成回滚日志==（XID/分支事务ID（Branch ID/变化前的数据/变化后的数据）。 将回滚日志存入一张日志表UNDO_LOG（==需要手动创建==）,并对UNDO_LOG表中的这条数据形成行锁（for update）。 若锁定失败，说明有其他事务在操作这条数据，它会在一段时间内重试，重试失败则回滚本地事务，并向TC汇报本地事务执行失败。 这样，可以保证：任何提交的业务数据的更新一定有相应的回滚日志存在\n目的：\n基于这样的机制，分支的本地事务便可以在全局事务的第一阶段提交，并马上释放本地事务锁定的资源。 有了回滚日志之后，可以在第一阶段释放对资源的锁定，降低了锁范围，提高效率，即使第二阶段发生异常需要回滚，只需找对undolog中对应数据并反解析成sql来达到回滚目的。 Seata通过代理数据源（DataSource-\u0026gt;DataSourceProxy）将业务sql的执行解析成undolog来与业务数据的更新同时入库，达到了对业务无侵入的效果 第二阶段：全局事务提交/回滚\n全局提交： 所有分支事务此时已经完成提交，所有分支事务提交都正常。 ==TM从TC获知后会决议执行====全局提交====，TC异步通知所有的RM释放UNDO_LOG表中的行锁==，同时清理掉UNDO_LOG表中刚才释放锁的那条数据。 全局回滚： 若任何一个RM一阶段事务提交失败，通知TC提交失败。 ==TM从TC获知后会决议执行全局回滚====，====TC向所有的RM发送回滚请求==。 RM通过XID和Branch ID找到相应的回滚日志记录，通过回滚记录生成反向的更新 SQL 并执行，以完成分支的回滚，同时释放锁，清除UNDO_LOG表中释放锁的那条数据。 2.6.5 TCC模式 seata也针对TCC做了适配兼容，支持TCC事务方案，原理前面已经介绍过，基本思路就是使用侵入业务上的补偿及事务管理器的协调来达到全局事务的一起提交及回滚。\n2.7 跨mysql ip测试 插入数据库\nundo_log 表\n发生异常\u0026ndash;回滚\n数据库可以看到之前被修改的数据被回滚了\n","date":"2023-05-14T15:03:55+08:00","permalink":"https://mikeLing-qx.github.io/p/seata/","title":"Seata"},{"content":" 参考资料:\nhttps://baijiahao.baidu.com/s?id=1683139600979891676\u0026wfr=spider\u0026for=pc https://juejin.cn/post/6844903655477346317 LoadBalance\n# https://github.com/Linyuzai/concept/wiki/Concept-WebSocket-LoadBalance \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.github.linyuzai\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;concept-websocket-loadbalance-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0.4\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 1. WebSocket 的功能 1）通知功能：\n保持一个长连接，当服务端有新的消息，能够实时的推送到使用方。像知乎的点赞通知、评论等，都可以使用WebSocket通信。\n某些使用H5的客户端，为了简化开发，也会使用WebSocket进行消息的通知，由于它是实时推送的，会有更好的用户体验。\n2）数据收集：\n一些次优级别的数据，比如行为日志、trace、异常执栈收集等，都可以开辟专门的WebSocket通道进行传输。这能够增加信息的集中度，并能及时的针对用户的行为进行合适的配置推送。由于大多数浏览器内核都支持，它将使客户端APM编程模型变得简单。\n3）加密 \u0026amp;\u0026amp; 认证：\n虽然使用Fiddler、Charles等能够抓到很多WebSocket包。但如果同时开启SSL，传输加密后的二进制数据，会大幅增加破解的成本，会安全的多。\n4）反向控制钩子：\n这个\u0026hellip;由于是双工长连接，==服务端完全可以推送一些钩子命令==，甚至直接是==代码，在客户端进行执行==。比如截个屏，录个音，种个小马。用户只要通过了授权申请，剩下的就随你发挥了。\n所以就一个梗：支付宝偷偷调用你的相机给你拍照\n2. WebSocket协议 2.1 http 单工： 数据传输只允许在一个方向上的传输，只能一方来发送数据，另一方来接收数据并发送。例如：对讲机 半双工：数据传输允许两个方向上的传输，但是同一时间内，只可以有一方发送或接受消息。例如：打电话 全双工：同时可进行双向传输。例如：websocket HTTP是单工的还是双工的还是半双工的 1.http1.0：单工。因为是短连接，客户端发起请求之后，服务端处理完请求并收到客户端的响应后即断开连接。\n2.http1.1：半双工。默认开启长连接keep-alive，开启一个连接可发送多个请求。\n3.http2.0：全双工，允许服务端主动向客户端发送数据。\n2.2 TCP三次握手四次挥手 2.3 WebSocket 和http 的关系 都是==应用层==\nTcp ==是传输层协议==; ==http协议是在Tcp协议之上建立的==，http在发起请求时通过tcp协议建立起连接服务器的，请求结束后，==立即断开tcp连接==\nHttp是无状态的短连接，而TCP是有状态的长连接\nSocket本身并不是协议，而是一个调用接口(API)。Socket的出现只是使得程序员更方便地使用TCP/IP协议栈而已，是对TCP/IP协议的抽象，从而形成了我们知道的一些最基本的函数接口\nHTTP是轿车，提供了封装或者显示数据的具体形式;Socket是发动机，提供了网络通信的能力。\nWebSocket借助http协议进行握手，三次握手成功后，就会变身为TCP通道，从此与http不再相见。\nWebSocket是一种在==单个TCP==连接上进行==全双工通信的协议==。在WebSocket API中，浏览器和服务器只需要完成一次握手（不是指建立TCP连接的那个三次握手，==是指在建立TCP连接后传输一次握手数据==），两者之间就直接可以创建持久性的连接，并进行==双向数据传输==。\nWebSocket：WebSocket协议只能浏览器发起么？\n不是。目前此协议的受众的也不仅仅是web开发者。\nWebSocket只是一种协议，它和http协议一样，使用类似==okhttp的组件==，可以在任何地方进行调用，甚至可以借助WebSocket实现RPC框架。\nWebSocket：WebSocket和长轮询有什么区别？\n​\t长轮询就是客户端发送一个请求，服务端将一直在这个连接上等待（当然有一个超长的超时时间），直到有数据才返回，它依然是一个一问一答的模式。比如著名的comted。==WebSocket在握手成功后，就是全双工的TCP通道，数据可以主动从服务端发送到客户端，处于链接两端的应用没有任何区别==。WebSocket创建的连接和Http的长连接是不一样的。由于==Http长连接底层依然是Http议，所以它还是一问一答==，只是Hold住了一条命长点的连接而已。长轮询和Http长连接是阻塞的I/O，但WebSocket可以是==非阻塞的（具体是多路复用）==\n2.4 如何创建一个webSocket 连接 WebSocket的连接创建是==借助Http协议进行的==。这样设计主要是考虑兼容性，在浏览器中就可以很方便的发起请求，看起来比较具有迷惑性。\n下图是一个典型的由浏览器发起的ws请求，可以看到和http请求长的是非常相似的。\n但是，它只是请求阶段长得像而已\n请求的地址: 一般是：ws://***，或者是使用了SSL/TLS加密的安全协议wss:，用来标识是WebSocket请求。\n1）首先，通过Http头里面的Upgrade域，请求进行协议转换。如果服务端支持的话，就可以切换到WebSocket协议。简单点讲：连接已经在那了，通过握手切换成ws协议，就是切换了连接的一个状态而已。\n2）Connection域可以认为是与Upgrade域配对的头信息。像nginx等代理服务器，是要先处理Connection，然后再发起协议转换的。\n3）Sec-WebSocket-Key 是随机的字符串，服务器端会用这些数据来构造出一个 SHA-1 的信息摘要。如此操作，可以尽量避免普通 HTTP 请求被误认为 WebSocket 协议。\n其他的，像Sec-WebSocket*字样的头信息，表明了客户端支持的子协议以及其他信息。像loT中很流行的mqtt，就可以作为WebSocket的子协议。\n3. stomp ​\tSTOMP是一个用于C/S之间进行异步消息传输的简单文本协议, 全称是Simple Text Oriented Messaging Protocol; 它其实是==消息队列的一种协议, 和AMQP,JMS是平级的==\n​\tSTOMP是一种基于帧的协议，一帧由一个命令，一组可选的Header和一个可选的Body组成。 STOMP是基于Text的，但也允许传输二进制数据。 它的默认编码是UTF-8，但它的消息体也支持其他编码方式，比如压缩编码。\n==stomp帧结构==\nCOMMAND header1:value1 header2:value2\nBody^@\n^@表示行结束符\n一个STOMP帧由三部分组成:命令，Header(头信息)，Body（消息体）\n命令使用UTF-8编码格式，命令有SEND、SUBSCRIBE、MESSAGE、CONNECT、CONNECTED等。 Header也使用UTF-8编码格式，它类似HTTP的Header，有content-length,content-type等。 Body可以是二进制也可以是文本。注意Body与Header间通过一个空行（EOL）来分隔。 例子\nSEND destination:/broker/roomId/1 content-length:57\n{“type\u0026quot;:\u0026ldquo;ENTER\u0026rdquo;,\u0026ldquo;content\u0026rdquo;:\u0026ldquo;o7jD64gNifq-wq-C13Q5CRisJx5E\u0026rdquo;}\n第1行：表明此帧为SEND帧，是COMMAND字段。 第2行：Header字段，消息要发送的目的地址，是相对地址。 第3行：Header字段，消息体字符长度。 第4行：空行，间隔Header与Body。 第5行：消息体，为自定义的JSON结构 2. stomp 服务端 ​\tSTOMP服务端被设计为客户端可以向其发送消息的一组目标地址。STOMP协议并没有规定目标地址的格式，它由使用协议的应用自己来定义。 例如/topic/a，/queue/a，queue-a对于STOMP协议来说都是正确的。应用可以自己规定不同的格式以此来表明不同格式代表的含义。比如应用自己可以定义以/topic打头的为发布订阅模式，消息会被所有消费者客户端收到，以/user开头的为点对点模式，只会被一个消费者客户端收到。\n3. stomp 客户端 ​\n对于STOMP协议来说, 客户端会扮演下列两种角色的任意一种：\n作为生产者，通过SEND帧发送消息到指定的地址 作为消费者，通过发送SUBSCRIBE帧到已知地址来进行消息订阅，而当生产者发送消息到这个订阅地址后，订阅该地址的其他消费者会受到通过MESSAGE帧收到该消息 实际上，WebSocket结合STOMP相当于构建了一个消息分发队列，客户端可以在上述两个角色间转换，订阅机制保证了一个客户端消息可以通过服务器广播到多个其他客户端，作为生产者，又可以通过服务器来发送点对点消息。\n4. spring websocket ==架构图==\n==各个组件介绍==：\n生产者型客户端（左上组件）: 发送SEND命令到某个目的地址(destination)的客户端。 消费者型客户端（左下组件）: 订阅某个目的地址(destination), 并接收此目的地址所推送过来的消息的客户端。 request channel: 一组用来接收生产者型客户端所推送过来的消息的线程池。 response channel: 一组用来推送消息给消费者型客户端的线程池。 broker: 消息队列管理者，也可以成为消息代理。它有自己的地址（例如“/topic”），客户端可以向其发送订阅指令，它会记录哪些订阅了这个目的地址(destination)。 应用目的地址(图中的”/app”): 发送到这类目的地址的消息在到达broker之前，会先路由到由应用写的某个方法。相当于对进入broker的消息进行一次拦截，目的是针对消息做一些业务处理。 非应用目的地址(图中的”/topic”，也是消息代理地址): 发送到这类目的地址的消息会直接转到broker。不会被应用拦截。 SimpAnnotatonMethod: 发送到应用目的地址的消息在到达broker之前, 先路由到的方法. 这部分代码是由应用控制的。 ==流转流程==\n生产者通过发送一条SEND命令消息到某个目的地址(destination)， 服务端request channel接受到这条SEND命令消息， 如果目的地址是应用目的地址则转到相应的由应用自己写的业务方法做处理（对应图中的SimpAnnotationMethod），再转到broker(SimpleBroker)。 如果目的地址是非应用目的地址则直接转到broker。 broker通过SEND命令消息来构建MESSAGE命令消息, 再通过response channel推送MESSAGE命令消息到所有订阅此目的地址的消费者 1. @SendTo ==它可以是方法的返回值 推送到消息代理器中, 由消息代理器广播到订阅路径中去==\n@MessageMapping(\u0026quot;/hello\u0026quot;) //使用MessageMapping注解来标识所有发送到“/hello”这个destination的消息，都会被路由到这个方法进行处理. @SendTo(\u0026quot;/topic/greetings\u0026quot;) //使用SendTo注解来标识这个方法返回的结果，都会被发送到它指定的destination，“/topic/greetings”. //传入的参数Message为客户端发送过来的消息，是自动绑定的。 public Greeting greeting(HelloMessage message) throws Exception { Thread.sleep(1000); // 模拟处理延时 return new Greeting(\u0026quot;Hello, \u0026quot; + HtmlUtils.htmlEscape(message.getName()) + \u0026quot;!\u0026quot;); //根据传入的信息，返回一个欢迎消息. } } ​\t上面方法中的==返回值，会被广播到/topic/greetings这个订阅路径中==，只要客户端订阅了这个路径，都会接收到消息。Spring处理消息的主要类是==SimpleBrokerMessageHandler==, 当需要发送广播消息时，最终会调用其中的==sendMessageToSubscribers()==方法：方法内部会循环调用当前所有订阅此Broker的客户端Session，然后逐个发送消息。这里，入参destination就是Broker的地址，而message，就是我们返回信息的封装\n2. @SendToUser @MessageMapping(\u0026quot;/hello\u0026quot;) //使用MessageMapping注解来标识所有发送到“/hello”这个destination的消息，都会被路由到这个方法进行处理. @SendToUser(\u0026quot;/topic/greetings\u0026quot;) //使用SendToUser注解来标识这个方法返回的结果，都会被发送到请求它的用户的destination. //传入的参数Message为客户端发送过来的消息，是自动绑定的。 public Greeting greeting(HelloMessage message) throws Exception { Thread.sleep(1000); // 模拟处理延时 return new Greeting(\u0026quot;Hello, \u0026quot; + HtmlUtils.htmlEscape(message.getName()) + \u0026quot;!\u0026quot;); //根据传入的信息，返回一个欢迎消息. } } 上面这段代码, return 之后会走到 AbstractMethodMessageHandler.java中的handleMatch方法\nprotected void handleMatch(T mapping, HandlerMethod handlerMethod, String lookupDestination, Message\u0026lt;?\u0026gt; message) { .... Object returnValue = invocable.invoke(message, new Object[0]); // 这里调用MessageMapping匹配到的方法 ..... // 开始处理返回值 this.returnValueHandlers.handleReturnValue(returnValue, returnType, message); ..... } ==SendToMethodReturnValueHandler==\n专门处理SendTo相关注解的类。当用SendTo注解的方法返回后，即调用此类中的handleReturnValue方法来进行处理\n3. SimpMessagingTemplate public void convertAndSendToUser(String user, String destination, Object payload, @Nullable Map\u0026lt;String, Object\u0026gt; headers, @Nullable MessagePostProcessor postProcessor) throws MessagingException { Assert.notNull(user, \u0026quot;User must not be null\u0026quot;); user = StringUtils.replace(user, \u0026quot;/\u0026quot;, \u0026quot;%2F\u0026quot;); destination = destination.startsWith(\u0026quot;/\u0026quot;) ? destination : \u0026quot;/\u0026quot; + destination; super.convertAndSend(this.destinationPrefix + user + destination, payload, headers, postProcessor); } user:用户标识，这里就是客户端与服务端链接的sessionId destination: 这是SendToUser注解后括号内的参数值 payload:Object类型，它标识Controller中定义的方法的返回值，这里就是GreetingController类中greeting方法的返回值 headers:返回信息的消息头 postProcessor:此处为Null\\\nSimpMessagingTemplate 是 Spring 框架中提供的一个用于在 WebSocket 中发送消息的工具类。它可以用于在服务器端向订阅了特定主题的客户端发送消息，实现了在 WebSocket 中进行消息的发布（Publish）操作。\nSimpMessagingTemplate 提供了多种方法来发送消息，包括：\nconvertAndSend(String destination, Object payload): 向指定的目的地发送消息。destination 参数表示消息的目的地，可以是一个主题（topic）或一个队列（queue），payload 参数表示要发送的消息体。 convertAndSendToUser(String user, String destination, Object payload): 向指定用户发送消息。user 参数表示用户的唯一标识，destination 参数表示消息的目的地，payload 参数表示要发送的消息体。 convertAndSendToUser(String user, String destination, Object payload, MessageHeaders headers): 向指定用户发送消息，并可以设置消息头（headers）。 convertAndSendToUser(String user, String destination, Object payload, Map\u0026lt;String, Object\u0026gt; headers): 向指定用户发送消息，并可以设置消息头（headers）。 SimpMessagingTemplate 还支持一些其他的方法，例如发送带有自定义消息头（headers）的消息，发送带有消息头和消息体（payload）的消息等。\n使用 SimpMessagingTemplate 可以方便地在服务器端向 WebSocket 客户端发送消息，实现了在 WebSocket 中进行消息的发布操作。需要注意的是，使用 SimpMessagingTemplate 发送的消息只会发送给订阅了相应主题或用户的客户端，因此可以实现点对点或广播式的消息发送。在使用 SimpMessagingTemplate 时，需要确保已经正确配置了 WebSocket 相关的配置，并且已经设置了相应的消息代理（Message Broker）来实现消息的传递。\n@RestController public class DemoController { @MessageMapping(\u0026quot;/chat\u0026quot;) @SendTo(\u0026quot;/topic/chat\u0026quot;) public String chat(String message) throws Exception { Thread.sleep(1000); // simulated delay return message + \u0026quot;mike ling\u0026quot;; } } @Bean public Executor websocketExecutor() { ThreadFactory namedThreadFactory = new ThreadFactoryBuilder() .setNameFormat(\u0026quot;websocket-pool-%d\u0026quot;).build(); return new ThreadPoolExecutor(5, 200, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy()); } @Bean public ExecutorSubscribableChannel messageChannel(Executor websocketExecutor) { return new ExecutorSubscribableChannel(websocketExecutor); } @Bean public SimpMessagingTemplate messagingTemplate(ExecutorSubscribableChannel messageChannel) { return new SimpMessagingTemplate(messageChannel); } ","date":"2023-04-10T15:48:18+08:00","permalink":"https://mikeLing-qx.github.io/p/websocket/","title":"Websocket"},{"content":"1 docker 1.1 简介 Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的Linux机器上，也可以实现虚拟化，容器是完全使用沙箱机制，相互之间不会有任何接口。 ==可以把它当成轻量级的虚拟机==吧（虽然一个是完全虚拟化，一个是操作系统层虚拟化）\n1.2 构成 ​\tDocker组成\n仓库 (Repository)\n镜像 (Image) 阿里云镜像: http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n容器 (container)\n2 docker入门 1.1 目录挂载 ==将宿主机中的目录与容器内的目录进行映射==\n创建容器 添加-v参数 后边为 宿主机目录:容器目录\ndocker run -id --name=c3 -v /opt/:/usr/local/myhtml centos 如果你共享的是多级的目录，可能会出现权限不足的提示。\n这是因为CentOS7中的安全模块selinux把权限禁掉了，我们需要添加参数 \u0026ndash;privileged=true 来解决挂载的目录没有权限的问题\ndocker run -id --privileged=true --name=c4 -v /opt/:/usr/local/myhtml centos 创建文件 touch a.txt 或者vim 查看文件 cat a.txt 输入内容 echo 123 \u0026gt; a.txt 1.2 复制文件 docker cp 需要拷贝的文件或目录 容器名称:容器目录 例如：docker cp 1.txt c2:/root 1.3 部署应用 创建mysql容器\ndocker run -di --name mysql -p 33306:3306 -e MYSQL_ROOT_PASSWORD=123456 mysql:5.7 -p 代表==端口映射==，格式为 宿主机映射端口:容器运行端口\n-e 代表添加环境变量 MYSQL_ROOT_PASSWORD是root用户的登陆密码\n2.1 环境准备 准备vmware 和 Centos7以上版本; linux内核版本推荐 3.10 查看linux内核版本\nuname -a 查看linux版本\ncat /etc/redhat-release 2.2 安装docker 更新yum包 (只是为了排除系统环境影响; ==不要用于生产环境; 仅限在本地玩==) yum update 安装需要的软件包 yum install -y yum-utils device-mapper-persistent-data lvm2 设置yum 源 (docker 仓库地址) yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 查看仓库所有docker版本 yum list docker-ce --showduplicates | sort -r 安装所需的版本 yum install docker-ce-版本号 例如: yum install docker-ce-17.12.1.ce 启动docker 并加入开机启动 systemctl start docker systemctl enable docker 验证是否安装成功 docker version 2.3 docker 安装 mysql 查看可用的mysql版本 docker search mysql 拉取mysql镜像 (默认拉取的是最新版本latest) docker pull mysql 运行mysql容器 docker run -p 3306:3306 --name mysql-test -v /opt/docker_v/mysql/conf:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=root -d imageID 参数说明\n-i 运行 ; -d 创建\n-p 端口映射; 映射容器服务的3306端口到宿主机的3306端口上; 外部主机可以直接通过 宿主机ip:3306 访问到 MySQL 的服务\n-v /opt/docker_v/mysql/conf:/etc/mysql/conf.d：将主机/opt/docker_v/mysql/conf目录挂载到容器的/etc/mysql/conf.d\n-e (添加环境变量) MYSQL_ROOT_PASSWORD=root：==初始化root用户的密码==\n-d: 后台运行容器，并返回容器ID imageID: mysql镜像ID\n进入mysql 容器 docker exec -it mysql-test bash 查看mysql容器信息 docker inspect mysql-test 查看容器 docker ps (查看正在运行的容器) docker ps -a (查看所有的容器) 3 docker常用命令 docker start/stop id/name 启动/停止某个容器 docker attach id 进入某个容器(使用attach退出后容器也跟着停止运行) docker exec -it id 启动一个伪终端以交互式的方式进入某个容器（使用exit退出后容器不停止运行） ctrl + d 退出容器 docker rm id/name 删除某个容器 docker rmi id/name 删除某个镜像 docker run \u0026ndash;name test -ti ubuntu /bin/bash 复制ubuntu容器并且重命名为test且运行，然后以伪终端交互式方式进入容器，运行bash docker build -t soar/centos:7.1 . 通过当前目录下的Dockerfile创建一个名为soar/centos:7.1的镜像 docker run -d -p 2222:22 \u0026ndash;name test soar/centos:7.1 以镜像soar/centos:7.1创建名为test的容器，并以后台模式运行，并做端口映射到宿主机2222端口，P参数重启容器宿主机端口会发生改变 日志查看 docker logs {containerId} docker volume prune 查看日志 docker logs -f {} 推送镜像 docker 地址 用户名/镜像名:tag 私服可以忽略用户名 推送私服需要https, 使用http需要 配置ip+端口 清理未使用的容器 docker container prune 4 docker 安装 redis 配置镜像仓库 ustc vi /etc/docker/daemon.json (不存在就手动创建)\n{ \u0026quot;registry-mirrors\u0026quot;: [\u0026quot;https://docker.mirrors.ustc.edu.cn\u0026quot;] } 配置完成后需要重启docker 服务\nsystemctl restart docker.service systemctl enable docker 查找所需的版本\nhttps://hub.docker.com/_/redis?tab=tags\u0026page=1\u0026ordering=last_updated\n这里下载最新的\ndocker pull redis:latest docker 安装redis\ndocker run -itd --name redis-test -p 6379:6379 redis 5. docker 命令自动补全 sudo mkdir -p /etc/bash_completion.d/ sudo docker completion bash \u0026gt; /etc/bash_completion.d/docker # 激活 source /etc/bash_completion source /etc/bash_completion.d/docker 6. docker-compose 1. 智慧城市 [root@host-19-112-71-8 xiaopin]# cat docker-compose-prod.yml networks: zs_network: external: true version: '3.0' services: smartcites_prod: hostname: smartcites_prod container_name: smartcites_prod privileged: true ulimits: nproc: 65535 nofile: soft: 20000 hard: 40000 build: context: smartcity-admin dockerfile: Dockerfile restart: always ports: - 9980:9980 environment: - JAVA_OPTS=-Xmx512M -Xms512M volumes: - ./data/mountEnterpriseImportTemp.xlsx:/app/smartcity/uploadPath/download/template/mountEnterpriseImportTemp.xlsx - ./data/uploadPath:/app/smartcity/uploadPath - ./data/smartcity-logs:/app/smartcity/logs extra_hosts: - xtbg.digitalgd.com.cn:210.76.73.80 - xtbg.zfsg.gd.gov.cn:157.122.49.24 #- xtbg.gdzwfw.gov.cn:157.122.49.24 - xtbg.gdzwfw.gov.cn:19.15.0.77 - yzh.gdgov.cn:19.15.18.170 - yzh.zszwfw.cn:19.112.202.181 networks: zs_network: ipv4_address: 192.168.88.10 epidemicapi: hostname: epidemicapi_prod container_name: epidemicapi_prod privileged: true ulimits: nproc: 65535 nofile: soft: 20000 hard: 40000 build: context: epidemicapi dockerfile: Dockerfile restart: always #ports: # - 9983:9983 environment: - JAVA_OPTS=-Xmx512M -Xms512M networks: zs_network: ipv4_address: 192.168.88.11 这是一个 Docker Compose 配置文件，用于定义和启动多个 Docker 容器服务。以下是配置文件的详细介绍：\nversion: '3.0'\n指定了 Docker Compose 文件的版本。在这个版本下，你可以使用一些版本 3 的特性来配置服务。\nnetworks\n定义了一个名为 zs_network 的外部网络。external: true 表示该网络已经存在，不会由 Docker Compose 创建。\nservices\n定义了两个服务 smartcites_prod 和 epidemicapi。每个服务的配置如下：\nsmartcites_prod\nhostname: 设置容器的主机名为 smartcites_prod。\ncontainer_name: 设置容器的名称为 smartcites_prod。\nprivileged: 给容器提供特权模式，允许容器进行一些特权操作。\nulimits: 设置容器的资源限制，包括进程数（nproc）和文件句柄数（nofile）。\nbuild 定义构建容器的上下文和 Dockerfile 的路径。\ncontext: 构建上下文的路径为 smartcity-admin。 dockerfile: Dockerfile 的路径为 Dockerfile。 restart: 设置容器在崩溃后自动重启（always）。\nports: 映射主机的端口 9980 到容器的端口 9980。\nenvironment: 设置环境变量，这里定义了 Java 虚拟机的内存选项。\nvolumes 映射主机的目录或文件到容器中。\n./data/mountEnterpriseImportTemp.xlsx: 映射到容器的 /app/smartcity/uploadPath/download/template/mountEnterpriseImportTemp.xlsx。 ./data/uploadPath: 映射到容器的 /app/smartcity/uploadPath。 ./data/smartcity-logs: 映射到容器的 /app/smartcity/logs。 extra_hosts: 添加额外的主机名和 IP 地址映射。\nnetworks: 连接到 zs_network 网络，并分配静态 IP 地址 192.168.88.10。\nepidemicapi\nhostname: 设置容器的主机名为 epidemicapi_prod。\ncontainer_name: 设置容器的名称为 epidemicapi_prod。\nprivileged: 给容器提供特权模式。\nulimits: 设置容器的资源限制。\nbuild 定义构建容器的上下文和 Dockerfile 的路径。\ncontext: 构建上下文的路径为 epidemicapi。 dockerfile: Dockerfile 的路径为 Dockerfile。 restart: 设置容器在崩溃后自动重启（always）。\nenvironment: 设置 Java 虚拟机的内存选项。\nnetworks: 连接到 zs_network 网络，并分配静态 IP 地址 192.168.88.11。\n总结\n这个配置文件用于定义两个服务 smartcites_prod 和 epidemicapi，并将它们连接到一个名为 zs_network 的网络。它配置了容器的资源限制、自动重启策略、端口映射、环境变量和卷挂载等。\ndocker logs --since 2024-09-13T16:13:00 docker logs --since 2024-09-23T09:55:00 -f smartcites_prod \u0026gt; logs.txt 7. 日志 Docker 容器的日志是由容器内运行的应用程序和其底层操作系统产生的。当应用程序或系统组件将信息输出到标准输出（stdout）或标准错误（stderr）时，这些输出被捕获并存储为日志。Docker 提供了一种机制来捕获这些输出，并允许用户通过 Docker 命令行界面（CLI）或其他工具来访问这些日志。\n以下是 Docker 容器日志的来源和处理方式的详细说明：\n应用程序输出： 容器中的应用程序通常会向标准输出（stdout）和标准错误（stderr）打印信息。这些可以是应用程序日志、错误消息、状态更新等。 容器启动脚本或入口点： Docker 容器可以配置一个入口点（entrypoint）或启动脚本，这些脚本可能会产生日志信息。 操作系统： 容器内的操作系统也会生成日志，尤其是与系统服务和内核相关的日志。 Docker 守护进程： Docker 守护进程本身也会产生日志，记录容器的创建、启动、停止等事件。 日志驱动： Docker 使用所谓的“日志驱动”来控制日志的捕获、存储和传输。默认的日志驱动是 json-file，它将日志以 JSON 格式写入文件。其他日志驱动包括 syslog、journald、gelf 等，它们可以将日志发送到不同的目标。 日志文件： 默认情况下，Docker 将每个容器的日志文件存储在宿主机的 /var/lib/docker/containers// 目录下，文件名为 -json.log。这些文件包含了容器的日志数据。 实时日志查看： 用户可以使用 docker logs 命令来查看容器的日志。这个命令从 Docker 守护进程获取日志数据，而不是直接从文件系统读取日志文件。 日志管理： 除了使用 docker logs 命令，用户还可以配置日志轮转、远程日志存储或日志分析工具来管理和分析日志数据。 日志配置： 在 Docker 容器运行时，可以通过设置环境变量或使用 Docker Compose 文件中的日志配置选项来配置日志行为。 通过这些机制，Docker 确保容器的日志可以被有效地捕获、存储和检索，从而帮助用户监控和调试容器化应用程序。\n","date":"2023-02-23T15:43:32+08:00","permalink":"https://mikeLing-qx.github.io/p/docker/","title":"Docker"},{"content":"0. Spring cloud gateway 概述 Spring Cloud Gateway需要Spring Boot和Spring Webflux提供的Netty运行时。它不能在传统的Servlet容器中或作为WAR构建。\n1. 执行流程 1: Gateway的客户端回向Spring Cloud Gateway发起请求，请求首先会被 ==HttpWebHandlerAdapter进行提取组装成网关的上下文==，然后网关的上下文 会==传递到DispatcherHandler==。\n2: ==DispatcherHandler是所有请求的分发处理器==，DispatcherHandler主 要负责分发请求对应的处理器，比如将请求分发到对应\tRoutePredicateHandlerMapping(==路由断言处理器映射器==）。\n3: 路由断言处理映射器主要用于路由的查找，以及找到路由后返回对应的 FilteringWebHandler。\n4: ==FilteringWebHandler主要负责组装Filter链表==并调用Filter执行一系 列Filter处理，然后把请求转到后端对应的代理服务处理，处理完毕后，将 Response返回到Gateway客户端。\n在Filter链中，通过虚线分割Filter的原因是，==过滤器可以在转发请求之前 处理或者接收到被代理服务的返回结果之后处理==。==所有的Pre类型的Filter 执行完毕之后==，才会转发请求到被代理的服务处理。被代理的服务把所有请求 完毕之后，才会执行==Post类型的后置过滤器==\n2. Gateway 路由 Gateway路由配置分为基于配置的==静态路由设置==和==基于代码动态路由配 置==， 静态路由是指在application.yml中把路由信息配置好了，而动态路由则支 持在代码中动态加载路由信息，更加灵活，我们接下来把这2种路由操作都实现一次。\n1:用户所有请求以/order开始的请求，都路由到 hailtaxi-order服务 2:用户所有请求以/driver开始的请求，都路由到 hailtaxi-driver服务 3:用户所有请求以/pay开始的请求，都路由到 hailtaxi-pay服务 2.1 配置文件路由 routes:路由配置 - id:唯一标识符 uri:路由地址，可以是 lb://IP:端口 也可以是 lb://${spring.application.name} predicates:断言，是指路由条件 - Path=/driver/**:路由条件。Predicate 接受一个输入参数，返回一 个布尔值结果。这里表示匹配所有以driver开始的请求。 filters:过滤器 - StripPrefix=1:真实路由的时候，去掉第1个路径，路径个数以/分割区 分 2.2 代码路由 RouteLocator 我们同样实现上面的功能，但这里基于代码方式实现。所有路由规则我们 可以从==数据库中读取并加载到程序中==。基于代码的路由配置我们只需要创建 ==RouteLocator 并添加路由配置==即可，代码如下\n2.3 Gateway-predicate 路由断言 内置的predicate 实现\n2.3.1 Path 其实就是路径匹配方式\nid：我们自定义的路由 ID，保持唯一 uri：目标服务地址 predicates：路由条件，Predicate 接受一个输入参数，返回一个布尔值 结果。该属性包含多种默认方法来将 Predicate 组合成其他复杂的逻辑 （比如：与，或，非） 2.3.2 Cookie 通过对应的cookie name 和正则表达式去匹配\n接受两个参数\n​\tcookie name\n​\t正则表达式\ngateway: routes: - id: hailtaxi-driver uri: lb://hailtaxi-driver predicates: - Path=/driver/** - Cookie=username,itheima 2.3.3 Header 和Cookie匹配一样\ngateway: routes: - id: hailtaxi-driver uri: lb://hailtaxi-driver predicates: - Path=/driver/** - Header=token,^(?!\\d+$)[\\da-zA-Z]+$ 2.3.4 Method gateway: routes: - id: hailtaxi-driver uri: lb://hailtaxi-driver predicates: - Path=/driver/** - Method=GET,POST [{\u0026quot;argsMap\u0026quot;:{\u0026quot;regexp\u0026quot;:\u0026quot;\\\\d+\u0026quot;,\u0026quot;header\u0026quot;:\u0026quot;Authorization\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;Header\u0026quot;,\u0026quot;type\u0026quot;:1},{\u0026quot;argsMap\u0026quot;:{\u0026quot;regexp\u0026quot;:\u0026quot;12345\u0026quot;,\u0026quot;name\u0026quot;:\u0026quot;sessionId\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;Cookie\u0026quot;,\u0026quot;type\u0026quot;:1},{\u0026quot;argsMap\u0026quot;:{\u0026quot;key-resolver\u0026quot;:\u0026quot;#{@ipKeyResolver}\u0026quot;,\u0026quot;redis-rate-limiter.burstCapacity\u0026quot;:\u0026quot;1\u0026quot;,\u0026quot;redis-rate-limiter.replenishRate\u0026quot;:\u0026quot;1\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;RequestRateLimiter\u0026quot;,\u0026quot;type\u0026quot;:2} ] 2.4 自定义断言 需求: 转发带token的请求到hailtaxi-drvier 服务中，这里定义请求带 token是指包含某个请求头的请求，==至于是什么请求头可以由配置指定==\n一. 修改配置文件\ngateway: #路由配置 routes: #唯一标识符 - id: hailtaxi-driver uri: lb://hailtaxi-driver #路由断言 predicates: # 自定义一个Token断言,如果请求包含Authorization的 token信息则通过 - Token=Authorization 二. 创建RoutePredicateFactory\n==断言工厂默认命名规则必须按照\u0026quot;名称\u0026quot;+RoutePredicateFactory，如上 TokenRoutePredicateFactory的断言名称为Token==\n@Slf4j @Component public class TokenRoutePredicateFactory extends AbstractRoutePredicateFactory\u0026lt;TokenRoutePredicateFactory.Config\u0026gt; { public TokenRoutePredicateFactory() { super(Config.class); } public Predicate\u0026lt;ServerWebExchange\u0026gt; apply(Config config) { return exchange -\u0026gt; { // 打印配置文件参数值 String headerName = config.getHeaderName(); HttpHeaders headers = exchange.getRequest().getHeaders(); List\u0026lt;String\u0026gt; header = headers.get(headerName); log.info(\u0026quot;Token Predicate headers:{}\u0026quot;, header); // 断言返回的是boolean值 return header!=null \u0026amp;\u0026amp; header.size()\u0026gt;0; }; } @Override public List\u0026lt;String\u0026gt; shortcutFieldOrder() { // [\u0026quot;aaa\u0026quot;,\u0026quot;bbb\u0026quot;] Authorization,adfa return Arrays.asList(\u0026quot;headerName\u0026quot;,\u0026quot;aaa\u0026quot;); } @Override public ShortcutType shortcutType() { return ShortcutType.DEFAULT; } @Data public static class Config{ private String headerName; private String aaa; } } 3. Gateway 过滤器 3.1 简介 过滤器分类\n默认过滤器:出厂自带，实现好了拿来就用，不需要实现 全局默认过滤器 局部默认过滤器 自定义过滤器:根据需求自己实现，实现后需配置，然后才能用哦。 全局过滤器:作用在所有路由上。 局部过滤器:配置在具体路由下，只作用在当前路由上。 Spring Cloud Gateway根据作用范围划分为GatewayFilter和 GlobalFilter，二者区别如下： 1. GatewayFilter : ==需要通过spring.cloud.routes.filters 配置在具体路由 下，只作用在当前路由上== 或通过==spring.cloud.default-filters配置在全 局==，作用在所有路由上；gateway内置了多种过滤器工厂，配套的过滤 器可以直接使用\nGlobalFilter : 全局过滤器，不需要在配置文件中配置，作用在所有的路由上，最终通过GatewayFilterAdapter包装成GatewayFilterChain 可识别的过滤器，它为请求业务以及路由的URI转换为真实业务服务的 请求地址的核心过滤器，不需要配置，系统初始化时加载，并作用在每 ==全局过滤器功能: 请求鉴权, 服务调用时长统计, 修改请求 或相应header, 限流, 去除路径== 常见的过滤器\n3.2 默认过滤器的使用 1. 添加相应头 AddResponseHeaderGatewayFilterFactory 属于 ==GatewayFilter==\n对输出响应头设置属性，比如对输出的响应设置其头部属性名称为：XResponse- Default-MyName , 值为itheima\nspring: cloud: gateway: # 配置全局默认过滤器 作用在所有路由上，也可单独为某个路由配置 default-filters: # 往响应过滤器中加入信息 - AddResponseHeader=X-Response-Default- MyName,itheima 2. 前缀处理 1. 路由去除 ​\t我们很多时候需要统一API路径，比如统一以/api 开始的请求调用hailtaxi-driver 服务，但真实服务接口地址又没有/api 路径，我们可以使用Gateway的过滤器处理请求路径\ngateway: routes: - id: hailtaxi-driver uri: lb://hailtaxi-driver predicates: - Path=/api/driver/** filters: - StripPrefix=1 # 表示真实请求地址是当前用户请求以/api开始的uri去除第一个路径/api 2. 简化请求 为了简化用户请求地址, 用户请求 http://localhost:8001/info/1 我们想统一路由到 http://localhost:18081/driver/info/1 ，可以使用==PrefixPath 过滤器增 加前缀==。\ngateway: routes: - id: hailtaxi-driver uri: lb://hailtaxi-driver predicates: - Path=/** filters: - PrefixPath=/driver 3.3 自定义Filter 1. GateFilter GatewayFilter 一般作用在某一个路由上，==需要实例化创建才能使用==，局部过滤器需要实现接口==GatewayFilter、Ordered== 。\n实现接口GatewayFilter, Ordered public class PayFilter implements GatewayFilter, Ordered { @Override public Mono\u0026lt;Void\u0026gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) { System.out.println(\u0026quot;GatewayFilter拦截器执行---pre-----PayFilter\u0026quot;); return chain.filter(exchange).then(Mono.fromRunnable(()-\u0026gt;{ System.out.println(\u0026quot;GatewayFilter拦截器执行---post-----PayFilter\u0026quot;); })); } @Override public int getOrder() { return 0; } } 继承AbstractGatewayFilterFactory, 交给spring容器管理 2. GlobalFilter GlobalFilter:过滤器拦截处理方法 Ordered: 过滤器也有多个，这里主要定义过滤器执行顺序，里面有个方法 getOrder()会返回过滤器执行顺序，返回值越小，越靠前执行 3.4 跨域配置 出于浏览器的==同源策略限制==。同源策略（Sameoriginpolicy）是一种约 定，它是浏览器==最核心也最基本的安全功能==，如果缺少了同源策略，则浏览器的正常功能可能都会受到影响。可以说Web是构建在同源策略基础之上的，浏览器只是针对同源策略的一种实现。==同源策略会阻止一个域的javascript脚本和另外一个域的内容进行交互==。\n==所谓同源（即指在同一个域）就是两个页面具有 相同的协议（protocol），主机（host）和端口号（port)==\n4. 源码导读 5. Actuator 监听 ​\t/gateway的actuator端点允许监视Spring Cloud Gateway应用程序并与之交互。要进行远程访问，必须在应用程序属性中暴露HTTP或JMX 端口。\nmanagement.endpoint.gateway.enabled=true # default value management.endpoints.web.exposure.include=gateway management: endpoint: gateway: enabled: true endpoints: web: exposure: include: gateway 路径 类型 描述 route_id String 路线编号。 route_object.predicate Object 路由谓词。 route_object.filters Array GatewayFilter工厂使用的路由。 order Number 路线顺序。 1. 获取所有路由配置 GET: http://localhost:8081/actuator/gateway/routes\n[ { \u0026quot;predicate\u0026quot;: \u0026quot;(((Paths: [/driver/**], match trailing slash: true \u0026amp;\u0026amp; Cookie: name=username regexp=itheima) \u0026amp;\u0026amp; Header: token regexp=123456) \u0026amp;\u0026amp; Methods: [GET, POST])\u0026quot;, \u0026quot;route_id\u0026quot;: \u0026quot;hailtaxi-driver\u0026quot;, \u0026quot;filters\u0026quot;: [ \u0026quot;[com.itheima.filter.PayMethodGatewayFilterFactory$$Lambda$883/0x0000000800671840@7670252b, order = 1]\u0026quot;, \u0026quot;[org.springframework.cloud.gateway.filter.factory.RequestRateLimiterGatewayFilterFactory$$Lambda$896/0x0000000800675840@65e52e7, order = 2]\u0026quot; ], \u0026quot;uri\u0026quot;: \u0026quot;lb://hailtaxi-driver\u0026quot;, \u0026quot;order\u0026quot;: 0 }, { \u0026quot;predicate\u0026quot;: \u0026quot;Paths: [/order/**], match trailing slash: true\u0026quot;, \u0026quot;route_id\u0026quot;: \u0026quot;hailtaxi-order\u0026quot;, \u0026quot;filters\u0026quot;: [], \u0026quot;uri\u0026quot;: \u0026quot;lb://hailtaxi-order\u0026quot;, \u0026quot;order\u0026quot;: 0 }, { \u0026quot;predicate\u0026quot;: \u0026quot;Paths: [/pay/**], match trailing slash: true\u0026quot;, \u0026quot;route_id\u0026quot;: \u0026quot;hailtaxi-pay\u0026quot;, \u0026quot;filters\u0026quot;: [], \u0026quot;uri\u0026quot;: \u0026quot;lb://hailtaxi-pay\u0026quot;, \u0026quot;order\u0026quot;: 0 } ] 2. 创建路由 GET: /actuator/gateway/routes/{route_id_to_create}\n{ \u0026quot;id\u0026quot;: \u0026quot;first_route\u0026quot;, \u0026quot;predicates\u0026quot;: [{ \u0026quot;name\u0026quot;: \u0026quot;Path\u0026quot;, \u0026quot;args\u0026quot;: {\u0026quot;_genkey_0\u0026quot;:\u0026quot;**/api/v1/hm\u0026quot;} }], \u0026quot;uri\u0026quot;: \u0026quot;http://localhost:8081\u0026quot;, \u0026quot;order\u0026quot;: 0 } 3. 删除路由 DELETE: /actuator/gateway/routes/{id_route_to_create}\n4. 刷新路由缓存 路由规则创建和删除之后不会立即生效, 需要刷新才可以生效\nPOST: /actuator/gateway/refresh\n6. 自动刷新路由 actuator\n查询全局过滤器， http://localhost:9001/actuator/gateway/globalfilters 查询路由中的过滤器， http://localhost:9001/actuator/gateway/routefilters 刷新路由缓存， http://localhost:9001/actuator/gateway/refresh 展示路由列表, http://localhost:9001/actuator/gateway/routes 获取单个路由的信息， http://localhost:9001/actuator/gateway/routes/host_route 创建路由，发送Post请求， ../actuator/gateway/routes/{id_route_to_create} 删除路由，发送Delete请求： ../actuator/gateway/routes/{id_route_to_delete} ​\t路由信息不论是通过yml 文件配置还是代码. 这些配置最终都是被封装到了RouteDefinition对象中, 所有路由信息在系统启动时就被加载装配好了, 并存到了内存中\n四种实现方式\nnacos 配置 + 事件监听 + 自动刷新 数据库配置 + 接口调用 + 自动刷新 changgou\nspring: cloud: gateway: globalcors: cors-configurations: '[/**]': # 匹配所有请求 allowedOrigins: \u0026quot;*\u0026quot; #跨域处理 允许所有的域 allowedMethods: # 支持的方法 - GET - POST - PUT - DELETE routes: # 配置 goods 服务通过 网关路由 - id: changgou_goods_route uri: lb://goods predicates: - Path=/api/album/**,/api/brand/**,/api/cache/**,/api/categoryBrand/**,/api/category/**,/api/para/**,/api/pref/**,/api/sku/**,/api/spec/**,/api/spu/**,/api/stockBack/**,/api/template/** filters: # - PrefixPath=/brand - StripPrefix=1 - name: RequestRateLimiter # 请求限流. 名字不能随意写 args: key-resolver: \u0026quot;#{@ipKeyResolver}\u0026quot; redis-rate-limiter.replenishRate: 1 redis-rate-limiter.burstCapacity: 1 # 配置 user 服务通过 网关路由 - id: changgou_user_route uri: lb://user predicates: - Path=/api/user/**,/api/address/**,/api/areas/**,/api/cities/**,/api/provinces/** filters: - StripPrefix=1 # 配置路由到订单微服务 - id: changgou_order_route uri: lb://order predicates: - Path=/api/cart/**,/api/categoryReport/**,/api/orderConfig/**,/api/order/**,/api/orderItem/**,/api/orderLog/**,/api/preferential/**,/api/returnCause/**,/api/returnOrder/**,/api/returnOrderItem/**,/api/order filters: - StripPrefix=1 application: name: gateway-web # Redis配置 redis: host: 192.168.211.132 port: 6379 server: port: 8001 eureka: client: service-url: defaultZone: http://127.0.0.1:7001/eureka instance: prefer-ip-address: true management: endpoint: gateway: enabled: true web: exposure: include: true # 超时设置 ribbon: ReadTimeout: 300000 0. yaml 配置转 Json ; mysql record gateway: routes: - id: iam-service uri: http://10.168.1.200:9000 predicates: - Path=/api/v1/account/** filters: - StripPrefix=3 - id: jc-service uri: http://10.168.1.200:10101 predicates: - Path=/api/v1/jc/** filters: - StripPrefix=3 - id: dh-hongmeng-webapi uri: lb://dh-hongmeng-webapi predicates: - Path=/api/v1/hm/** # 云眼 - id: yy-service uri: http://10.168.1.200:30900 predicates: - Path=/api/v1/eagle/** filters: - StripPrefix=3 1. nacos 通过配置文件刷新\n2. mysql 7. gateway集成权限 ","date":"2023-01-01T15:12:11+08:00","permalink":"https://mikeLing-qx.github.io/p/spring_cloud_gateway/","title":"Spring_cloud_gateway"},{"content":"1. Demo 依赖\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba.csp\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;sentinel-core\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.8.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 如何对下面这个接口进行限流?\n@RestController @RequestMapping(\u0026quot;/user\u0026quot;) public class UserController { @Resource private UserService userService; @RequestMapping(\u0026quot;/list\u0026quot;) public List\u0026lt;User\u0026gt; getUserList() { return userService.getList(); } } @Service public class UserServiceImpl implements UserService { //模拟查询数据库数据，返回结果 @Override public List\u0026lt;User\u0026gt; getList() { List\u0026lt;User\u0026gt; userList = new ArrayList\u0026lt;\u0026gt;(); userList.add(new User(\u0026quot;1\u0026quot;, \u0026quot;望月理奈\u0026quot;, 18)); userList.add(new User(\u0026quot;2\u0026quot;, \u0026quot;kotoko\u0026quot;, 20)); userList.add(new User(\u0026quot;3\u0026quot;, \u0026quot;花鸟玲爱\u0026quot;, 21)); return userList; } } 1. 抛出异常的方式 @RestController @RequestMapping(\u0026quot;/user\u0026quot;) public class UserController { //资源名称 public static final String RESOURCE_NAME = \u0026quot;userList\u0026quot;; @Resource private UserService userService; @RequestMapping(\u0026quot;/list\u0026quot;) public List\u0026lt;User\u0026gt; getUserList() { List\u0026lt;User\u0026gt; userList = null; Entry entry = null; try { // 被保护的业务逻辑 entry = SphU.entry(RESOURCE_NAME); userList = userService.getList(); } catch (BlockException e) { // 资源访问阻止，被限流或被降级 return Collections.singletonList(new User(\u0026quot;xxx\u0026quot;, \u0026quot;资源访问被限流\u0026quot;, 0)); } catch (Exception e) { // 若需要配置降级规则，需要通过这种方式记录业务异常 Tracer.traceEntry(e, entry); } finally { // 务必保证 exit，务必保证每个 entry 与 exit 配对 if (entry != null) { entry.exit(); } } return userList; } } 同时需要定义限流的规则\n@SpringBootApplication public class SpringmvcApplication { public static void main(String[] args) throws Exception { SpringApplication.run(SpringmvcApplication.class, args); //初始化限流规则 initFlowQpsRule(); } //定义了每秒最多接收2个请求 private static void initFlowQpsRule() { List\u0026lt;FlowRule\u0026gt; rules = new ArrayList\u0026lt;\u0026gt;(); FlowRule rule = new FlowRule(UserController.RESOURCE_NAME); // set limit qps to 2 rule.setCount(2); rule.setGrade(RuleConstant.FLOW_GRADE_QPS); rule.setLimitApp(\u0026quot;default\u0026quot;); rules.add(rule); FlowRuleManager.loadRules(rules); } } 2. 注解方式 @Service public class UserServiceImpl implements UserService { //资源名称 public static final String RESOURCE_NAME_QUERY_USER_BY_NAME = \u0026quot;queryUserByUserName\u0026quot;; //value是资源名称，是必填项。blockHandler填限流处理的方法名称 @Override @SentinelResource(value = RESOURCE_NAME_QUERY_USER_BY_NAME, blockHandler = \u0026quot;queryUserByUserNameBlock\u0026quot;) public User queryByUserName(String userName) { return new User(\u0026quot;0\u0026quot;, userName, 18); } //注意细节，一定要跟原函数的返回值和形参一致，并且形参最后要加个BlockException参数 //否则会报错，FlowException: null public User queryUserByUserNameBlock(String userName, BlockException ex) { //打印异常 ex.printStackTrace(); return new User(\u0026quot;xxx\u0026quot;, \u0026quot;用户名称：{\u0026quot; + userName + \u0026quot;},资源访问被限流\u0026quot;, 0); } } 2. Sentinel Sentinel 是面向分布式服务架构的流量控制组件，主要以流量为切入点，从限流、流量整形、熔断降级、系统负载保护、热点防护等多个维度来帮助开发者保障微服务的稳定性。\n0. hystrix对比 对比内容 Sentinel Hystrix 隔离策略 信号量隔离 线程池隔离/信号量隔离 熔断降级策略 基于响应时间或失败比率 基于失败比率 实时指标实现 滑动窗口 滑动窗口（基于 RxJava） 规则配置 支持多种数据源 支持多种数据源 扩展性 多个扩展点 插件的形式 基于注解的支持 支持 支持 限流 基于 QPS，支持基于调用关系的限流 不支持 流量整形 支持慢启动、匀速器模式 不支持 系统负载保护 支持 不支持 控制台 开箱即用，可配置规则、查看秒级监控、机器发现等 不完善 常见框架的适配 Servlet、Spring Cloud、Dubbo、gRPC 等 Servlet、Spring Cloud Netflix 1. 基本概念 资源 资源是 Sentinel 的关键概念。它可以是 Java 应用程序中的任何内容，例如，由应用程序提供的服务，或由应用程序调用的其它应用提供的服务，甚至可以是一段代码。\n==只要通过 Sentinel API 定义的代码，就是资源==，能够被 Sentinel 保护起来。大部分情况下，可以使用方法签名，URL，甚至服务名称作为资源名来标示资源。\n规则 围绕资源的实时状态设定的规则，可以包括流量控制规则、熔断降级规则以及系统保护规则。所有规则可以动态实时调整。 2. 隔离机制 (线程池/信号量) ==线程池隔离==顾名思义就是通过==Java的线程池进行隔离==，B服务调用C服务==给予固定的线程数量==比如10个线程，如果此时==C服务宕机了==就算大量的请求过来，==调用C服务的接口只会占用10个线程==不会占用其他工作线程资源，因此==B服务就不会出现级联故障==\n信号量隔离是使用JUC下的Semaphore来实现的，当拿不到信号量的时候直接拒接因此不会出现超时占用其他工作线程的情况。\nSemaphore semaphore = new Semaphore(10,true); //获取信号量 semaphore.acquire(); //do something here //释放信号量 semaphore.release(); ​ 线程池隔离==针对不同的资源分别创建不同的线程池==，不同服务调用都发生在不同的线程池中，在线程池排队、超时等阻塞情况时可以快速失败。线程池隔离的好处是==隔离度比较高==，可以针对某个资源的线程池去进行处理而不影响其它资源，但是代价就是==线程上下文切换的 overhead 比较大==，特别是对==低延时的调用有比较大的影响==。而信号量隔离非常轻量级，仅限制对某个资源调用的并发数，而不是显式地去创建线程池，所以 overhead 比较小，但是效果不错，==也支持超时失败==。\n比较项 线程池隔离 信号量隔离 线程 与调用线程不同，使用的是线程池创建的线程 与调用线程相同 开销 排队，切换，调度等开销 无线程切换性能更高 是否支持异步 支持 不支持 是否支持超时 支持超时 支持超时(新版本支持) 并发支持 支持通过线程池大小控制 支持通过最大信号量控制 3. 核心功能 3.1 流量控制 ​\t流量控制在网络传输中是一个常用的概念，它用于==调整网络包的发送数据==。然而，从系统稳定性角度考虑，在处理请求的速度上，也有非常多的讲究。任意时间到来的请求往往是随机不可控的，而系统的处理能力是有限的。我们需要根据系统的处理能力对流量进行控制。Sentinel 作为一个调配器，可以根据需要把随机的请求调整成合适的形状，如下图所示：\n流量控制有以下几个角度:\n资源的调用关系，例如资源的调用链路，资源和资源之间的关系； 运行指标，例如 QPS、线程池、系统负载等； 控制的效果，例如直接限流、冷启动、排队等。 Sentinel 的设计理念是让您自由选择控制的角度，并进行灵活组合，从而达到想要的效果。\n3.2 熔断降级 对调用链路中的不稳定因素进行熔断也是 Sentinel 的使命之一。由于调用关系的复杂性，如果调用链路中的某个资源出现了不稳定，可能会导致请求发生堆积，进而导致级联错误 ​\tSentinel 和 Hystrix 的==原则是一致的==: 当检测到调用链路中某个资源出现不稳定的表现，例如请求响应时间长或异常比例升高的时候，则对这个资源的调用进行限制，==让请求快速失败==，避免影响到其它的资源而导致级联故障。\nSentinel熔断降级设计\nHystrix 通过 线程池隔离 的方式，来对依赖（在 Sentinel 的概念中对应 资源）进行了隔离。这样做的好处是资源和资源之间做到了最彻底的隔离。缺点是除了==增加了线程切换的成本（过多的线程池导致线程数目过多）==，还需要预先给各个资源做线程池大小的分配。\nSentinel熔断降级设计：\n==并发线程数限制==:和资源池隔离的方法不同，Sentinel 通过==限制资源并发线程的数量，来减少不稳定资源对其它资源的影响==。这样不但没有线程切换的损耗，也不需要您预先分配线程池的大小。当某个资源出现不稳定的情况下，例如响应时间变长，对资源的直接影响就是会造成线程数的逐步堆积。当线程数在特定资源上堆积到一定的数量之后，对该资源的新请求就会被拒绝。堆积的线程完成任务后才开始继续接收请求。\n==响应时间降级==:除了对并发线程数进行控制以外，Sentinel 还可以通过响应时间来快速降级不稳定的资源。当依赖的资源出现响应时间过长后，所有对该资源的访问都会被直接拒绝，直到过了指定的时间窗口之后才重新恢复。\n3.3 Sentinel的规则 流量控制规则 QPS 流量控制 线程数流量控制 熔断降级规则 系统保护规则 访问控制规则 热点规则 ","date":"2023-01-01T15:10:40+08:00","permalink":"https://mikeLing-qx.github.io/p/spring_cloud_sentinel/","title":"Spring_cloud_sentinel"},{"content":"1. 拦截器 参考资料: https://www.cnblogs.com/monkeydai/p/16625918.html\n==Mybatis拦截器应用场景==\nSql摘要统计/监控 动态分页 多租户 脱敏 加解密 使用场景是什么, 支持自定义加解密处理器 可以通过注解@ScheField 指定不同的加解密 方式, 默认方式为DefaultSecureHandler 返回原字符串, , 注解上也可以配置自定义的加解密处理器,继承 SecureFiledHandler, 里面有两抽象方法, encrypt 和decrypt 需要把加解密处理器注入到spring中, 在调用的时候会通过class 去获取实例, 通过反射去执行相关的加解密方法 同时并不是所有的查询和插入操作都走拦截器只有在 mapper 类上有 @SecureMapper 注解时才走 对接中山密评的. 因为sdk的调用不允许多线程, 因此创建PasswordSdkManager, 里面一个固定大小的线程池, 把sdk实例, 创放到Threadlocal 中, 通过config.properties 动态加载配置(不通过spring管理PasswordSdkManager) WcspSdkHandler 密码处理器中, 通过 PasswordSdkManager.submitTask(new Callable() {..}) 调用sdk 获取加解密的数据, 但是在这里sdk调用的耗时比较长, 已经严重影响到了性能 现在我的疑问是: 在结果处理 或者 参数处理的时候, 如何提高性能 使用Methodhandle来替换反射调用, 比传统的反射更加灵活, 性能更高 使用redis 缓存 定位到问题就是调用第三方sdk 的时候耗时较长 多线程 ? \u0026ndash; 在listDecrypt 和 listEncrypt 进行处理 可不可以集成为spring-boot-starter ? 具体使用步骤\n冗余字段新增 mybatis拦截器的密文处理器 现在因为, 密码sdk是不允许多线程调用的, 我的设想是创建一个固定大小的线程池, 核心线程数等于最大线程数, 线程初始化的时候同时实例化密码sdk对象, 把它放到线程ThreadLocal中, 这样是否可以解决 密码sdk是不允许多线程调用的 线程安全问题? 如果可以提供代码参考示例, 使用 密码sdk实例的创建如下 ​```LightDataService ldService = LightDataService.getInstance(TENANT_ID, APPID, params, SECRET);``` 这是我的完整密文处理器 ​```@Component public class WcspSdkHandler extends SecureFieldHandler { private final String TENANT_ID = \u0026quot;xj4xq38bzxtscgh9\u0026quot;; private final String APPID = \u0026quot;35a06ef5d23c46b38504d49eb6153b35\u0026quot;; private final String SECRET = \u0026quot;C20D60D04D9CE7E2D95B1C35ACD49445\u0026quot;; private final static HashMap\u0026lt;String, Object\u0026gt; params = new HashMap\u0026lt;\u0026gt;(); static { params.put(CryptoServicePlatform.INIT_PARAM_URL, \u0026quot;https://wcsp.k8s.dev:32443\u0026quot;); params.put(CryptoServicePlatform.INIT_PARAM_WORKDIR, \u0026quot;E:\\\\develop\\\\zs-ipr-secret\\\\\u0026quot;); } @Override String encrypt(String data2Encrypt) { String encData; try { LightDataService ldService = LightDataService.getInstance(TENANT_ID, APPID, params, SECRET); encData = Data.toBase64String(ldService.encrypt(Data.fromUTF8String(data2Encrypt))); } catch (Exception e) { throw new RuntimeException(e); } return encData; } @Override String decrypt(String data2Decrypt) { String decData; try { LightDataService ldService = LightDataService.getInstance(TENANT_ID, APPID, params, SECRET); decData = Data.toUTF8String(ldService.decrypt(Data.fromBase64String(data2Decrypt))); } catch (Exception e) { throw new RuntimeException(e); } return decData; } }``` ==mybatis interceptor==\n只需要写一个类,然后继承mybatis的Interceptor方法,然后==使用@Intercepts注解==说明需要拦截的类和方法即可.可以在@Intercepts中==配置多个类和方法==,用于拦截多个方法.然后把我们定义的类定义到mybatis的配置文件的元素中 或者 添加到 sqlSessionFactoryList .\nmybatis在解析配置文件的时候会自动的为我们生成代理,拦截我们需要拦截的方法.\nmybatis中==只能拦截下面类中的方法,其他的都不会进行拦截==\nExecutor 执行器(update, query, flushStatements, commit, rollback, getTransaction, close, isClosed) insert、update和delete这三种操作在数据库层面都是对数据进行修改，所以在Executor接口中，MyBatis只定义了一个update方法用来执行这三种操作 ParameterHandler 参数处理器 (getParameterObject, setParameters) ResultSetHandler 结果处理器(handleResultSets, handleOutputParameters) StatementHandler sql 语法构建器(prepare, parameterize, batch, update, query) 常见的mybatis 用法\n// 大家经常见到的mybatis的用法 public static void main(String[] args) throws IOException { SqlSession session = null; try { // ①：加载Mybatis配置文件 InputStream inputStream = Resources.getResourceAsStream(\u0026quot;mybatis-config.xml\u0026quot;); // ②：构建SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); // ③：获取SqlSession并获取代理对象查询 session = sqlSessionFactory.openSession(); ProductDao mapper = session.getMapper(ProductDao.class); System.out.println(mapper.findById(100001L)); } finally { if (!ObjectUtils.isEmpty(session)) { session.close(); } } } demo\n/** * @author Duansg * @date 2022-10-08 11:45 下午 */ @Intercepts(value = { @Signature(type = Executor.class, method = \u0026quot;query\u0026quot;, args = {MappedStatement.class, Object.class, RowBounds.class, ResultHandler.class, CacheKey.class, BoundSql.class}), @Signature(type = Executor.class, method = \u0026quot;query\u0026quot;, args = {MappedStatement.class, Object.class, RowBounds.class, ResultHandler.class})} ) public class ExampleInterceptor implements Interceptor { @Override public Object intercept(Invocation invocation) throws Throwable { System.err.println(\u0026quot;晓断测试Interceptor\u0026quot;); return invocation.proceed(); } @Override public Object plugin(Object target) { return Plugin.wrap(target, this); } @Override public void setProperties(Properties properties) { System.err.println(\u0026quot;晓断测试Interceptor-Properties\u0026quot; + JSON.toJSONString(properties)); } } ==添加到 sqlSessionFactoryList==\n@Configuration @AutoConfigureAfter(PageHelperAutoConfiguration.class) public class MyBatisConfig { @Autowired private List\u0026lt;SqlSessionFactory\u0026gt; sqlSessionFactoryList; @PostConstruct public void addMyInterceptor() { MybatisInterceptor e = new MybatisInterceptor(); for (SqlSessionFactory sqlSessionFactory : sqlSessionFactoryList) { sqlSessionFactory.getConfiguration().addInterceptor(e); } } } ​\tInterceptor拦截器的实现需要实现Mybatis提供的Interceptor接口，并重写其中的三个方法：\nintercept：拦截方法，用于在执行SQL语句前后进行处理。 plugin：用于生成代理对象，将拦截器应用到Mybatis中。 setProperties：用于设置拦截器的属性。 ==拦截器的执行顺序==\nExecutor首先执行的是没有问题的。ParameterHandler -\u0026gt; ResultHandler 的先后顺序也没有问题\n如果你拦截的是StatementHandler.query()方法，那么顺序是Executor -\u0026gt; ParameterHandler -\u0026gt; StatementHandler -\u0026gt; ResultHandler。 如果你拦截的是StatementHandler.prepare()方法，那么顺序是Executor -\u0026gt; StatementHandler-\u0026gt; ParameterHandler -\u0026gt; ResultHandler 通过注解的方式 必须要加类上的注解, 不然不生效 1. 测试各种情况下的拦截 a. 只更新指定的加密字段 b. 更新非加密字段 c. sql a. 单String b. List\u0026lt;String\u0026gt; c. 多String d. Map e. Bean对象 2. mybatis 执行过程 3. mybatis 主要成员 Configuration MyBatis所有的配置信息都保存在Configuration对象中，配置文件中的大部分配置都会 存储到该类中\nSqlSession 作为MyBatis工作的主要顶层API，表示和数据库交互时的会话，完成必要数据库增删改查功能\n根据statement id，在mybatis配置对象configuration中获取到对应的mappedstatement对象，然后调用执行器来执行具体操作\nsqlSession虽然叫程序和数据库之间的SQL会话，但是它并没有具体去执行sql语句，最终的sql语句的执行是由执行器Executor执行的，而==SqlSession的作用只是创建了MappedStatement对象以及调用执行器去执行SQL,他的commit、rollback方法同样最终都是调用的执行器Executor的对应的方法==\nExecutor MyBatis执行器，是MyBatis调度的核心，负责sql语句的生成和查询缓存的维护\n根据传递的参数，完成sql语句的动态解析，生成BoundSql对象，供StatementHandler使用 为查询创建缓存，以提高性能 创建JDBC的Statement链接对象，传递给StatementHandler对象，返回List查询结果 Executor是跟SqlSession绑定在一起的，每一个SqlSession都拥有一个新的Executor对象，由Configuration创建。\nExecutor接口的实现类有==BaseExecutor和CachingExecutor==，而BaseExecutor的子类又有SimpleExecutor、ReuseExecutor和BatchExecutor，但==BaseExecutor是一个抽象类==，其只实现了一些公共的封装，而把真正的核心实现都==通过方法抽象出来给子类实现==，如doUpdate()、doQuery()；CachingExecutor只是在Executor的基础上加入了缓存的功能，底层还是通过Executor调用的，所以真正有作用的Executor只有SimpleExecutor、ReuseExecutor和BatchExecutor。它们都是自己实现的Executor核心功能，没有借助任何其它的Executor实现，它们是实现不同也就注定了它们的功能也是不一样的。\nStatementHandler 封装了JDBC Statement操作，负责对JDBC statement的操作，如设置参数等\n1. 对于JDBC的preparedStatement类型的对象，创建过程中，sql语句字符串会包含若干个？占位符，然后再赋值。 2. StatementHandler通过parameterize（statement）方法对statement进行设值 StatementHandler通过List query(Statement statement,ResultHandler resultHandler)方法来完成执行Statement，和将Statement对象返回的resultSet封装成List statementHandler接口的实现大致有四个，其中三个实现类都是和JDBC中的Statement响对应的：\nSimpleStatementHandler，这个很简单了，就是对应我们JDBC中常用的Statement接口，用于简单SQL的处理； PreparedStatementHandler，这个对应JDBC中的PreparedStatement，预编译SQL的接口； CallableStatementHandler，这个对应JDBC中CallableStatement，用于执行存储过程相关的接口； RoutingStatementHandler，这个接口是以上三个接口的路由，没有实际操作，只是负责上面三个StatementHandler的创建及调用。 ParameterHandler 负责对用户传递的参数转换成JDBC Statement所对应的数据类型，对statement对象的？占位符进行赋值\nResultSetHandler 负责将JDBC返回的ResultSet结果集对象转换成List类型的集合\nTypeHandler 负责java数据类型和jdbc数据类型（也可以说是数据表列类型）之间的映射和转换\nMappedStatement MappedStatement维护一条==\u0026lt;select|update|delete|insert\u0026gt;节点的封装==\nMappedStatement 中包含了一条SQL语句操作所需要的全部信息，如下：\nid ：对应的SQL语句ID。 sqlSource ：SQL语句的封装。 commandType：Sql命令类型，取值INSERT, UPDATE, DELETE, SELECT, FLUSH。 resource：对应的Mapper.xml的路径。 parameterMap：参数映射。 resultMaps：结果映射。 timeout：超时时间。 statementType：语句类型，取值STATEMENT, PREPARED, CALLABLE。表示对应的jdbc statement类型。 fetchSize：FetchSize大小。 resultSetType：结果集类型，取值FORWARD_ONLY,SCROLL_SENSITIVE, SCROLL_INSENSITIVE 或者null。 有了MappedStatement，当执行一个SQL语句时，Mybatis就能知道应该如何去操作数据库，这些SQL语句被封装成MappedStatement后，存放在Configuration对象的mappedStatements属性中，供MyBatis在进行数据库操作时使用。\ngetBoundSql()方法是MappedStatement类的一个方法，它用来获取BoundSql对象。\nBoundSql是MyBatis的核心处理单元之一，你可以理解为它代表了一段已准备好待执行的SQL语句。主要包含以下信息：\nsql: 实际的SQL语句，所有的参数都已经被替换成？问号作为占位符。 parameterObject: 参数对象，用来替换SQL中的问号。可以是一个简单类型如String、Integer或者是一个POJO类或者Map等复合类型。 parameterMappings: 一个ParameterMapping对象的列表，每个ParameterMapping对象代表一个?占位符及其映射。 additionalParameters: 额外的参数，通常我们不需要关心。 getBoundSql()方法接收一个参数对象，该参数对象通过sqlSource.getBoundSql(parameterObject)获取BoundSql对象。\n为了以后可能需要重用SQL，这部分信息被封装到BoundSql类中，并在StatementHandler下一步需要准备（Prepare）Statement时被使用。\nSqlSource 负责根据用户传递的parameterObject，动态的生成SQL语句，将信息封装到BoundSql对象中并返回\nBoundSql 表示动态生成的SQL语句以及相应的参数信息\n代表了一段已准备好待执行的SQL语句。主要包含以下信息：\nsql: 实际的SQL语句，所有的参数都已经被替换成？问号作为占位符。 parameterObject: 参数对象，用来替换SQL中的问号。可以是一个简单类型如String、Integer或者是一个POJO类或者Map等复合类型。 parameterMappings: 一个ParameterMapping对象的列表，每个ParameterMapping对象代表一个?占位符及其映射。 additionalParameters: 额外的参数，通常我们不需要关心。 getBoundSql()方法接收一个参数对象，该参数对象通过sqlSource.getBoundSql(parameterObject)获取BoundSql对象。\n为了以后可能需要重用SQL，这部分信息被封装到BoundSql类中，并在StatementHandler下一步需要准备（Prepare）Statement时被使用。\n3. 常用 sql \u0026lt;!--热门套餐，查询前5条--\u0026gt; \u0026lt;select id=\u0026quot;findHotSetmeal\u0026quot; resultType=\u0026quot;map\u0026quot;\u0026gt; select s.name, count(o.id) setmeal_count ,count(o.id)/t.total proportion,s.remark from t_order o, t_setmeal s,(select count(id) total from t_order) t where s.id = o.setmeal_id group by o.setmeal_id order by setmeal_count desc limit 0,4 \u0026lt;/select\u0026gt; \u0026lt;!--新增--\u0026gt; \u0026lt;insert id=\u0026quot;add\u0026quot; parameterType=\u0026quot;Order\u0026quot;\u0026gt; \u0026lt;selectKey resultType=\u0026quot;java.lang.Integer\u0026quot; order=\u0026quot;AFTER\u0026quot; keyProperty=\u0026quot;id\u0026quot;\u0026gt; SELECT LAST_INSERT_ID() \u0026lt;/selectKey\u0026gt; insert into t_order(member_id,orderDate,orderType,orderStatus,setmeal_id) values (#{memberId},#{orderDate},#{orderType},#{orderStatus},#{setmealId}) \u0026lt;/insert\u0026gt; 条件 choose, otherwise\n\u0026lt;select id=\u0026quot;listEnforceAction\u0026quot; parameterType=\u0026quot;map\u0026quot; resultType=\u0026quot;map\u0026quot;\u0026gt; SELECT ea.*,b.name,b.id_code FROM enforce_action ea LEFT JOIN account b on ea.account_id = b.id WHERE ea.status !=0 \u0026lt;if test=\u0026quot;params.accountId != null\u0026quot;\u0026gt; \u0026lt;choose\u0026gt; \u0026lt;when test=\u0026quot;params.cid == null and params.did == null\u0026quot;\u0026gt; AND ea.id IN ( SELECT p.action_id FROM enforce_participate p LEFT JOIN enforce_action a ON p.action_id = a.id WHERE p.account_id = #{params.accountId} AND p.status != 0 ORDER BY a.start_time DESC ) \u0026lt;/when\u0026gt; \u0026lt;otherwise\u0026gt; \u0026lt;if test=\u0026quot;params.actionIds != null and params.actionIds.size\u0026gt;0\u0026quot;\u0026gt; AND ea.id IN ( \u0026lt;foreach collection=\u0026quot;params.actionIds\u0026quot; item=\u0026quot;item\u0026quot; separator=\u0026quot;,\u0026quot; \u0026gt; #{item} \u0026lt;/foreach\u0026gt; ) \u0026lt;/if\u0026gt; \u0026lt;/otherwise\u0026gt; \u0026lt;/choose\u0026gt; \u0026lt;/if\u0026gt; ","date":"2022-11-10T14:55:11+08:00","permalink":"https://mikeLing-qx.github.io/p/mybatis/","title":"Mybatis"},{"content":"参考资料\n思路拓展: model 用thymeleaf 转 html 再转 pdf https://zhuanlan.zhihu.com/p/378852796\nItext 上手: https://github.com/CuteXiaoKe/iText7-examples\n视图模型\n1. Itext入门 maven\n\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;com.itextpdf\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;itext7-core\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;7.2.1\u0026lt;/version\u0026gt;\r\u0026lt;/dependency\u0026gt;\r1. 生成pdf 第 1 步：创建一个 PdfWriter 对象\r第 2 步：创建一个 PdfDocument 对象\r第 3 步：添加一个空页面 PdfDocument类的addNewPage()方法用于在 PDF 文档中创建一个空白页面\r第 4 步：创建一个 Document 对象\r步骤 5：关闭文档\rimport com.itextpdf.kernel.pdf.PdfDocument; import com.itextpdf.kernel.pdf.PdfWriter; import com.itextpdf.layout.Document; public class create_PDF { public static void main(String args[]) throws Exception { // 1、Creating a PdfWriter String dest = \u0026quot;C:/itextExamples/sample.pdf\u0026quot;; PdfWriter writer = new PdfWriter(dest);\r// 2、Creating a PdfDocument PdfDocument pdfDoc = new PdfDocument(writer);\r// 3、Adding an empty page pdfDoc.addNewPage(); // 4、Creating a Document Document document = new Document(pdfDoc); // 5、Closing the document document.close();\rSystem.out.println(\u0026quot;PDF Created\u0026quot;); }\r}\r2. 字体设置 itext使用包含特殊字符的字体——制作字体不求人\n普通职员（办公室/写字楼工作人员）\r中文默认不输出, 需要对Document 设置字体\n# 常用字体程序及对应编码\rSTSong-Light ==\u0026gt; UniGB-UCS2-H\rHeiseiKakuGo-W5 ==\u0026gt; UniJIS-UCS2-H\rHeiseiMin-W3 ==\u0026gt; UniJIS-UCS2-H\rpublic static void fun2() throws Exception{ //font-asian中文包\rPdfDocument pdfDocument = new PdfDocument(new PdfWriter(dest2));\rDocument document = new Document(pdfDocument);\rPdfFont font = PdfFontFactory.createFont(\u0026quot;STSong-Light\u0026quot;,\u0026quot;UniGB-UCS2-H\u0026quot;);\rdocument.setFont(font);\rParagraph paragraph = new Paragraph(\u0026quot;hello 瓜田李下\u0026quot;);\rdocument.add(paragraph);\rdocument.close();\r}\rpublic static void fun3() throws Exception{ //系统中文包\rPdfDocument pdfDocument = new PdfDocument(new PdfWriter(dest3));\rDocument document = new Document(pdfDocument);\rPdfFont font = PdfFontFactory.createFont(\u0026quot;./fonts/simkai.ttf\u0026quot;);\rdocument.setFont(font);\rParagraph paragraph = new Paragraph(\u0026quot;hello 瓜田李下\u0026quot;);\rdocument.add(paragraph);\rdocument.close();\r}\r3. 添加空白页 添加第二页为空白页, 立即刷新后再继续\npdf.addNewPage(2).flush();\r4. Div, Paragraph Div div = new Div();\rdiv.setWidth(UnitValue.createPercentValue(100));\rdiv.setHeight(UnitValue.createPercentValue(100));\rdiv.setHorizontalAlignment(HorizontalAlignment.CENTER);\rParagraph p1 = new Paragraph();\rp1.setHorizontalAlignment(HorizontalAlignment.CENTER);\rp1.setMaxWidth(UnitValue.createPercentValue(75));\rp1.setMarginTop(180f);\rp1.setCharacterSpacing(0.4f);\rStyle large = new Style();\rlarge.setFontSize(22);\rlarge.setFontColor(GenoColor.getThemeColor());\rp1.add(new Text(\u0026quot;尊敬的 \u0026quot;).addStyle(large));\r...\rParagraph p2 = new Paragraph();\r...\rdiv.add(p1);\r整块的内容用Div包裹，这里整块包裹的好处是什么？一方面排版分明成体系，另一方面若需求是整块的内容必须在同一个版面，你可以对Div设置div.setKeepTogether(true);，尽量保证若整块的内容超出了一页，那这块内容会自动整块出现在下一页，上一页剩下的就留白了\n可以看到Div，Paragraph可以设置很多属性，实际上我们常用的组件除了这两种，还有Table，Cell，List，他们大部分的属性都是一样的，只是部分属性只在部分组件起效果，所以当你设置某个属性没起效果也不用奇怪\nParagraph需要**特别注意的一点**，想要**段落文字居中**，不要用setHorizontalAlignment(HorizontalAlignment.CENTER);这是组件的居中对段落无效，甚至对段落里你放Text也无效，需要改用setTextAlignment(TextAlignment.CENTER);\nParagraph段落的行距也是个高频问题，这里给出官方我看到的解释，参考https://itextpdf.com/en/resources/books/itext-7-building-blocks/chapter-4-adding-abstractelement-objects-part-1，搜关键字setFixedLeading，我的理解该方法设值行高绝对值，官方解释是两行文字中间基线之间的距离\n如果想了解详细的什么属性哪里能起作用哪里不行，请访问该地址\n5. Table useAllAvailableWidth表示页面有多宽，我就有多宽\ntable.startNewRow();表示新起一行，table每画一行都要新起一行\n同样table内容需要居中，和段落一样，请设置new Cell().setTextAlignment(TextAlignment.CENTER)\n每个table中cell都有默认高度，会比实际输入字体高些，此时设置setHeight，若更大没有问题，若高度小于或接近字体大小文字可能就消失了，若想让Cell高度更接近文字高度，请设置Cell的padding，即cell.setPadding(-2)，设置负值即可\n6. Tab, \\t itext7中如果要表示段落前的空格，不能使用\\t，但换行可以使用\\n\n实现Tab 的方式\n\\u00a0符号，大概7、8个该符号可表示tab，可能不是很准确\np1.setFirstLineIndent(24)，表示段落前留多少空，需要知道一个字多大，设置成两倍就行\nTab也是集成AbstractElement的组件，通过以下方式也可实现相同的效果\np2.add(new Tab());\rp2.addTabStops(new TabStop(20, TabAlignment.LEFT));\r​\n7. 换页 我常用的换页方法为如下，该方法可保证立即换页\ndoc.add(new AreaBreak(AreaBreakType.NEXT_PAGE));\r当然PdfDocument有addNewPage其实也可以用，但有时候你没把握好刷新时间可能导致某些混乱\n","date":"2022-11-09T19:21:43+08:00","permalink":"https://mikeLing-qx.github.io/p/itext/","title":"Itext"},{"content":"1. 概念 Service: 是一个==公开的接口或抽象类==, 定义了一个抽象的功能模块 Service Provider: 是service 接口的一个==实现类== Service Loder : SPI 机制中的核心组件, 负责==在运行时== **==发现并加载== **==Service Provider== 1.1 三大规范要素 ==规范的配置文件== ==service provider 类必须具备无参的默认构造方法== ==保证能加载到配置文件和 Service provider 类== 2. SPI 运行流程 它的作用是什么, 解决了什么?\n提供了一种==组件发现和注册的方式==, 可以用于实现各种插件, 或者灵活替换框架所使用的组件 如果要实现一个SPI 应用, 要怎么做\n背后的设计思想是什么? 得到什么启示\n面向接口 + 配置文件 + 反射技术\r3. JDBC SPI 出现前是如何使用的\n使用SPI 之后\nJDBC 实现原理:\nClassLoder 加载类, getResource/ getResources, 可以根据指定的路径, 读取classpath 中对应的文件, 我们可以用它来读取厂商放在Jar 包中的配置文件, 需要约定好配置文件的路径和格式 4. SPI 与 Springboot 自动配置 ","date":"2022-11-03T15:21:43+08:00","permalink":"https://mikeLing-qx.github.io/p/javaspi/","title":"JavaSPI"},{"content":"1. 运行时数据区 简述一下 运行时数据区的结构和各部分的作用 结构图\n虚拟机栈 是一个栈结构, 线程每执行一个方法都会有一个栈帧入栈, 方法执行结束后栈帧出栈, 栈帧中存储的是方法所需要的数据 (局部变量表, 操作数栈, 动态连接, 方法出口)\n==虚拟机栈是基于线程的==: 只有一个main()方法, 也是以线程的方式运行的, 在线程的生命周期中, 参与执行的方法栈帧会频繁地入栈和出栈, 虚拟机栈的生命周期和线程一样\n栈大小: 每个虚拟机栈==大小1M==\n堆栈溢出: 栈帧深度压栈但并不出栈, 导致栈空间不足, 抛出StackOverFlowError, ==典型的就是递归调用==\n栈帧的组成:\n局部变量表：存放我们的局部变量的（方法内的变量）。首先它是一个32 位的长度，主要存放我们的 Java 的八大基础数据类型，一般 32 位就可以存放下，如果是 64 位的就使用高低位占用两个也可以存放下，如果是局部变量是一个对象，存放的一个引用地址即可。\n操作数栈：存放 java 方法执行的操作数的，它也是一个栈，操作的元素可以是任意的 java 数据类型，一个方法刚刚开始的时候操作数栈为空，操作数栈本质上是JVM执行引擎的一个工作区，方法在执行，才会对操作数栈进行操作。\n动态链接：Java 语言特性多态\n完成出口：正常返回（调用程序计数器中的地址作为返回）、异常的话（通过异常处理器表\u0026lt;非栈帧中的\u0026gt;来确定）\n本地方法栈 本地方法栈和虚拟机栈类似，具备线程隔离的特性，不同的是，==本地方法栈服务的对象是JVM执行的native方法==，而虚拟机栈服务的是JVM执行的java方法，虚拟机规范里对这块所用的语言、数据结构、没有强制规定，虚拟机可以自由实现它，==hotspot把它和虚拟机栈合并成了1个==\n程序计数器 较小的内存空间，存储当前线程执行的字节码的偏移量；各线程之间独立存储，互不影响\n方法区 方法区（Method Area）是==可供各线程共享的运行时内存区域==，主要用来存储==已被虚拟机加载的类信息、常量、静态变量、JIT编译器编译后的代码缓存等等==，它有个别名叫做：非堆（non-heap），主要是为了和堆区分开。\n方法区中存储的信息大致可分以下两类：\n1、类信息：主要指类相关的版本、字段、方法、接口描述、引用等\n2、运行时常量池：编译阶段生成的常量与符号引用、运行时加入的动态变量\n运行时常量池 在jvm规范中，方法区除了存储类信息之外，还包含了运行时常量池。这里首先要来讲一下常量池的分类\n常量池可分两类：\n1、Class常量池（静态常量池）\n2、运行时常量池\n3、字符串常量池（没有明确的官方定义，其目的是为了更好的使用\nString ，真实的存储位置在堆）\n堆 1、堆被划分为新生代和老年代（ Tenured ），\n2、新生代与老年代的比例的值为 1:2 ，该值可以通过参数\n–XX:NewRatio 来指定\n3、新生代又被进一步划分为 Eden 和 Survivor 区， Survivor 由 From Survivor 和 To Survivor 组成，eden，from，to的大小比例为：8：1：1；可通过参数 **-**XX:SurvivorRatio 来指定\n2. 对象 JVM 对象内存布局, new 一个对象有多大 阐述对象的分配策略 new 一个对象都有哪些步骤 User user01 = new User() 对象内存布局\n对象头: 8 字节\n对象头-类型指针: 4字节\n数组多一个数组长度: 4字节\n对象创建过程\n3. 字节码 4. 类加载 JVM 类加载机制说一下\n一个java 类在整个运行时会经过的阶段\n加载 加载 loading”是整个类加载（class loading）过程的一个阶段，加载阶段\n虚拟机需要完成以下 3 件事情：\n1）通过==一个类的全限定名来获取定义此类的二进制字节流==。\n2）将这个字节流所代表的==静态存储结构转化==为方法区的运行时数据结构。\n3）在内存中生成一个代表这个类的 java.lang.Class 对象，作为方法区这个类的各种数据的访问入口。\n注意：\n加载的字节码来源，不一定非得是class文件，可以是符合字节码规范的任意地方，甚至二进制流等 从字节码到内存，是由加载器（ClassLoader）完成的，下面我们详细看一下加载器相关内容 验证 例如：\n1、文件格式验证（版本号，是不是CAFEBABYE开头，\u0026hellip;\u0026hellip;\u0026hellip;.）\n2、元数据验证（验证属性、字段、类关系、方法等是否合规）\n3、字节码验证\n4、符号引用验证\n准备 为class中定义的各种类变量（静态变量）==分配内存，并赋初始值==，注意是对应类型的初始值，赋具体值在后面的初始化阶段。注意！即便是static变量，它在这个阶段初始化进内存的依然是该类型的初始值！而不是用户代码里的初始值。\n//类变量：在准备阶段为它开辟内存空间，但是它是int的初始值，也就是 0，而真正123的赋值，是在下面的初始化阶段 public static int a = 123; //类成员变量(实例变量)的赋值是在类对象被构造时才会赋值 public String address = \u0026quot;北京\u0026quot;; //final修饰的类变量，编译成字节码后，是一个ConstantValue类型,在准 备阶段，直接给定值123，后期也没有二次初始化一说 public static final int b = 123;\r解析 将常量池内的符号引用替换为直接引用的过程\n初始化 类加载的最后一个步骤，经过这个步骤后，类信息完全进入了jvm内存，直到它被垃圾回收器回收\n1、前面几个阶段都是虚拟机来搞定的。我们也干涉不了，从代码上只能遵从它的语法要求。而这个阶段，是初始化赋值，java虚拟机才真正开始执行类中编写的java程序代码，将主导权移交给应用程序。\n2、在准备阶段，静态变量已经赋过一次系统要求的初始值了，而在初始化阶段要执行初始化函数 函数，注意 并不是程序员在代码中编写的，而是由 javac 编译器自动生成的，\n3、 函数是由编译器自动收集类中的所有静态变量的赋值动作和静态语句块（ static 代码块）中的语句合并产生的。\n4、 函数与类的构造函数（虚拟机视角的 函数）是不同的， 函数是在运行期创建对象时才执行，而 在类加载的时候就执行了。\n5、虚拟机能保障父类的 函数优先于子类 函数的执行。\n6、在 函数中会对类变量赋具体的值，也就是我们说的：\npublic static int a = 123; 这行代码的123才真正赋值完成\n双亲委派模型 类加载器加载某个类的时候，因为有多个加载器，甚至可以有各种自定义的，他们呈父子关系。这给人一种印象，子类的加载会覆盖父类，其实恰恰相反！\n与普通类继承属性不同，==类加载器会优先调父类的 loadClass 方法，如果父类能加载，直接用父类的==，否则最后一步才是自己尝试加载，从源代码上可以验证\n采用双亲委派模式的是好处是Java类随着它的类加载器一起具备了一种带有优先级的层次关系，通过这种层级关可以==避免类的重复加载==，当父加载器已经加载了该类时，就没有必要子加载器再加载一次。\n其次是考虑到==安全因素==，java核心api中定义类型不会被随意替换，假设通过网络传递一个名为 java.lang.Integer 的类，通过双亲委派模型传递到启动类加载器，而启动类加载器发现这个名字的类，发现该类已被加载，就不会重新加载网络传递过来的 java.lang.Integer ，而直接返回已加载过的Integer.class ，这样便可以==防止核心API库被随意篡改==\n双亲委派是可以被打破的\ntomcat 类加载的层次结构\n为什么要打破该模型\ntomcat 启动后会启一个jvm进程的, 它支持多个web应用部署到同一个tomcat里\n1、对于不同的web应用中的class和外部jar包，需要相互隔离，不能因为不同的web应用引用了相同的jar或者有相同的class导致一个加载成功了另一个加载不了。\n2、web容器支持jsp文件修改后不用重启，jsp文件也是要编译成.class文件的，每一个jsp文件对应一个JspClassLoader，它的加载范围仅仅是这个jsp文件所编译出来的那一个.class文件，当Web容器检测到jsp文件被修改时，会替换掉目前JasperLoader的实例，并通过再建立一个新的Jsp类加载器来实现JSP文件的热部署功能。\n5. GC JVM怎么判断一个类是不是垃圾？ 引用计数 (弱引用); 可达性分析 GC ROOTS\n说到GC ROOTS，你知道Java中哪些对象可作为GC ROOTS吗？\n对象不可达是不是立即被回收死亡？\nCMS垃圾回收器的回收过程\n如何解决跨代引用？\nCMS收集器的流程，缺点；G1收集器的流程，相对于CMS收集器的优点\nG1了解不，说说G1\n并发标记的过程是怎么样的？\n可达性分析 来判定对象是否存活的。这个算法的基本思路就是通过一系列的称为==“GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链（ Reference Chain ），当一个对象到 GC Roots 没有任何引用链相连时，则证明此对象是不可用的==。\n作为 GC Roots 的对象包括下面几种（重点是前面 4 种）：\n虚拟机栈（栈帧中的本地变量表）中引用的对象；各个线程调用方法堆栈中使用到的参数、局部变量、临时变量等。\n方法区中类静态属性引用的对象；java 类的引用类型静态变量。\n方法区中常量引用的对象；比如：字符串常量池里的引用。\n本地方法栈中 JNI（即一般说的 Native 方法）引用的对象。\nJVM 的内部引用（ class 对象、异常对象 NullPointException 、 OutofMemoryError ，系统类加载器）。（非重点）\n所有被同步锁( synchronized )持有的对象。（非重点）\nJVM 内部的 JMXBean 、 JVMTI 中注册的回调、本地代码缓存等（非重点）\nJVM 实现中的“临时性”对象，跨代引用的对象（在使用分代模型回收时只回收部分代的对象）（非重点）\n除了这些固定的 GC Roots 集合以外，跟进用户选用的垃圾回收器以及当前回收的内存区域不同，还可能会有其他对象\u0026quot;临时\u0026quot;加入成为 GC Roots 。\n被标记为finalize 之后并不是非死不可, 真正的死亡需要经过两次标记过程,\n没有找到GC ROOTS 的引用链, 它将第一次标记 随后进行一次筛选, 如果对象覆盖了finalize, 可以再finalize 中拯救 , 俗称对象的自我救赎 注意:\nfinalize 只会执行一次, 不会执行多次 不建议使用finalize, 因为不太可靠 常见的回收算法 复制算法 将内存按容量划分为大小相等的两块, 每次只使用其中的一块 当其中一块内存用完了, 就将还存活的对象复制到另一块, 原来的内存空间清理 带来的好处是：\n1、实现简单，运行高效，\n2、每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要按顺序分配内存即可，\n存在的弊端是：\n1、内存的使用率缩小为原来的一半。\n2、内存移动是必须实打实的移动（复制），所以对应的引用（直接指针）\n==适用于新生代==\n标记清除算法 1、回收效率略低，如果大部分对象是朝生夕死，那么回收效率降低，因为==需要大量标记对象和回收对象==，对比复制回收效率要低，所以该算法不适合新生代。\n2、它的主要问题是在标记清除之后会==产生大量不连续的内存碎片==，空间碎片太多可能会导致以后在程序运行过程中需要==分配较大对象时==，无法找到足够的连续内存而不得不提前触发另一次垃圾回收动作。\n3、标记清除算法适用于老年代\n标记整理法 标记处所有需要回收的对象 标记成功后, 后续步骤不是直接对对象进行清理, 而是让所有存活的对象都向一端移动 然后直接清理掉端边界以外的内存 1、标记整理需要扫描两遍\n2、标记整理与标记清除算法的区别主要在于对象的移动。对象移动不单单会加重系统负担，同时需要全程暂停户线程才能进行，同时所有引用对象的地方都需要更新（直接指针需要调整）。\n3、标记整理算法不会产生内存碎片，但是效率偏低。\n4、标记整理算法适用于老年代。\n回收器 ","date":"2022-10-13T19:21:43+08:00","permalink":"https://mikeLing-qx.github.io/p/jvm/","title":"Jvm"},{"content":" java RecursiveTask 类中的示例 为什么是f1.fork, f2.compute, + f1. join 而不是 f1.fork, f2.fork?\nclass Fibonacci extends RecursiveTask\u0026lt;Integer\u0026gt; {\rfinal int n;\rFibonacci(int n) {\rthis.n = n;\r}\rprotected Integer compute() {\rif (n \u0026lt;= 1) {\rreturn n;\r}\rFibonacci f1 = new Fibonacci(n - 1);\rf1.fork();\rFibonacci f2 = new Fibonacci(n - 2);\rreturn f2.compute() + f1.join();\r}\r}\r两种方式的具体执行差异：\nf1.fork(); f2.compute() + f1.join(); 这是通常推荐的正确使用方式。在这种做法中，f1被放入ForkJoinPool以异步执行；然后当前线程执行f2.compute()方法，这是个阻塞调用，并等待其完成；最后，调用f1.join()获取结果，如果此时f1已经计算完成，join()立即返回，否则会等待f1计算完毕。这样f1和f2都有并发执行的机会，可以充分利用CPU资源。\nf1.fork(); f2.fork(); f1.join() + f2.join(); 在这种情况下，f1和f2都被放入ForkJoinPool以异步执行，然后当前线程没有进行任何计算，而是等待f1和f2结束，意味着f1和f2都得等待一个可用的线程去执行，如果ForkJoinPool中没有空闲线程或其线程数已达上限，那么f2会被阻塞在队列中等待。这消除了一部分并发性，相比第一种方式可能性能更低一些。\n所以，基于以上解说，第一种方式更被推荐，能更有效利用多核CPU并发计算的优势。\nimport java.util.concurrent.ForkJoinPool;\rimport java.util.concurrent.RecursiveTask;\rpublic class RecursiveTaskTest {\rstatic class Fibonacci extends RecursiveTask\u0026lt;Integer\u0026gt; {\rfinal int n;\rFibonacci(int n) { this.n = n; }\rprotected Integer compute() {\rif (n \u0026lt;= 1)\rreturn n;\rFibonacci f1 = new Fibonacci(n - 1);\rf1.fork();\rFibonacci f2 = new Fibonacci(n - 2);\rreturn f2.compute() + f1.join();\r}\r}\rstatic class Fibonacci2 extends RecursiveTask\u0026lt;Integer\u0026gt; {\rfinal int n;\rFibonacci2(int n) { this.n = n; }\rprotected Integer compute() {\rif (n \u0026lt;= 1)\rreturn n;\rFibonacci2 f1 = new Fibonacci2(n - 1);\rFibonacci2 f2 = new Fibonacci2(n - 2);\rf1.fork();\rf2.fork();\rreturn f1.join() + f2.join();\r}\r}\rpublic static void main(String[] args) {\rint n = 30;\rlong start1 = System.currentTimeMillis();\rFibonacci fib = new Fibonacci(n);\rInteger result1 = new ForkJoinPool().invoke(fib);\rlong end1 = System.currentTimeMillis();\rSystem.out.println(\u0026quot;Method1 result: \u0026quot; + result1 + \u0026quot;. Time: \u0026quot; + (end1 - start1) + \u0026quot;ms\u0026quot;);\rlong start2 = System.currentTimeMillis();\rFibonacci2 fib2 = new Fibonacci2(n);\rInteger result2 = new ForkJoinPool().invoke(fib2);\rlong end2 = System.currentTimeMillis();\rSystem.out.println(\u0026quot;Method2 result: \u0026quot; + result2 + \u0026quot;. Time: \u0026quot; + (end2 - start2) + \u0026quot;ms\u0026quot;);\r}\r}\r","date":"2022-09-03T18:14:03+08:00","permalink":"https://mikeLing-qx.github.io/p/forkjoin/","title":"ForkJoin"},{"content":"1. ES入门 1.1 简介 ElasticSearch，简称为es， es是一个开源的高扩展的分布式全文检索引擎，它可以近乎实时的存储、检索数据；本身扩展性很好，可以扩展到上百台服务器，处理PB级别的数据。es也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单。\n==中文API:== https://elasticsearchjava-api.readthedocs.io/en/latest/index.html\n1.2 es重要字段与mysql的对比 index 库DB\ntype 表table\nmapping 相当于数据库的表结构\nfield 相当于mysql 表的字段\ndocument 行数据 row\n1.2.1 索引 Index 一个索引就是一个拥有几分相似特征的文档的集合。比如说，你可以有一个客户数据的索引，另一个产品目录的索引，还有一个订单数据的索引。==一个索引由一个名字来标识（必须全部是小写字母的==），并且当我们要对对应于这个索引中的文档进行索引、搜索、更新和删除的时候，都要使用到这个名字。在一个集群中，可以定义任意多的索引。\n1.2.2 类型 Type 在一个索引中，你可以定义一种或多种类型。一个类型是你的索引的一个逻辑上的分类/分区，其语义完全由你来定。通常，会为具有一组共同字段的文档定义一个类型。比如说，我们假设你运营一个博客平台并且将你所有的数据存储到一个索引中。在这个索引中，你可以为用户数据定义一个类型，为博客数据定义另一个类型，当然，也可以为评论数据定义另一个类型。\n1.2.3 文档 ducument 一个文档是一个可被索引的基础信息单元。比如，你可以拥有某一个客户的文档，某一个产品的一个文档，当然，也可以拥有某个订单的一个文档。文档以JSON（Javascript Object Notation）格式来表示，而JSON是一个到处存在的互联网数据交互格式。\n在一个index/type里面，你可以存储任意多的文档。注意，尽管一个文档，物理上存在于一个索引之中，==文档必须被索引/赋予一个索引的type==。\n1.2.4 字段 field 相当于是数据表的字段，对文档数据根据不同属性进行的分类标识的\n1.2.5 映射 mapping mapping是处理数据的方式和规则方面做一些限制，如某个字段的数据类型、默认值、分词器、是否被索引等等，这些都是映射里面可以设置的，其它就是处理es里面数据的一些使用规则设置也叫做映射，按着最优规则处理数据对性能提高很大，因此才需要建立映射，并且需要思考如何建立映射才能对性能更好。\nmapping是类似于数据库中的表结构定义，主要作用如下：\n定义index下的字段名 定义字段类型，比如数值型、浮点型、布尔型等 定义倒排索引相关的设置，比如是否索引、记录position等 1.2.6 cluster \u0026amp; node 一个节点可以通过配置集群名称的方式来加入一个指定的集群; 默认elasticsearch\n1.2.6 分片和副本 shard \u0026amp; replicas 分片:\nElasticsearch提供了将索引划分成多份的能力，这些份就叫做分片。当你==创建一个索引==的时候，你可以指定你想要的分片的数量。每个分片本身也是一个功能完善并且独立的“索引”，这个“索引”可以被放置到集群中的任何节点上。分片很重要，主要有两方面的原因： 1）允许你水平分割/扩展你的内容容量。 2）允许你在分片（潜在地，位于多个节点上）之上进行分布式的、并行的操作，进而提高性能/吞吐量。\nElasticsearch允许你创建分片的一份或多份拷贝，这些拷贝叫做复制分片，或者直接叫复制。\n在分片/节点失败的情况下，提供了高可用性 ; 复制分片从不与原/主要（original/primary）分片置于同一节点上，==搜索可以在所有的复制上并行运行==。总之，==每个索引可以被分成多个分片==。一个索引也可以被复制0次（意思是没有复制）或多次。==一旦复制了，每个索引就有了主分片==（作为复制源的原来的分片）和==复制分片==（主分片的拷贝）之别。==分片和复制的数量可以在索引创建的时候指定==。在索引创建之后，你可以在任何时候动态地改变复制的数量，但是你事后==不能改变分片的数量==。\n1.3 倒排索引 例如我们使用新华字典查询汉字，新华字典有==偏旁部首的目录（索引）==，我们查字首先查这个目录，找到这个目录中对应的偏旁部首，就可以通过这个目录中的偏旁部首找到这个==字所在的位置（文档）==\n倒排索引：通过索引找到该数据所在的文档。【快】\n原理:\n将数据==加入==到索引库（你可以理解成另外一个数据库）时，会先提取数据中的词汇（==分词==），将==词汇加入到文档域(document)==，文档域中==记录==了==词汇==以及词汇在哪条数据记录中==出现过的数据下标==。用户在==搜索数据时==，==先将用户搜索的数据进行词汇提取==，然后把==对应词汇拿到索引域中进行匹配查找==，查找后会==找到对应的下标ID==，再根据对应==下标ID==到文档域中找==真实数据==。\n原始文档数据\n完整倒排索引\n就编号8—拉斯—2—｛（3;1;\u0026lt;4\u0026gt;），（5;1;\u0026lt;4\u0026gt;）｝来说，文档频率2表示在两个文档出现。“\u0026lt;4\u0026gt;”表示单词出现的位置是文档中的第4个单词。 这个倒排索引基本上是一个完备的索引系统了，实际搜索系统的索引结构基本如此。\n倒排索引的结构是 ==文档编号, 出现次数, 第几个单词==\n1.4 es的端口 有两个： 一种 是9200端口（RestClient）rest 接口，基于http协议； 另一种是用 节点的9300端口（TransportClient），基于Tcp协议,集群内部进行通信的端口； 1.5 版本变动 6.0的版本不允许一个index下面有多个type\n5.0的版本里面还允许\nmapping types这个操作在7.0里面将被删除掉\n官方回复 Indices created in Elasticsearch 6.0.0 or later may only contain a single mapping type. Indices created in 5.x with multiple mapping types will continue to function as before in Elasticsearch 6.x. Mapping types will be completely removed in Elasticsearch 7.0.0 2. Docker 安装 elasticsearch 先拉取镜像 docker pull elasticsearch:xxx 使用kibana查看es; 要把kibana和es加入同一个网络里面 docker network create esnet 安装es docker run -id -p 9200:9200 -p 9300:9300 --name=es --network esnet f057ebddf832 出现的问题\n在es安装的时候报警告 IPv4 forwarding is disabled. Networking will not work. 在宿主机上执行echo \u0026ldquo;net.ipv4.ip_forward=1\u0026rdquo; \u0026raquo;/usr/lib/sysctl.d/00-system.conf 重启network 和docker 服务: systemctl restart network \u0026amp;\u0026amp; systemctl restart docker 验证容器是否正常启动 es启动之后自动关闭, docker ps 查看已启动的容器发现并没有es; 确认创建的方式是已守护式容器的方式创建的; 查看容器的日志\ndocker logs -f 容器id 可以看到是因为 max_map_count的值太小了, 需要设置到262144\n修改参数;\n查看max_map_count : cat /proc/sys/vm/max_map_count 65530 设置max_map_count: sysctl -w vm.max_map_count=262144 vm.max_map_count = 26214 重启容器 docker start 容器id ==测试使用es的http通信端口访问==\n安装chrome ==插件elasticsearch head 插件后查看es, 并创建myes索引库==\n#![elasticsearch Head](images\\elasticsearch Head.png)\n3. ==快速安装==(可以直接用这个) 安装kibana 和 es\ndocker run -it --name=elasticsearch -d -p 9200:9200 -p 9300:9300 -p 5601:5601 f057ebddf832 (镜像id) docker run -it -d -e ELASTICSEARCH_URL=http://127.0.0.1:9200 --name kibana --network=container:elasticsearch 6f6c0975b647 访问\nhttp://192.168.182.129:5601 ==kibana可以看到刚刚在eshead里面创建的索引==\n安装ik分词器 (注意==必须要和es的版本一致==)\nelasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.8.9/elasticsearch-analysis-ik-6.8.9.zip 这里速度太慢了; 我们直接翻墙去下然后复制到容器里面\ndocker cp /usr/local/elasticsearch-analysis-ik-6.8.9.zip elasticsearch:/usr/share/elasticsearch/plugins 这里由于ik分词器没有进行解压; 导致 elasticsearch 容器 无法启动 ;\n==修改无法启动的容器中的内容==\n进入docker 目录\n/var/lib/docker/ 查找与es相关的内容 移除掉这个zip包, 在windows解压后直接把文件传输到该宿主机目录下重启es即可\nfind ./ | grep elasticsearch/plugins/ ./overlay2/e208e75c08bda5d10b58ebd9aa5ceabe5e85b1db78a471ebf3fe468740e70d74/diff/usr/share/eh/plugins/elasticsearch-analysis-ik-6.8.9.zip\n进入以下目录\ncd /var/lib/docker/overlay2/e208e75c08bda5d10b58ebd9aa5ceabe5e85b1db78a471ebf3fe468740e70d74/diff/usr/share/elasticsearch/plugins 4. ik分词器测试 智能分词器 ik_smart 最大分词 (可以看到, ==这个分词器 除了分了 最大 还有 最, 以及 大)== 默认分词器 ==所有的中文都是 以字符的形式进行划分的== ==注意分词会默认把英文转换成小写的== 5. kibana 使用dsl语句查询es数据 ==只需要把语句复制进去看看效果就好了==\n1. 操作索引库 基本语句 # 查看所有索引 GET /_cat/indices?v # 新增索引 PUT /user # 删除索引 DELETE /user # 查看索引所有数据 GET /user/_search # 根据id查询 GET /user/userinfo/4 # 对搜索结果排序 (按照年龄降序) GET /user/_search { \u0026quot;query\u0026quot;:{ \u0026quot;match_all\u0026quot;: {} }, \u0026quot;sort\u0026quot;:{ \u0026quot;age\u0026quot;:{ \u0026quot;order\u0026quot;:\u0026quot;desc\u0026quot; } } } # 分页查询 from 表示从第几条数据开始查; size 每页数据量 GET /user/_search { \u0026quot;query\u0026quot;:{ \u0026quot;match_all\u0026quot;: {} }, \u0026quot;sort\u0026quot;:{ \u0026quot;age\u0026quot;:{ \u0026quot;order\u0026quot;:\u0026quot;desc\u0026quot; } }, \u0026quot;from\u0026quot;: 0, \u0026quot;size\u0026quot;: 5 } # user索引创建映射 PUT /user/userinfo/_mapping { \u0026quot;properties\u0026quot;: { \u0026quot;name\u0026quot;:{ \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;ik_smart\u0026quot;, \u0026quot;search_analyzer\u0026quot;: \u0026quot;ik_smart\u0026quot;, \u0026quot;store\u0026quot;: false }, \u0026quot;city\u0026quot;:{ \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;ik_smart\u0026quot;, \u0026quot;search_analyzer\u0026quot;: \u0026quot;ik_smart\u0026quot;, \u0026quot;store\u0026quot;: false }, \u0026quot;age\u0026quot;:{ \u0026quot;type\u0026quot;: \u0026quot;long\u0026quot;, \u0026quot;store\u0026quot;: false }, \u0026quot;description\u0026quot;:{ \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;ik_smart\u0026quot;, \u0026quot;search_analyzer\u0026quot;: \u0026quot;ik_smart\u0026quot;, \u0026quot;store\u0026quot;: false } } } # 更新文档数据 (不保留原有数据) PUT /user/userinfo/4 { \u0026quot;name\u0026quot;: \u0026quot;启祥\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;在广州读书,家在惠州,工作在深圳\u0026quot; } # 删除文档数据 DELETE /user/userinfo/2 # 新增文档数据 id=1 PUT /user/userinfo/1 { \u0026quot;name\u0026quot;:\u0026quot;李四\u0026quot;, \u0026quot;age\u0026quot;:22, \u0026quot;city\u0026quot;:\u0026quot;深圳\u0026quot;, \u0026quot;description\u0026quot;:\u0026quot;李四来自湖北武汉！\u0026quot; } #新增文档数据 id=2 PUT /user/userinfo/2 { \u0026quot;name\u0026quot;:\u0026quot;王五\u0026quot;, \u0026quot;age\u0026quot;:35, \u0026quot;city\u0026quot;:\u0026quot;深圳\u0026quot;, \u0026quot;description\u0026quot;:\u0026quot;王五家住在深圳！\u0026quot; } #新增文档数据 id=3 PUT /user/userinfo/3 { \u0026quot;name\u0026quot;:\u0026quot;张三\u0026quot;, \u0026quot;age\u0026quot;:19, \u0026quot;city\u0026quot;:\u0026quot;深圳\u0026quot;, \u0026quot;description\u0026quot;:\u0026quot;在深圳打工，来自湖北武汉\u0026quot; } #新增文档数据 id=4 PUT /user/userinfo/4 { \u0026quot;name\u0026quot;:\u0026quot;张三丰\u0026quot;, \u0026quot;age\u0026quot;:66, \u0026quot;city\u0026quot;:\u0026quot;武汉\u0026quot;, \u0026quot;description\u0026quot;:\u0026quot;在武汉读书，家在武汉！\u0026quot; } #新增文档数据 id=5 PUT /user/userinfo/5 { \u0026quot;name\u0026quot;:\u0026quot;赵子龙\u0026quot;, \u0026quot;age\u0026quot;:77, \u0026quot;city\u0026quot;:\u0026quot;广州\u0026quot;, \u0026quot;description\u0026quot;:\u0026quot;赵子龙来自深圳宝安，但是在广州工作！\u0026quot;, \u0026quot;address\u0026quot;:\u0026quot;广东省茂名市\u0026quot; } #新增文档数据 id=6 PUT /user/userinfo/6 { \u0026quot;name\u0026quot;:\u0026quot;赵毅\u0026quot;, \u0026quot;age\u0026quot;:55, \u0026quot;city\u0026quot;:\u0026quot;广州\u0026quot;, \u0026quot;description\u0026quot;:\u0026quot;赵毅来自广州白云区，从事电子商务8年！\u0026quot; } #新增文档数据 id=7 PUT /user/userinfo/7 { \u0026quot;name\u0026quot;:\u0026quot;赵哈哈\u0026quot;, \u0026quot;age\u0026quot;:57, \u0026quot;city\u0026quot;:\u0026quot;武汉\u0026quot;, \u0026quot;description\u0026quot;:\u0026quot;武汉赵哈哈，在深圳打工已有半年了，月薪7500！\u0026quot; } 2. DSL查询基本语句 # 查询语句 # 过滤查询 term GET _search { \u0026quot;query\u0026quot;:{ \u0026quot;term\u0026quot;:{ \u0026quot;description\u0026quot;:\u0026quot;广州\u0026quot; } } } # 过滤查询 terms 可以指定多个匹配条件 GET _search { \u0026quot;query\u0026quot;:{ \u0026quot;terms\u0026quot;:{ \u0026quot;city\u0026quot;: [ \u0026quot;武汉\u0026quot;, \u0026quot;广州\u0026quot; ] } } } # 过滤查询 (范围过滤) #gt表示\u0026gt; gte表示=\u0026gt; #lt表示\u0026lt; lte表示\u0026lt;= GET _search { \u0026quot;query\u0026quot;:{ \u0026quot;range\u0026quot;: { \u0026quot;age\u0026quot;: { \u0026quot;gte\u0026quot;: 30, \u0026quot;lte\u0026quot;: 57 } } } } # exits 过滤查询 (可以用来查找具有某个域的数据) GET _search { \u0026quot;query\u0026quot;: { \u0026quot;exists\u0026quot;:{ \u0026quot;field\u0026quot;:\u0026quot;age\u0026quot; } } } # bool 过滤 #must : 多个查询条件的完全匹配,相当于 and。 #must_not : 多个查询条件的相反匹配，相当于 not。 #should : 至少有一个查询条件匹配, 相当于 or。 GET _search { \u0026quot;query\u0026quot;: { \u0026quot;bool\u0026quot;: { \u0026quot;must\u0026quot;: [ { \u0026quot;term\u0026quot;: { \u0026quot;city\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;深圳\u0026quot; } } }, { \u0026quot;range\u0026quot;:{ \u0026quot;age\u0026quot;:{ \u0026quot;gte\u0026quot;:20, \u0026quot;lte\u0026quot;:30 } } } ] } } } #字符串匹配 GET _search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;description\u0026quot;: \u0026quot;惠州\u0026quot; } } } # 前缀匹配查询 GET _search { \u0026quot;query\u0026quot;: { \u0026quot;prefix\u0026quot;: { \u0026quot;name\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;赵\u0026quot; } } } } #多个域匹配搜索 GET _search { \u0026quot;query\u0026quot;: { \u0026quot;multi_match\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;深圳\u0026quot;, \u0026quot;fields\u0026quot;: [ \u0026quot;city\u0026quot;, \u0026quot;description\u0026quot; ] } } } 注意 elasticsearch中本没有修改，它的修改原理是该是先删除再新增修改和新增是同一个接口，区分的依据就是id。\n3. 提升查询子句 Boosting query clause ==bool 查询不仅能合并简单的 match, must等查询, 也能合并其他的查询, 包括其他 bool查询; boost 可以控制搜索权重, 使得满足条件的查询 等分_score 更高, 它们的结果会出现在列表前面==\nGET /_search { \u0026quot;query\u0026quot;: { \u0026quot;bool\u0026quot;: { \u0026quot;must\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;content\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;full text search\u0026quot;, \u0026quot;operator\u0026quot;: \u0026quot;and\u0026quot; } } }, \u0026quot;should\u0026quot;: [ { \u0026quot;match\u0026quot;: { \u0026quot;content\u0026quot;: \u0026quot;Elasticsearch\u0026quot; }}, { \u0026quot;match\u0026quot;: { \u0026quot;content\u0026quot;: \u0026quot;Lucene\u0026quot; }} ] } } } 指定权重\nPost /_search { \u0026quot;query\u0026quot;: { \u0026quot;bool\u0026quot;: { \u0026quot;must\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;content\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;full text search\u0026quot;, \u0026quot;operator\u0026quot;: \u0026quot;and\u0026quot; } } }, \u0026quot;should\u0026quot;: [ { \u0026quot;match\u0026quot;: { \u0026quot;content\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;Elasticsearch\u0026quot;, \u0026quot;boost\u0026quot;: 3 } }}, { \u0026quot;match\u0026quot;: { \u0026quot;content\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;Lucene\u0026quot;, \u0026quot;boost\u0026quot;: 2 } }} ] } } } boost参数被用来增加一个子句的相对权重(当boost大于1时)，或者减小相对权重(当boost介于0到1时)，但是增加或者减小不是线性的 6. 将数据库的数据导入到es中去 springboot整个es有四种方法，分别是TransportClient、RestClient、SpringData-Es、Elasticsearch-SQL。\n这里我们使用SpringData-Es; spring-boot 版本是 2.1.17.RELEASE\n引入依赖 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-elasticsearch\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 配置elasticsearch地址 spring: data: elasticsearch: cluster-name: docker-cluster cluster-nodes: 192.168.182.129:9300 创建存入elasticsearch的 pojo类 (这里选择商标分类 trademarkclass) 加上了Document 注解之后, 默认情况下这个实体类中的所有属性都会被建立索引, 并且分词\n@Document(indexName = \u0026quot;ipr_trademarkclass\u0026quot;, type = \u0026quot;mybunnygirl\u0026quot;) // 这里相当于配置的是库名,表名 public class TrademarkClassInfo { @Id private Integer id; private String ordinal; // 设置类型, 以及使用ik_smart分词器进行分词; 不设置则使用默认的 @Field(type = FieldType.Text, analyzer = \u0026quot;ik_smart\u0026quot;) private String name; @Field(type = FieldType.Text, analyzer = \u0026quot;ik_smart\u0026quot;) private String description; private Integer status; private String remark; // 这里使用localdatetime的 话 查数据的时候会有反序列化异常;使用Date 类型去替代 private LocalDateTime created; private LocalDateTime modified; } ,可以使用 elasticsearch的通用mapper操作es @Repository public interface TrademarkInfoMapper extends ElasticsearchRepository\u0026lt;TrademarkInfo, Integer\u0026gt; { } 同时需要在启动类上扫描elasticsearch的通用mapper 所在的包 @SpringBootApplication @EnableElasticsearchRepositories(basePackages = \u0026quot;com.example.elasticipr.dao.esdao\u0026quot;) @MapperScan(basePackages = \u0026quot;com.example.elasticipr.dao.sqldao\u0026quot;) public class ElasticIprApplication { public static void main(String[] args) { SpringApplication.run(ElasticIprApplication.class, args); } } 将mysql的数据查出来后, 导入到es中\n@Service public class ElasticsearchServiceImpl implements ElasticsearchService { @Autowired private TrademarkClassMapper trademarkClassMapper; @Autowired private TrademarkClassInfoMapper trademarkClassInfoMapper; @Override public void importTrademarkClassToEs() { List\u0026lt;TrademarkClass\u0026gt; trademarkClasses = trademarkClassMapper.selectAll(); trademarkClasses.forEach(System.out::println); String json = JSON.toJSONString(trademarkClasses); List\u0026lt;TrademarkClassInfo\u0026gt; trademarkInfos = JSON.parseArray(json, TrademarkClassInfo.class); trademarkClassInfoMapper.saveAll(trademarkInfos); } } ==导入成功==\n7. 用es进行查询 ==查询description字段 中带材料 关键字的==\n** * @description: 根据关键字搜索 * @author: Qixiang * @date: 2020/9/24 15:55 * @param: searchMap * @return: java.util.Map */ @Override public Map search(Map\u0026lt;String, String\u0026gt; searchMap) { //1.条件构建 NativeSearchQueryBuilder builder = buildBasicQuery(searchMap); //2.搜索列表 Map resultMap = searchList(builder); return resultMap; } /** * 数据搜索 * @param builder * @return */ private Map searchList(NativeSearchQueryBuilder builder){ Map resultMap=new HashMap();//返回结果 //查询解析器 NativeSearchQuery searchQuery = builder.build(); Page\u0026lt;TrademarkClassInfo\u0026gt; trademarkClassInfos = esTemplate.queryForPage(searchQuery,TrademarkClassInfo.class); //存储对应数据 resultMap.put(\u0026quot;rows\u0026quot;, trademarkClassInfos.getContent()); resultMap.put(\u0026quot;totalPages\u0026quot;, trademarkClassInfos.getTotalPages()); return resultMap; } /** * 构建基本查询 * @param searchMap * @return */ private NativeSearchQueryBuilder buildBasicQuery(Map\u0026lt;String,String\u0026gt; searchMap) { // 查询构建器 NativeSearchQueryBuilder nativeSearchQueryBuilder = new NativeSearchQueryBuilder(); if(searchMap!=null){ //1.关键字查询 if(!StringUtils.isEmpty(searchMap.get(\u0026quot;keyword\u0026quot;))){ nativeSearchQueryBuilder.withQuery(QueryBuilders.matchQuery(\u0026quot;description\u0026quot;,searchMap.get(\u0026quot;keyword\u0026quot;))); } } return nativeSearchQueryBuilder; } 8. 打印ES查询执行的sql NativeSearchQuery searchQuery = nativeSearchQueryBuilder.build(); LOGGER.info(\u0026quot;DSL-filter:{}\u0026quot;, searchQuery.getFilter().toString()); LOGGER.info(\u0026quot;DSL-query:{}\u0026quot;, searchQuery.getQuery().toString()); 9. Spring Data ElastiSearch Api 解析 1. ElasticsearchRepository 里面有几个特殊的search方法, 完成特殊查询通常需要的是的 QueryBuilder和 SearchQuery 两个参数\n关系图\n实际使用中, ==主要任务就是要构建NativeSearchQuery== 来完成复杂的查询\n一般情况下使用Builder; 通过\nNativeSearchQueryBuilder builder = new NativeSearchQueryBuilder(); builder.withQuery().withFilter().withSort().withHighlightFields().build(); QueryBuilder 主要用来构建查询条件, 过滤条件; SortBuilder 主要是构建排序\n2. ElasticSearchTemplate 更多是对ESRepository的补充，里面提供了一些更底层的方法\nES提供了批量插入数据的功能——bulk。 可以迅速插入百万级的数据。\n==mapper.saveAll 就是用了 bulkindex==\npublic void bulkIndex(List\u0026lt;IndexQuery\u0026gt; queries) { BulkRequestBuilder bulkRequest = this.client.prepareBulk(); Iterator var3 = queries.iterator(); while(var3.hasNext()) { IndexQuery query = (IndexQuery)var3.next(); bulkRequest.add(this.prepareIndex(query)); } BulkResponse bulkResponse = (BulkResponse)bulkRequest.execute().actionGet(); if (bulkResponse.hasFailures()) { Map\u0026lt;String, String\u0026gt; failedDocuments = new HashMap(); BulkItemResponse[] var5 = bulkResponse.getItems(); int var6 = var5.length; for(int var7 = 0; var7 \u0026lt; var6; ++var7) { BulkItemResponse item = var5[var7]; if (item.isFailed()) { failedDocuments.put(item.getId(), item.getFailureMessage()); } } throw new ElasticsearchException(\u0026quot;Bulk indexing has failures. Use ElasticsearchException.getFailedDocuments() for detailed messages [\u0026quot; + failedDocuments + \u0026quot;]\u0026quot;, failedDocuments); } } public void bulkUpdate(List\u0026lt;UpdateQuery\u0026gt; queries) { BulkRequestBuilder bulkRequest = this.client.prepareBulk(); Iterator var3 = queries.iterator(); while(var3.hasNext()) { UpdateQuery query = (UpdateQuery)var3.next(); bulkRequest.add(this.prepareUpdate(query)); } BulkResponse bulkResponse = (BulkResponse)bulkRequest.execute().actionGet(); if (bulkResponse.hasFailures()) { Map\u0026lt;String, String\u0026gt; failedDocuments = new HashMap(); BulkItemResponse[] var5 = bulkResponse.getItems(); int var6 = var5.length; for(int var7 = 0; var7 \u0026lt; var6; ++var7) { BulkItemResponse item = var5[var7]; if (item.isFailed()) { failedDocuments.put(item.getId(), item.getFailureMessage()); } } throw new ElasticsearchException(\u0026quot;Bulk indexing has failures. Use ElasticsearchException.getFailedDocuments() for detailed messages [\u0026quot; + failedDocuments + \u0026quot;]\u0026quot;, failedDocuments); } } public void bulkIndex(List\u0026lt;Person\u0026gt; personList) { int counter = 0; try { if (!elasticsearchTemplate.indexExists(PERSON_INDEX_NAME)) { elasticsearchTemplate.createIndex(PERSON_INDEX_TYPE); } List\u0026lt;IndexQuery\u0026gt; queries = new ArrayList\u0026lt;\u0026gt;(); for (Person person : personList) { IndexQuery indexQuery = new IndexQuery(); indexQuery.setId(person.getId() + \u0026quot;\u0026quot;); indexQuery.setObject(person); indexQuery.setIndexName(PERSON_INDEX_NAME); indexQuery.setType(PERSON_INDEX_TYPE); //上面的那几步也可以使用IndexQueryBuilder来构建 //IndexQuery index = new IndexQueryBuilder().withId(person.getId() + \u0026quot;\u0026quot;).withObject(person).build(); queries.add(indexQuery); if (counter % 500 == 0) { elasticsearchTemplate.bulkIndex(queries); queries.clear(); System.out.println(\u0026quot;bulkIndex counter : \u0026quot; + counter); } counter++; } if (queries.size() \u0026gt; 0) { elasticsearchTemplate.bulkIndex(queries); } System.out.println(\u0026quot;bulkIndex completed.\u0026quot;); } catch (Exception e) { System.out.println(\u0026quot;IndexerService.bulkIndex e;\u0026quot; + e.getMessage()); throw e; } } 常用 Search queries. QueryBuilders.matchPhraseQuery(\u0026quot;\u0026quot;, \u0026quot;\u0026quot;).slop(2); QueryBuilders.termQuery(); // 不进行分词的; 而term一般适用于做过滤器filter的情况，譬如我们去查询title中包含“浣溪沙”且userId=1时，那么就可以用termQuery(\u0026quot;userId\u0026quot;, 1)作为查询的filter QueryBuilders.rangeQuery().gte().lte(); QueryBuilders.matchPhraseQuery().slop(); QueryBuilders.multiMatchQuery().type(Type.PHRASE_PREFIX); // slop: 查询词条能够相隔多远时仍然将文档视为匹配 // PHRASE_PREFIX ; // PHRASE // CROSS_FIELDS 希望这个词条的分词词汇是分配到不同字段中的 // MOST_FIELDS 越多字段匹配的文档分越高 // BEST_FIELDS 完全匹配的文档占的评分比较高 1 查询上下文：查询操作不仅仅会进行查询，还会计算分值，用于确定相关度； 2 过滤器上下文：查询操作仅判断是否满足查询条件，不会计算得分，查询的结果可以被缓存。 所以，根据实际的需求是否需要获取得分，考虑性能因素，选择不同的查询子句。\n完全包含查询 如果不希望ES进行分词, 我们需要配置一下Operator\nmatchQuery，multiMatchQuery，queryStringQuery等，都可以设置operator。默认为Or，设置为And后，就会把符合包含所有输入的才查出来。 如果是and的话，譬如用户输入了5个词，但包含了4个，也是显示不出来的。我们可以通过设置精度来控制\nQueryBuilders.multiMatchQuery().type(Type.PHRASE_PREFIX).operator(Operator.AND).minimumShouldMatch(\u0026quot;75\u0026quot;); // minimumShouldMatch 最少匹配了多少百分比的会查询出来 合并查询 boolQuery, 可以设置多个条件的查询方式, 它的作用是用来组合多个Query, 支持四种组合 must, mustnot filter, should\nmust代表返回的文档必须满足must子句的条件，会参与计算分值； filter代表返回的文档必须满足filter子句的条件，但不会参与计算分值； should代表返回的文档可能满足should子句的条件，也可能不满足，有多个should时满足任何一个就可以，通过minimum_should_match设置至少满足几个。 mustnot代表必须不满足子句的条件。 聚合查询 参考资料: https://blog.csdn.net/winterking3/article/details/83785750\n10 . 搭建 ES集群 参考 day2 拓展\n","date":"2022-09-03T14:17:25+08:00","permalink":"https://mikeLing-qx.github.io/p/elasticsearch%E5%85%A5%E9%97%A8/","title":"ElasticSearch入门"},{"content":"分布式锁 学习目标 什么是分布式 什么是锁 什么是分布式锁 分布式锁的使用场景-为什么要使用分布式锁 分布式锁需要具备哪些功能/条件 分布式锁的解决方案 1 分布式锁介绍 1.1 什么是分布式 一个大型的系统往往被分为几个子系统来做，一个子系统可以部署在一台机器的多个 JVM(java虚拟机) 上，也可以部署在多台机器上。但是每一个系统不是独立的，不是完全独立的。需要相互通信，共同实现业务功能。\n一句话来说：分布式就是通过计算机网络将后端工作分布到多台主机上，多个主机一起协同完成工作。\n1.2 什么是锁\u0026ndash;作用安全 现实生活中，当我们需要保护一样东西的时候，就会使用锁。例如门锁，车锁等等。很多时候可能许多人会共用这些资源，就会有很多个钥匙。但是有些时候我们希望使用的时候是独自不受打扰的，那么就会在使用的时候从里面反锁，等使用完了再从里面解锁。这样其他人就可以继续使用了。\nJAVA程序中，当存在多个线程可以同时改变某个变量（可变共享变量）时，就需要对变量或代码块做同步，使其在修改这种变量时能够线性执行消除并发修改变量，而同步的本质是通过锁来实现的。如 Java 中 synchronize 是在对象头设置标记，Lock 接口的实现类基本上都只是某一个 volitile 修饰的 int 型变量其保证每个线程都能拥有对该 int 的可见性和原子修改\n1.4 什么是分布式锁 任何一个分布式系统都无法同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance），最多只能同时满足两项。CAP\n当在分布式模型下，数据只有一份（或有限制），此时需要利用锁的技术控制某一时刻修改数据的进程数。\n分布式锁: 在分布式环境下，多个程序/线程都需要对某一份(或有限制)的数据进行修改时，针对程序进行控制，保证同一时间节点下，只有一个程序/线程对数据进行操作的技术。\n1.5 分布式锁的真实使用场景 场景一：\n场景二：\n1.5 分布式锁的执行流程 1.6 分布式锁具备的条件 互斥性：同一时刻只能有一个服务(或应用)访问资源，特殊情况下有读写锁 原子性：一致性要求保证加锁和解锁的行为是原子性的 安全性：锁只能被持有该锁的服务(或应用)释放 容错性：在持有锁的服务崩溃时，锁仍能得到释放避免死锁 可重用性：同一个客户端获得锁后可递归调用\u0026mdash;重入锁和不可重入锁 公平性：看业务是否需要公平，避免饿死\u0026ndash;公平锁和非公平锁 支持阻塞和非阻塞：和 ReentrantLock 一样支持 lock 和 trylock 以及 tryLock(long timeOut)\u0026mdash;阻塞锁和非阻塞锁==PS:::自选锁== 高可用：获取锁和释放锁 要高可用 高性能：获取锁和释放锁的性能要好 持久性：锁按业务需要自动续约/自动延期 2.分布式锁的解决方案 2.1 数据库实现分布式锁 2.1.1 基于数据库表实现 准备工作：创建tb_program表，用于记录当前哪个程序正在使用数据\nCREATE TABLE `tb_program` (\r`program_no` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL COMMENT '程序的编号'\rPRIMARY KEY (`program_no`) USING BTREE\r) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Compact;\r实现步骤：\n程序访问数据时，将程序的编号（insert）存入tb_program表； 当insert成功，代表该程序获得了锁，即可执行逻辑； 当program_no相同的其他程序进行insert是，由于主键冲突会导致insert失败，则代表获取锁失败； 获取锁成功的程序在逻辑执行完以后，删除该数据,代表释放锁。 2.1.2 基于数据库的排他锁实现 除了可以通过增删操作数据表中的记录以外，其实还可以借助数据中自带的锁来实现分布式的锁。我们还用刚刚创建的那张数据库表，基于MySql的InnoDB引擎(MYSQL的引擎种类)可以通过数据库的排他锁来实现分布式锁。\n实现步骤：\n在查询语句后面增加for update，数据库会在查询过程中给数据库表增加排他锁。当某条记录被加上排他锁之后，其他线程无法再在该行记录上增加排他锁 获得排它锁的线程即可获得分布式锁，执行方法的业务逻辑 执行完方法之后，再通过connection.commit();操作来释放锁。 实现代码\npom.xml\n\u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;UTF-8\u0026quot;?\u0026gt;\r\u0026lt;project xmlns=\u0026quot;http://maven.apache.org/POM/4.0.0\u0026quot;\rxmlns:xsi=\u0026quot;http://www.w3.org/2001/XMLSchema-instance\u0026quot;\rxsi:schemaLocation=\u0026quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026quot;\u0026gt;\r\u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt;\r\u0026lt;groupId\u0026gt;com.itheima\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;mysql-demo\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt;\r\u0026lt;!--依赖包--\u0026gt;\r\u0026lt;dependencies\u0026gt;\r\u0026lt;!-- MySql --\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;5.1.32\u0026lt;/version\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;!-- Test dependencies --\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;junit\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;junit\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;4.12\u0026lt;/version\u0026gt;\r\u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;/dependencies\u0026gt;\r\u0026lt;/project\u0026gt;\rBook\npublic class Book {\r// 图书ID\rprivate Integer id;\r// 图书名称\rprivate String name;\r// 图书价格\rprivate Float price;\r// 图书图片\rprivate String pic;\r// 图书描述\rprivate String desc;\r}\rBookDao\npublic interface BookDao {\r/**\r* 查询所有的book数据\r* @return\r*/\rList\u0026lt;Book\u0026gt; queryBookList(String name) throws Exception;\r}\rBookDaoImpl实现类\npublic class BookDaoImpl implements BookDao {\r/***\r* 查询数据库数据\r* @return\r* @throws Exception\r*/\rpublic List\u0026lt;Book\u0026gt; queryBookList(String name) throws Exception{\r// 数据库链接\rConnection connection = null;\r// 预编译statement\rPreparedStatement preparedStatement = null;\r// 结果集\rResultSet resultSet = null;\r// 图书列表\rList\u0026lt;Book\u0026gt; list = new ArrayList\u0026lt;Book\u0026gt;();\rtry {\r// 加载数据库驱动\rClass.forName(\u0026quot;com.mysql.jdbc.Driver\u0026quot;);\r// 连接数据库\rconnection = DriverManager.getConnection(\u0026quot;jdbc:mysql://39.108.189.37:3306/lucene\u0026quot;, \u0026quot;ybbmysql\u0026quot;, \u0026quot;ybbmysql\u0026quot;);\r//关闭自动提交\rconnection.setAutoCommit(false);\r// SQL语句\rString sql = \u0026quot;SELECT * FROM book where id = 1 for update\u0026quot;;\r// 创建preparedStatement\rpreparedStatement = connection.prepareStatement(sql);\r// 获取结果集\rresultSet = preparedStatement.executeQuery();\r// 结果集解析\rwhile (resultSet.next()) {\rBook book = new Book();\rbook.setId(resultSet.getInt(\u0026quot;id\u0026quot;));\rbook.setName(resultSet.getString(\u0026quot;name\u0026quot;));\rlist.add(book);\r}\rSystem.out.println(name + \u0026quot;执行了for update\u0026quot;);\rSystem.out.println(\u0026quot;结果为:\u0026quot; + list);\r//锁行后休眠5秒\rThread.sleep(5000);\r//休眠结束释放\rconnection.commit();\rSystem.out.println(name + \u0026quot;结束\u0026quot;);\r} catch (Exception e) {\re.printStackTrace();\r}\rreturn list;\r}\r}\r测试类\npublic class Test {\rprivate BookDao bookDao = new BookDaoImpl();\r@org.junit.Test\rpublic void testLock() throws Exception {\rnew Thread(new LockRunner(\u0026quot;线程1\u0026quot;)).start();\rnew Thread(new LockRunner(\u0026quot;线程2\u0026quot;)).start();\rnew Thread(new LockRunner(\u0026quot;线程3\u0026quot;)).start();\rnew Thread(new LockRunner(\u0026quot;线程4\u0026quot;)).start();\rnew Thread(new LockRunner(\u0026quot;线程5\u0026quot;)).start();\rThread.sleep(200000L);\r}\rclass LockRunner implements Runnable {\rprivate String name;\rpublic LockRunner(String name) {\rthis.name = name;\r}\rpublic void run() {\rtry {\rbookDao.queryBookList(name);\r}catch (Exception e){\re.printStackTrace();\r}\r}\r}\r}\r执行结果\n2.1.3 优点及缺点 **优点：**简单，方便，快速实现\n**缺点：**基于数据库，开销比较大，对数据库性能可能会存在影响，服务数量比较多的情况下，数据库也要做集群，使用数据库加锁，锁状态不能同步到其他机器上面，使用insert的方式有一个同步的过程，如果访问从机是可以加到锁的。\n2.2 Redis实现分布式锁 2.2.1 基于 REDIS 的 SETNX()、EXPIRE() 、GETSET()方法做分布式锁 实现原理：\nsetnx():setnx 的含义就是 SET if Not Exists，其主要有两个参数 setnx(key, value)。该方法是原子的，如果 key 不存在，则设置当前 key 成功，返回 1；如果当前 key 已经存在，则设置当前 key 失败，返回 0\rexpire():expire 设置过期时间，要注意的是 setnx 命令不能设置 key 的超时时间，只能通过 expire() 来对 key 设置。\rgetset():这个命令主要有两个参数 getset(key，newValue)。该方法是原子的，对 key 设置 newValue 这个值，并且返回 key 原来的旧值。假设 key 原来是不存在的，那么多次执行这个命令，会出现下边的效果：\rgetset(key, “value1”) 返回 null 此时 key 的值会被设置为 value1\rgetset(key, “value2”) 返回 value1 此时 key 的值会被设置为 value2\r实现流程：\nsetnx(lockkey, 当前时间+过期超时时间)，如果返回 1，则获取锁成功；如果返回 0 则没有获取到锁。 get(lockkey) 获取值 oldExpireTime ，并将这个 value 值与当前的系统时间进行比较，如果小于当前系统时间，则认为这个锁已经超时，可以允许别的请求重新获取。 计算 newExpireTime = 当前时间+过期超时时间，然后 getset(lockkey, newExpireTime) 会返回当前 lockkey 的值currentExpireTime。判断 currentExpireTime 与 oldExpireTime 是否相等，如果相等，说明当前 getset 设置成功，获取到了锁。如果不相等，说明这个锁又被别的请求获取走了，那么当前请求可以直接返回失败，或者继续重试。 在获取到锁之后，当前线程可以开始自己的业务处理，当处理完毕后，比较自己的处理时间和对于锁设置的超时时间，如果小于锁设置的超时时间，则直接执行 delete 释放锁；如果大于锁设置的超时时间，则不需要再锁进行处理。 思路: 如果处理时间大于锁设置的超时时间, 需要先去拿到锁的预期的超时时间 和现在的锁的超时时间, 如果不一致, 说明业务处理超时的时候 锁被别的程序获取了, 所以这次的操作只能进行rollback 在Linux虚拟机中创建redis容器\ndocker run -di \u0026ndash;name=tensquare_redis -p 6379:6379 redis\n代码实现\npom.xml文件\n\u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;UTF-8\u0026quot;?\u0026gt;\r\u0026lt;project xmlns=\u0026quot;http://maven.apache.org/POM/4.0.0\u0026quot; xmlns:xsi=\u0026quot;http://www.w3.org/2001/XMLSchema-instance\u0026quot;\rxsi:schemaLocation=\u0026quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\u0026quot;\u0026gt;\r\u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt;\r\u0026lt;groupId\u0026gt;com.itheima\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;redis\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt;\r\u0026lt;name\u0026gt;redis\u0026lt;/name\u0026gt;\r\u0026lt;description\u0026gt;redis实现分布式锁测试\u0026lt;/description\u0026gt;\r\u0026lt;parent\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;2.1.4.RELEASE\u0026lt;/version\u0026gt;\r\u0026lt;relativePath/\u0026gt; \u0026lt;!-- lookup parent from repository --\u0026gt;\r\u0026lt;/parent\u0026gt;\r\u0026lt;properties\u0026gt;\r\u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt;\r\u0026lt;/properties\u0026gt;\r\u0026lt;dependencies\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-data-redis\u0026lt;/artifactId\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt;\r\u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt;\r\u0026lt;exclusions\u0026gt;\r\u0026lt;exclusion\u0026gt;\r\u0026lt;groupId\u0026gt;org.junit.vintage\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;junit-vintage-engine\u0026lt;/artifactId\u0026gt;\r\u0026lt;/exclusion\u0026gt;\r\u0026lt;/exclusions\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;/dependencies\u0026gt;\r\u0026lt;build\u0026gt;\r\u0026lt;plugins\u0026gt;\r\u0026lt;plugin\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt;\r\u0026lt;/plugin\u0026gt;\r\u0026lt;/plugins\u0026gt;\r\u0026lt;/build\u0026gt;\r\u0026lt;/project\u0026gt;\rRedisUtil工具类\n@Component\rpublic class RedisUtil {\r//定义默认超时时间:单位毫秒\rprivate static final Integer LOCK_TIME_OUT = 10000;\r@Autowired\rprivate StringRedisTemplate stringRedisTemplate;\r/**\r* 外部调用加锁方法\r*/\rpublic Boolean tryLock(String key, Long timeout) throws Exception{\r//获取当前系统时间设置为开始时间\rLong startTime = System.currentTimeMillis();\r//设置返回默认值-false:加锁失败\rboolean flag = false;\r//死循环获取锁:1.获取锁成功退出 2.获取锁超时退出\rwhile(true){\r//判断是否超时\rif((System.currentTimeMillis() - startTime) \u0026gt;= timeout){\rbreak;\r}else{\r//获取锁\rflag = lock(key);\r//判断是否获取成功\rif(flag){\rbreak;\r}else{\r//休息0.1秒重试,降低服务压力\rThread.sleep(100);\r}\r}\r}\rreturn flag;\r}\r/**\r* 加锁实现\r* @param key\r* @return\r*/\rprivate Boolean lock(String key){\rreturn (Boolean) stringRedisTemplate.execute((RedisCallback) redisConnection -\u0026gt; {\r//获取当前系统时间\rLong time = System.currentTimeMillis();\r//设置锁超时时间\rLong timeout = time + LOCK_TIME_OUT + 1;\r//setnx加锁并获取解锁结果\rBoolean result = redisConnection.setNX(key.getBytes(), String.valueOf(timeout).getBytes());\r//加锁成功返回true\rif(result){\rreturn true;\r}\r//加锁失败判断锁是否超时\rif(checkLock(key, timeout)){\r//getset设置值成功后,会返回旧的锁有效时间\rbyte[] newtime = redisConnection.getSet(key.getBytes(), String.valueOf(timeout).getBytes());\rif(time \u0026gt; Long.valueOf(new String(newtime))){\rreturn true;\r}\r}\r//默认加锁失败\rreturn false;\r});\r}\r/**\r* 释放锁\r*/\rpublic Boolean release(String key){\rreturn (Boolean) stringRedisTemplate.execute((RedisCallback) redisConnection -\u0026gt; {\rLong del = redisConnection.del(key.getBytes());\rif (del \u0026gt; 0){\rreturn true;\r}\rreturn false;\r});\r}\r/**\r* 判断锁是否超时\r*/\rprivate Boolean checkLock(String key, Long timeout){\rreturn (Boolean) stringRedisTemplate.execute((RedisCallback) redisConnection -\u0026gt; {\r//获取锁的超时时间\rbyte[] bytes = redisConnection.get(key.getBytes());\rtry {\r//判断锁的有效时间是否大与当前时间\rif(timeout \u0026gt; Long.valueOf(new String(bytes))){\rreturn true;\r}\r}catch (Exception e){\re.printStackTrace();\rreturn false;\r}\rreturn false;\r});\r}\r}\rRedisController测试类\n@RestController\r@RequestMapping(value = \u0026quot;/redis\u0026quot;)\rpublic class RedisController {\r@Autowired\rprivate RedisUtil redisUtil;\r/**\r* 获取锁\r* @return\r*/\r@GetMapping(value = \u0026quot;/lock/{name}\u0026quot;)\rpublic String lock(@PathVariable(value = \u0026quot;name\u0026quot;)String name) throws Exception{\rBoolean result = redisUtil.tryLock(name, 3000L);\rif(result){\rreturn \u0026quot;获取锁成功\u0026quot;;\r}\rreturn \u0026quot;获取锁失败\u0026quot;;\r}\r/**\r* 释放锁\r* @param name\r*/\r@GetMapping(value = \u0026quot;/unlock/{name}\u0026quot;)\rpublic String unlock(@PathVariable(value = \u0026quot;name\u0026quot;)String name){\rBoolean result = redisUtil.release(name);\rif(result){\rreturn \u0026quot;释放锁成功\u0026quot;;\r}\rreturn \u0026quot;释放锁失败\u0026quot;;\r}\r}\r2.2.2 redis锁的key键如何选择 如果锁的key是一个定值，意味着所有加锁的线程访问同一把锁，效率非常低下 所以锁的key最好不要是定值而是和业务相关的 ex： 业务逻辑是和商品相关的，加锁的目的是为了保证商品数据修改的原子性 锁的key应该是和商品id相关联 锁得名称应该如何设计 应该是和当前的服务还有业务逻辑相关联的 ex： 商品服务下的商品加锁：锁的key可以设计为=\u0026gt;shop:mer:ID:1 2.2.3 优点及缺点 优点：性能极高\n缺点：失效时间设置没有定值。设置的失效时间太短，方法没等执行完锁就自动释放了，那么就会产生并发问题。如果设置的时间太长，如果加锁的服务奔溃了，其他获取锁的线程就可能要平白的多等一段时间，用户体验会降低。\n2.3 zookeeper实现分布式锁 2.3.1 zookeeper 锁相关基础知识 zookeeper 一般由多个节点构成（单数），采用 zab 一致性协议。因此可以将 zk 看成一个单点结构，对其修改数据其内部自动将所有节点数据进行修改而后才提供查询服务。 zookeeper 的数据以目录树的形式，每个目录称为 znode， znode 中可存储数据（一般不超过 1M），还可以在其中增加子节点。 子节点有三种类型。序列化节点，每在该节点下增加一个节点自动给该节点的名称上自增。临时节点，一旦创建这个 znode 的客户端与服务器失去联系，这个 znode 也将自动删除。最后就是普通节点。 Watch 机制，client 可以监控每个节点的变化，当产生变化会给 client 产生一个事件。 2.3.2 zookeeper 分布式锁的原理 获取和释放锁原理：利用临时节点与 watch 机制。每个锁占用一个普通节点 /lock，当需要获取锁时在 /lock 目录下创建一个临时节点，创建成功则表示获取锁成功，失败则 watch/lock 节点，有删除操作后再去争锁。临时节点好处在于当进程挂掉后能自动上锁的节点自动删除即取消锁。\n获取锁的顺序原理：上锁为创建临时有序节点，每个上锁的节点均能创建节点成功，只是其序号不同。只有序号最小的可以拥有锁，如果这个节点序号不是最小的则 watch 序号比本身小的前一个节点 (公平锁)。\n2.3.2 zookeeper实现分布式锁流程 简易流程\n获取锁流程：\n先有一个锁根节点，lockRootNode，这可以是一个永久的节点 客户端获取锁，先在 lockRootNode 下创建一个顺序的临时节点，保证客户端断开连接，节点也自动删除 调用 lockRootNode 父节点的 getChildren() 方法，获取所有的节点，并从小到大排序，如果创建的最小的节点是当前节点，则返回 true,获取锁成功，否则，关注比自己序号小的节点的释放动作(exist watch)，这样可以保证每一个客户端只需要关注一个节点，不需要关注所有的节点，避免羊群效应。 如果有节点释放操作，重复步骤 3 释放锁流程：\n只需要删除步骤 2 中创建的节点即可\n2.3.2 优点及缺点 优点：\n客户端如果出现宕机故障的话，锁可以马上释放 可以实现阻塞式锁，通过 watcher 监听，实现起来也比较简单 集群模式，稳定性比较高 缺点：\n一旦网络有任何的抖动，Zookeeper 就会认为客户端已经宕机，就会断掉连接，其他客户端就可以获取到锁。 性能不高，因为每次在创建锁和释放锁的过程中，都要动态创建、销毁临时节点来实现锁功能。ZK 中创建和删除节点只能通过 Leader 服务器来执行，然后将数据同步到所有的 Follower 机器上。(zookeeper对外提供服务的只有leader) 2.4 consul实现分布式锁(eureka/Register:保存服务的IP 端口 服务列表) 2.4.1 实现原理及流程 基于Consul注册中心的Key/Value存储来实现分布式锁以及信号量的方法主要利用Key/Value存储API中的acquire和release操作来实现。acquire和release操作是类似Check-And-Set的操作：\nacquire操作只有当锁不存在持有者时才会返回true，并且set设置的Value值，同时执行操作的session会持有对该Key的锁，否则就返回false\rrelease操作则是使用指定的session来释放某个Key的锁，如果指定的session无效，那么会返回false，否则就会set设置Value值，并返回true\r实现流程\n实现步骤：\n客户端创建会话session，得到sessionId； 使用acquire设置value的值，若acquire结果为false，代表获取锁失败； acquire结果为true，代表获取锁成功，客户端执行业务逻辑； 客户端业务逻辑执行完成后，执行release操作释放锁； 销毁当前session，客户端连接断开。 代码:\ndocker安装consul\n下载镜像\ndocker pull consul\n开启容器\ndocker run -p 8500:8500/tcp consul agent -server -ui -bootstrap-expect=1 -client=0.0.0.0\n启动consul命令: consul agent -dev\npom.xml文件\n\u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;UTF-8\u0026quot;?\u0026gt;\r\u0026lt;project xmlns=\u0026quot;http://maven.apache.org/POM/4.0.0\u0026quot; xmlns:xsi=\u0026quot;http://www.w3.org/2001/XMLSchema-instance\u0026quot;\rxsi:schemaLocation=\u0026quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\u0026quot;\u0026gt;\r\u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt;\r\u0026lt;parent\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;2.2.5.RELEASE\u0026lt;/version\u0026gt;\r\u0026lt;relativePath/\u0026gt; \u0026lt;!-- lookup parent from repository --\u0026gt;\r\u0026lt;/parent\u0026gt;\r\u0026lt;groupId\u0026gt;com.example\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;demo-consul\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt;\r\u0026lt;name\u0026gt;demo-consul\u0026lt;/name\u0026gt;\r\u0026lt;description\u0026gt;Demo project for Spring Boot\u0026lt;/description\u0026gt;\r\u0026lt;properties\u0026gt;\r\u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt;\r\u0026lt;spring-cloud.version\u0026gt;Hoxton.SR3\u0026lt;/spring-cloud.version\u0026gt;\r\u0026lt;/properties\u0026gt;\r\u0026lt;dependencies\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-cloud-starter-consul-discovery\u0026lt;/artifactId\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt;\r\u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt;\r\u0026lt;exclusions\u0026gt;\r\u0026lt;exclusion\u0026gt;\r\u0026lt;groupId\u0026gt;org.junit.vintage\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;junit-vintage-engine\u0026lt;/artifactId\u0026gt;\r\u0026lt;/exclusion\u0026gt;\r\u0026lt;/exclusions\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;/dependencies\u0026gt;\r\u0026lt;dependencyManagement\u0026gt;\r\u0026lt;dependencies\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-cloud-dependencies\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;${spring-cloud.version}\u0026lt;/version\u0026gt;\r\u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt;\r\u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;/dependencies\u0026gt;\r\u0026lt;/dependencyManagement\u0026gt;\r\u0026lt;build\u0026gt;\r\u0026lt;plugins\u0026gt;\r\u0026lt;plugin\u0026gt;\r\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt;\r\u0026lt;/plugin\u0026gt;\r\u0026lt;/plugins\u0026gt;\r\u0026lt;/build\u0026gt;\r\u0026lt;/project\u0026gt;\rpublic class ConsulUtil {\rprivate ConsulClient consulClient;\rprivate String sessionId = null;\r/**\r* 构造函数\r*/\rpublic ConsulUtil(ConsulClient consulClient) {\rthis.consulClient = consulClient;\r}\r/**\r* 创建session\r*/\rprivate String createSession(String name, Integer ttl){\rNewSession newSession = new NewSession();\r//设置锁有效时长\rnewSession.setTtl(ttl + \u0026quot;s\u0026quot;);\r//设置锁名字\rnewSession.setName(name);\rString value = consulClient.sessionCreate(newSession, null).getValue();\rreturn value;\r}\r/**\r* 获取锁\r*/\rpublic Boolean lock(String name, Integer ttl){\r//定义获取标识\rBoolean flag = false;\r//创建session\rsessionId = createSession(name, ttl);\r//死循环获取锁\rwhile (true){\r//执行acquire操作\rPutParams putParams = new PutParams();\rputParams.setAcquireSession(sessionId);\rflag = consulClient.setKVValue(name, \u0026quot;local\u0026quot; + System.currentTimeMillis(), putParams).getValue();\rif(flag){\rbreak;\r}\r}\rreturn flag;\r}\r/**\r* 释放锁\r*/\rpublic Boolean release(String name){\r//执行acquire操作\rPutParams putParams = new PutParams();\rputParams.setReleaseSession(sessionId);\rBoolean value = consulClient.setKVValue(name, \u0026quot;local\u0026quot; + System.currentTimeMillis(), putParams).getValue();\rreturn value;\r}\r测试代码:\n@SpringBootTest\rclass DemoApplicationTests {\r@Test\rpublic void testLock() throws Exception {\rLoggerContext loggerContext= (LoggerContext) LoggerFactory.getILoggerFactory();\r//设置全局日志级别\rch.qos.logback.classic.Logger logger=loggerContext.getLogger(\u0026quot;root\u0026quot;);\rlogger.setLevel(Level.toLevel(\u0026quot;info\u0026quot;));\rnew Thread(new LockRunner(\u0026quot;线程1\u0026quot;)).start();\rnew Thread(new LockRunner(\u0026quot;线程2\u0026quot;)).start();\rnew Thread(new LockRunner(\u0026quot;线程3\u0026quot;)).start();\rnew Thread(new LockRunner(\u0026quot;线程4\u0026quot;)).start();\rnew Thread(new LockRunner(\u0026quot;线程5\u0026quot;)).start();\rThread.sleep(200000L);\r}\rclass LockRunner implements Runnable {\rprivate String name;\rpublic LockRunner(String name) {\rthis.name = name;\r}\r@Override\rpublic void run() {\rConsulUtil lock = new ConsulUtil(new ConsulClient());\rtry {\rif (lock.lock(\u0026quot;test\u0026quot;, 10)) {\rSystem.out.println(name + \u0026quot;获取到了锁\u0026quot;);\r//持有锁5秒\rThread.sleep(5000);\r//释放锁\rlock.release(\u0026quot;test\u0026quot;);\rSystem.out.println(name + \u0026quot;释放了锁\u0026quot;);\r}\r} catch (Exception e) {\re.printStackTrace();\r}\r}\r}\r}\r结果\n2.4.2 优点及缺点 **优点：**基于consul注册中心即可实现分布式锁，实现简单、方便、快捷\n缺点：\nlock delay：consul实现分布式锁存在延迟，一个节点释放锁了，另一个节点不能立马拿到锁。需要等待lock delay时间后才可以拿到锁。 高负载的场景下，不能及时的续约，导致session timeout, 其他节点拿到锁。 ","date":"2022-08-03T19:53:40+08:00","permalink":"https://mikeLing-qx.github.io/p/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/","title":"分布式锁"},{"content":"分布式事务 学习目标 什么是事务 什么是分布式事务 分布式事务的产生过程 ==什么是CAP定理（面试）== ==分布式事务解决方案（面试）== 1 分布式事务介绍 1.1 什么是事务 数据库事务(简称：事务，Transaction)是指数据库执行过程中的一个逻辑单位，由一个有限的数据库操作序列构成[由当前业务逻辑多个不同操作构成]。\n事务拥有以下四个特性，习惯上被称为ACID特性：\n原子性(Atomicity)：事务作为一个整体被执行，包含在其中的对数据库的操作要么全部被执行，要么都不执行。==记录之前的版本，允许回滚。==\n一致性(Consistency)：一致性是指事务使得系统从一个一致的状态转换到另一个一致状态。事务的一致性决定了一个系统设计和实现的复杂度，也导致了事务的不同隔离级别。==事务开始和结束之间的中间状态不会被其他事务看到。==\n隔离性(Isolation)：多个事务并发执行时，并发事务之间互相影响的程度，比如一个事务会不会读取到另一个未提交的事务修改的数据。==适当的破坏一致性来提升性能与并行度==\n持久性(Durability)：已被提交的事务对数据库的修改应该永久保存在数据库中。==每一次的事务提交后就会保证不会丢失。==\n延申拓展：\n事务隔离性(面试)：\n脏读：事务A修改了一个数据，但未提交，事务B读到了事务A未提交的更新结果，如果事务A提交失败，事务B读到的就是脏数据。 幻读：在同一个事务中，同一个查询多次返回的结果不一致。事务A新增了一条记录，事务B在事务A==新增==提交前后各执行了一次查询操作，发现后一次比前一次多了一条记录。幻读是由于并发事务增加记录导致的，这个不能像不可重复读通过记录加锁解决，因为对于新增的记录根本无法加锁。 不可重复读：在同一个事务中，对于同一份数据读取到的结果不一致。比如，事务B在事务A==更新或删除==提交前读到的结果，和提交后读到的结果可能不同。不可重复读出现的原因就是事务并发修改记录，要避免这种情况，最简单的方法就是对要修改的记录加锁，这会导致锁竞争加剧，影响性能。 事务的隔离级别(面试)：\nRead Uncommitted(读未提交)：最低的隔离级别，什么都不需要做，一个事务可以读到另一个事务未提交的结果。所有的并发事务问题都会发生。 Read Committed(读已提交)：只有在事务提交后，其更新结果才会被其他事务看见。可以解决脏读问题。 Repeated Read(可重复读)：在一个事务中，对于同一份数据的读取结果总是相同的，无论是否有其他事务对这份数据进行操作，以及这个事务是否提交。可以解决脏读、不可重复读。 Serialization：事务串行化执行，隔离级别最高，牺牲了系统的并发性。可以解决并发事务的所有问题。 自行复习/了解：spring事务的传播机制(spring事务面试必问)\n1.2 什么是分布式事务 分布式事务指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点之上，且属于不同的应用，分布式事务需要保证这些操作要么全部成功，要么全部失败。本质上来说，分布式事务就是为了保证不同数据库的数据一致性。\n1.3 事务的演变 1.3.1 单服务单数据库的本地事务 事务仅限于对单一数据库资源的访问控制,架构服务化以后，事务的概念延伸到了服务中。倘若将一个单一的服务操作作为一个事务，那么整个服务操作只能涉及一个单一的数据库资源,这类基于单个服务单一数据库资源访问的事务，被称为本地事务(Local Transaction)。\n1.3.2 单一服务多数据库的分布式事务 最早的分布式事务应用架构很简单，不涉及服务间的访问调用，仅仅是服务内操作涉及到对多个数据库资源的访问。\n1.3.3 多服务多数据库的分布式事务 当一个服务操作访问不同的数据库资源，又希望对它们的访问具有事务特性时，就需要采用分布式事务来协调所有的事务参与者。在这种情况下，起始于某个服务的事务在调用另外一个服务的时候，需要以某种机制流转到另外一个服务，从而使被调用的服务访问的资源也自动加入到该事务当中来。下图反映了这样一个跨越多个服务的分布式事务：\n1.3.4 多服务多数据源的分布式事务 如果将上面这两种场景(一个服务可以调用多个数据库资源，也可以调用其他服务)结合在一起，对此进行延伸，整个分布式事务的参与者将会组成如下图所示的树形拓扑结构。在一个跨服务的分布式事务中，事务的发起者和提交均系同一个，它可以是整个调用的客户端，也可以是客户端最先调用的那个服务。\n较之基于单一数据库资源访问的本地事务，分布式事务的应用架构更为复杂。在不同的分布式应用架构下，实现一个分布式事务要考虑的问题并不完全一样，比如对多资源的协调、事务的跨服务传播等，实现机制也是复杂多变。\n事务的作用：==保证每个事务的数据一致性。==\n1.4 CAP定理（面试） CAP 定理，又被叫作布鲁尔定理。对于设计分布式系统(不仅仅是分布式事务)的架构师来说，CAP 就是你的入门理论。\n**C (一致性)：**对某个指定的客户端来说，读操作能返回最新的写操作。\n对于数据分布在不同节点上的数据来说，如果在某个节点更新了数据，那么在其他节点如果都能读取到这个最新的数据，那么就称为强一致，如果有某个节点没有读取到，那就是分布式不一致。\n**A (可用性)：**非故障的节点在合理的时间内返回合理的响应(不是错误和超时的响应)。可用性的两个关键一个是合理的时间，一个是合理的响应。\n合理的时间指的是请求不能无限被阻塞，应该在合理的时间给出返回。合理的响应指的是系统应该明确返回结果并且结果是正确的，这里的正确指的是比如应该返回 50，而不是返回 40。\n**P (网络分区容错性)：**当出现网络分区后，系统能够继续工作。打个比方，这里集群有多台机器，有台机器网络出现了问题，但是这个集群仍然可以正常工作。\nCAP定理中: 所有架构只能够满足C A P 三个中间的两个!\n2 分布式事务解决方案（面试） 1.XA两段提交(强一致)\n2.TCC三段提交(强一致)\n3.本地消息表(MQ+Table)(最终一致)\n4.事务消息(RocketMQ[alibaba])(最终一致)\n==5.Seata(alibaba)==\n6.RabbitMQ的ACK机制实现分布式事务(拓展)\n一致性拓展：\n强一致性：只要系统提供服务，那么数据一定一致的。 弱一致性：系统提供服务的时候，数据有可能不是一致的，而是要等待一段时间才能一致。 最终一致性：是弱一致性的特例，数据最终总会一致的，但是需要一定时间。 2.1 基于XA协议的两阶段提交(2PC) X/Open 组织（即现在的 Open Group ）定义了分布式事务处理模型\nXA协议：XA 是 X/Open 定义的交易中间件与数据库之间的接口规范（即接口函数），交易中间件用它来通知数据库事务的开始、结束以及提交、回滚等， XA 接口函数由数据库厂商提供。\n两阶段提交协议(Two Phase Commitment Protocol)中，涉及到两种角色\n一个事务协调者（coordinator）：负责协调多个参与者进行事务投票及提交(回滚) 多个事务参与者（participants）：即本地事务执行者\n总共处理步骤有两个 （1）投票阶段（voting phase）：协调者将通知事务参与者准备提交或取消事务，然后进入表决过程。参与者将告知协调者自己的决策：同意（事务参与者本地事务执行成功，但未提交）或取消（本地事务执行故障）； （2）提交阶段（commit phase）：收到参与者的通知后，协调者再向参与者发出通知，根据反馈情况决定各参与者是否要提交还是回滚；\n如果任一资源管理器在第一阶段返回准备失败，那么事务管理器会要求所有资源管理器在第二阶段执行回滚操作。通过事务管理器的两阶段协调，最终所有资源管理器要么全部提交，要么全部回滚，最终状态都是一致的\n优点： 尽量保证了数据的强一致（无法完全保证），适合对数据强一致要求很高的关键领域。\n缺点：\n**同步阻塞问题：**执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。 **单点故障：**由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题） **数据不一致：**在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，这回导致只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据不一致性的现象。 **==二阶段无法解决的问题：==**协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交 2.2 三段提交（3PC） 三段提交是两段提交的升级版\nCanCommit阶段：询问阶段\n类似2PC的准备阶段，协调者向参与者发送CanCommit请求，询问是否可以执行事务提交操作，然后开始等待参与者的响应。\nPreCommit阶段：事务执行但不提交阶段\n协调者根据参与者的反应情况来决定是否可以进行事务的PreCommit操作：\n协调者从所有的参与者获得的反馈都是Yes响应 发送预提交请求协调者向参与者发送PreCommit请求；\n参与者接收到PreCommit请求后，执行事务操作，并将undo（执行前数据）和redo（执行后数据）信息记录到事务日志中；\n参与者成功的执行了事务操作，则返回ACK（确认机制：已确认执行）响应，同时开始等待最终指令。\n有任何一个参与者向协调者发送了No响应，或者等待超时 协调者向所有参与者发送中断请求请求。 参与者收到来自协调者的中断请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。 doCommit阶段：事务提交阶段\n执行提交\n协调接收到所有参与者返回的ACK响应后，协调者向所有参与者发送doCommit请求。\n参与者接收到doCommit请求之后，执行最终事务提交，事务提交完之后，向协调者发送Ack响应并释放所有事务资源。\n协调者接收到所有参与者的ACK响应之后，完成事务。\n中断事务\n协调者没有接收到参与者发送的ACK响应（可能是接受者发送的不是ACK响应，也可能响应超时），协调者向所有参与者发送中断请求； 参与者接收到中断请求之后，利用其在阶段二记录的undo信息来执行事务的回滚操作，并在完成回滚之后，向协调者发送ACK消息，释放所有的事务资源。 协调者接收到参与者反馈的ACK消息之后，执行事务的中断。 **优点：**相对于2PC，3PC主要解决的单点故障问题，并减少阻塞，因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行commit。而不会一直持有事务资源并处于阻塞状态。\n**缺点：**会导致数据一致性问题。由于网络原因，协调者发送的中断响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到中断命令并执行回滚的参与者之间存在数据不一致的情况。\n2.3 TCC补偿机制 TCC 其实就是采用的补偿机制，其核心思想是：==针对每个操作，都要注册一个与其对应的确认和补偿（撤销）操作==。三个阶段如下：\n操作方法 含义 Try 预留业务资源/数据效验-尝试检查当前操作是否可执行 Confirm 确认执行业务操作，实际提交数据，不做任何业务检查。try成功，confirm必定成功 Cancel 执行业务出错时，需要回滚数据的状态下执行的业务逻辑 其核心在于将业务分为两个操作步骤完成。==不依赖事务协调器对分布式事务的支持，而是通过对业务逻辑的分解来实现分布式事务==。\n例如： 小红要向小白转账100元，执行流程：\n首先在 Try 阶段，要先调用远程接口检查小红的账户余额是否大于等于100元，若足够则对余额进行冻结，检查小白的账户状态是否正常。 在 Confirm 阶段，执行远程调用的转账的操作，扣除小红账户100元，小白账户加100元。 如果第2步执行成功，那么转账成功，小红账户解冻，流程结束。 如果第二步执行失败，则调用服务A的Cancel方法，账户余额回滚100元及解冻小红账户，同时调用服务B的Cancel方法，账户扣除100元。 优点： 跟2PC比起来，实现以及流程相对简单了一些。\n缺点：\n在2 3 4步中都有可能失败，从而导致数据不一致。\nTCC属于应用层的一种补偿方式，需要程序员在实现的时候多写很多补偿的代码，复杂业务场景下代码逻辑非常复杂。\n幂等性无法确保。\n2.4 本地消息表（异步确保） 本地消息表这种实现方式应该是业界使用最多的，其核心思想是将==分布式事务拆分成本地事务进行处理==，这种思路是来源于ebay。我们可以从下面的流程图中看出其中的一些细节：\n工作流程：\n==消息生产方，需要额外建一个消息表==，并记录消息发送状态。消息表和业务数据要在一个事务里提交，也就是说他们要在一个数据库里面。然后消息会经过MQ发送到消息的消费方。如果消息发送失败，会进行重试发送。 ==消息消费方，需要处理这个消息==，并完成自己的业务逻辑。此时如果本地事务处理成功，表明已经处理成功了，如果处理失败， 果是业务上面的失败，可以给生产方发送一个业务补偿消息，通知生产方进行回滚等操作。 生产方和消费方定时扫描本地消息表，把还没处理完成的消息或者失败的消息再发送一遍。 优点： 一种非常经典的实现，避免了分布式事务，实现了最终一致性。\n缺点： 消息表会耦合到业务系统中，如果没有封装好的解决方案，会有很多杂活需要处理。\n2.5 MQ 事务消息 有一些第三方的MQ是支持事务消息的，比如RocketMQ，他们支持事务消息的方式也是类似于采用的二阶段提交，但是市面上一些主流的MQ都是不支持事务消息的，比如 RabbitMQ 和 Kafka 都不支持（RabbitMQ、Kafka基于ACK机制）。\n以阿里的 RocketMQ 中间件为例，流程为：\n发送一个事务消息，这个时候，RocketMQ将消息状态标记为Prepared，注意此时这条消息消费者是无法消费到的。 执行业务代码逻辑。 确认发送消息，RocketMQ将消息状态标记为可消费，这个时候消费者才能真正消费到这条消息。 如果步骤3确认消息发送失败，RocketMQ会定期扫描消息集群中的事务消息，如果发现了Prepared消息，它会向消息发送端(生产者)确认。RocketMQ会根据发送端设置的策略来决定是回滚还是继续发送确认消息。这样就保证了消息发送与本地事务同时成功或同时失败。 正常流程图：\n完整流程图：\n优点： 实现了最终一致性，不需要依赖本地数据库事务。\n缺点： 目前主流MQ中只有RocketMQ支持事务消息。\n延申拓展：\nMQ非事务消息实现：\n方案一：创建独立消息服务 方案二：使用非事务MQ（RabbitMQ/Kafka）的ACK机制 2.6 Seata 2.6.1 Seata简介 2019 年 1 月，阿里巴巴中间件团队发起了开源项目 Fescar（Fast \u0026amp; Easy Commit And Rollback），和社区一起共建开源分布式事务解决方案。Fescar 的愿景是让分布式事务的使用像本地事务的使用一样，简单和高效，并逐步解决开发者们遇到的分布式事务方面的所有难题。\nFescar 开源后，蚂蚁金服加入 Fescar 社区参与共建，并在 Fescar 0.4.0 版本中贡献了 TCC 模式。\n为了打造更中立、更开放、生态更加丰富的分布式事务开源社区，经过社区核心成员的投票，大家决定对 Fescar 进行品牌升级，并更名为 Seata，意为：Simple Extensible Autonomous Transaction Architecture，是一套一站式分布式事务解决方案。\nSeata 融合了阿里巴巴和蚂蚁金服在分布式事务技术上的积累，并沉淀了新零售、云计算和新金融等场景下丰富的实践经验。\n核心组件：\nTransaction Coordinator (TC)： 事务协调器，维护全局事务的运行状态，负责协调并驱动全局事务的提交或回滚。 Transaction Manager (TM)： 控制全局事务的边界，负责开启一个全局事务，并最终发起全局提交或全局回滚的决议。 Resource Manager (RM)： 控制分支事务，负责分支注册、状态汇报，并接收事务协调器的指令，驱动分支（本地）事务的提交和回滚。 工作流程：\nTM 向 TC 申请开启一个全局事务，全局事务创建成功并生成一个全局唯一的事务ID（XID），XID 在微服务调用链路的上下文中传播。 RM 向 TC 注册分支事务，接着执行这个分支事务并提交事务（==重点：RM在此阶段就已经执行了本地事务的提交/回滚==），最后将执行结果汇报给TC。 TM 根据 TC 中所有的分支事务的执行情况，发起全局提交或回滚决议。 TC 调度 XID 下管辖的全部分支事务完成提交或回滚请求。 2.6.2 Seata支持的模式 seata中有两种常见分布式事务实现方案，AT及TCC\nAT模式：赖于RM拥有本地数据库事务的能力，对于客户业务无侵入性\nTCC 模式\n2.6.3 Seata的优点 对业务无侵入：即减少技术架构上的微服务化所带来的分布式事务问题对业务的侵入 高性能：减少分布式事务解决方案所带来的性能消耗(2PC)\n2.6.4 AT模式 Seata AT模式是基于XA事务演进而来的一个分布式事务中间件，XA是一个基于数据库实现的分布式事务协议，本质上和两阶段提交一样，需要数据库支持，Mysql5.6以上版本支持XA协议，其他数据库如Oracle，DB2也实现了XA接口。\nAT模式分为两个阶段，如下：\n第一阶段：本地数据备份阶段 Seata 的 JDBC 数据源代理通过对业务 SQL 的解析，把==业务数据在变化前后的数据镜像组织成回滚日志==（XID/分支事务ID（Branch ID/变化前的数据/变化后的数据）。 将回滚日志存入一张日志表UNDO_LOG（==需要手动创建==）,并对UNDO_LOG表中的这条数据形成行锁（for update）。 若锁定失败，说明有其他事务在操作这条数据，它会在一段时间内重试，重试失败则回滚本地事务，并向TC汇报本地事务执行失败。 这样，可以保证：任何提交的业务数据的更新一定有相应的回滚日志存在\n目的：\n基于这样的机制，分支的本地事务便可以在全局事务的第一阶段提交，并马上释放本地事务锁定的资源。 有了回滚日志之后，可以在第一阶段释放对资源的锁定，降低了锁范围，提高效率，即使第二阶段发生异常需要回滚，只需找对undolog中对应数据并反解析成sql来达到回滚目的。 Seata通过代理数据源（DataSource-\u0026gt;DataSourceProxy）将业务sql的执行解析成undolog来与业务数据的更新同时入库，达到了对业务无侵入的效果 第二阶段：全局事务提交/回滚\n全局提交： 所有分支事务此时已经完成提交，所有分支事务提交都正常。 ==TM从TC获知后会决议执行====全局提交====，TC异步通知所有的RM释放UNDO_LOG表中的行锁==，同时清理掉UNDO_LOG表中刚才释放锁的那条数据。 全局回滚： 若任何一个RM一阶段事务提交失败，通知TC提交失败。 ==TM从TC获知后会决议执行全局回滚====，====TC向所有的RM发送回滚请求==。 RM通过XID和Branch ID找到相应的回滚日志记录，通过回滚记录生成反向的更新 SQL 并执行，以完成分支的回滚，同时释放锁，清除UNDO_LOG表中释放锁的那条数据。 2.6.5 TCC模式 seata也针对TCC做了适配兼容，支持TCC事务方案，原理前面已经介绍过，基本思路就是使用侵入业务上的补偿及事务管理器的协调来达到全局事务的一起提交及回滚。\n2.7 跨mysql ip测试 插入数据库\nundo_log 表\n发生异常\u0026ndash;回滚\n数据库可以看到之前被修改的数据被回滚了\n","date":"2022-07-13T19:53:40+08:00","permalink":"https://mikeLing-qx.github.io/p/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/","title":"分布式事务"},{"content":"1. 入门使用 ==参考资料==: https://juejin.cn/post/7097389644441976846\n1. @EnableCaching：启用缓存功能 ​\t开启缓存功能，配置类中需要加上这个注解，有了这个注解之后，spring才知道你需要使用缓存的功能，其他和缓存相关的注解才会有效，spring中主要是==通过aop实现的==，通过aop来拦截需要使用缓存的方法，实现缓存的功能。\n2. @Cacheable：赋予缓存功能 ​\t@Cacheable可以==标记在一个方法上，也可以标记在一个类上==。当标记在一个方法上时表示该==方法是支持缓存的==，==当标记在一个类上时则表示该类所有的方法都是支持缓存的==。对于一个支持缓存的方法，==Spring会在其被调用后将其返回值缓存起来==，以保证下次利用同样的参数来执行该方法时可以直接从缓存中获取结果，而不需要再次执行该方法。Spring在缓存方法的返回值时是以键值对进行缓存的，值就是方法的返回结果，==至于键的话，Spring又支持两种策略，默认策略和自定义策略==，这个稍后会进行说明。需要注意的是当一个支持缓存的方法在对象==内部被调用时是不会触发缓存功能==的。@Cacheable可以指定三个属性，==value、key和condition。==\n1. value 属性: 指定cache名称 ​\tvalue和cacheNames属性作用一样，必须指定其中一个，表示当前方法的返回值是会被缓存在哪个Cache上的，对应Cache的名称。其可以是一个Cache也可以是多个Cache，当需要指定多个Cache时其是一个数组。\n​\t可以将Cache想象为一个HashMap，系统中可以有很多个Cache，每个Cache有一个名字，你需要将方法的返回值放在哪个缓存中，需要通过缓存的名称来指定。\nimport org.springframework.cache.CacheManager; import org.springframework.cache.annotation.EnableCaching; import org.springframework.cache.concurrent.ConcurrentMapCacheManager; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.ComponentScan; import org.springframework.context.annotation.Configuration; @EnableCaching //@0 @ComponentScan @Configuration public class MainConfig1 { //@1：缓存管理器 @Bean public CacheManager cacheManager() { //创建缓存管理器(ConcurrentMapCacheManager：其内部使用ConcurrentMap实现的)，构造器用来指定缓存的名称，可以指定多个 ConcurrentMapCacheManager cacheManager = new ConcurrentMapCacheManager(\u0026quot;cache1\u0026quot;); return cacheManager; } } 缓存管理器，类型为CacheManager，CacheManager这个是个接口，有好几个实现（比如使用redis、ConcurrentMap来存储缓存信息），此处我们使用ConcurrentMapCacheManager，内部使用ConcurrentHashMap将缓存信息直接存储在本地jvm内存中，不过线上环境一般是集群的方式，可以通过redis实现\n2. key 属性: 自定义key ​\tkey属性用来指定Spring缓存方法的返回结果时对应的key的，上面说了你可以将Cache理解为一个hashMap，缓存以key-\u0026gt;value的形式存储在hashmap中，value就是需要缓存值（即方法的返回值）\n属性名称 描述 实例 methodName 当前方法名 #root.methodName method 当前方法 #root.method,name target 当前被调用的对象 #root.targetClass args 当前方法参数组成的数组 #root.args[0] caches 当前被调用方法使用的cache #root.caches[0].name 2. 问题 如果项目重启, 那么会对redis 缓存有影响吗? 重新请求接口走的是缓存吗? 有影响, 不走缓存 3. demo 开启@EnableCaching 注解\n==注入 cacheManager bean==\n@Bean public CacheManager cacheManager(RedisTemplate redisTemplate) { // 1. 非锁方式：nonLockingRedisCacheWriter(RedisConnectionFactory connectionFactory); // 有锁方式：lockingRedisCacheWriter(RedisConnectionFactory connectionFactory); RedisCacheWriter redisCacheWriter = RedisCacheWriter .nonLockingRedisCacheWriter(Objects.requireNonNull(redisTemplate.getConnectionFactory())); //2.创建Jackson对象并传入需要序列化的对象 Jackson2JsonRedisSerializer\u0026lt;Object\u0026gt; serializer = new Jackson2JsonRedisSerializer\u0026lt;\u0026gt;(Object.class); ObjectMapper om = new ObjectMapper(); om.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); om.activateDefaultTyping(om.getPolymorphicTypeValidator(), ObjectMapper.DefaultTyping.NON_FINAL, JsonTypeInfo.As.WRAPPER_ARRAY); serializer.setObjectMapper(om); //3.传入 Jackson对象 并获取 RedisSerializationContext对象 RedisSerializationContext\u0026lt;Object, Object\u0026gt; serializationContext = RedisSerializationContext.fromSerializer(serializer); //4.配置RedisCacheConfiguration /* * RedisCacheConfiguration.defaultCacheConfig() * 设置 value 的序列化 serializeValuesWit(SerializationPari\u0026lt;?\u0026gt; valueSerializationPari) * 设置 key 的序列化 serializeKeysWith(SerializationPari valueSerializationPari) */ RedisCacheConfiguration redisCacheConfiguration = RedisCacheConfiguration.defaultCacheConfig() .entryTtl(Duration.ofDays(1L)) .serializeValuesWith(serializationContext.getValueSerializationPair()); //5.创建RedisCacheManager(RedisCacheWriter redisCacheWriter, RedisCacheConfiguration redisCacheConfiguration)对象并返回 return new RedisCacheManager(redisCacheWriter, redisCacheConfiguration); } 使用 @Cacheable(cacheNames = {\u0026quot;zxipr\u0026quot;}, key = \u0026quot;#root.methodName\u0026quot;, cacheManager = \u0026quot;cacheManager\u0026quot;) public List\u0026lt;PatentTrademarkApplyCountEntity\u0026gt; listPatentTrademarkApplyTimeTrend(){} @Cacheable(cacheNames = {\u0026quot;zxipr:trademark\u0026quot;}, key = \u0026quot;#root.methodName\u0026quot;, cacheManager = \u0026quot;cacheManager\u0026quot;) public TrademarkCountEntity countTrademarks() {} ","date":"2022-06-06T15:07:52+08:00","permalink":"https://mikeLing-qx.github.io/p/spring_cache/","title":"Spring_cache"},{"content":"1. 概述 常见lamda 函数式接口:\nrunAsync：异步执行没有返回值；\rsupplyAsync：异步执行有返回值；\rthenApply：继续执行当前线程future完成的函数，不需要阻塞等待其处理完成；\rthenApplyAsync：在不同线程池异步地应用参数中的函数；\rthenCompose：用于多个彼此依赖的futrue进行串联起来, 串行\rthenCombine：并联起两个独立的future，注意，这些future都是在长时间计算都完成以后, 并行关系 每个变种函数的第三个方法也许会发现里面都有一个 ==Executor 类型的参数==，用于指定线程池\n如果没有指定线程池，那自然就会有一个默认的线程池，也就是 ForkJoinPool\nForkJoinPool 的线程数默认是==CPU 的核心数==\n不要所有业务共用一个线程池，因为，一旦有任务执行一些很慢的 I/O 操作，就会导致线程池中所有线程都阻塞在 I/O 操作上，从而造成线程饥饿，进而影响整个系统的性能\nJdk9 升级内容\n添加了新的工厂方法\n支持延迟和超时处理\norTimeout()\rcompleteOnTimeout()\r2. 类结构 1. Future 实现了 Future 接口，那就具有 Future 接口的相关特性，请脑补 Future 那少的可怜的 5 个方法，这里不再赘述，具体请查看 不会用Java Future，我怀疑你泡茶没我快\n2. CompletionStage CompletionStage 这个接口还是挺陌生的，中文直译过来是【竣工阶段】，如果将烧水泡茶比喻成一项大工程，他们的竣工阶段体现是不一样的\n3. 线程关系 单看线程1 或单看线程 2 就是一种串行关系，做完一步之后做下一步 一起看线程1 和 线程 2，它们彼此就是并行关系，两个线程做的事彼此独立互补干扰 泡茶就是线程1 和 线程 2 的汇总/组合，也就是线程 1 和 线程 2 都完成之后才能到这个阶段（当然也存在线程1 或 线程 2 任意一个线程竣工就可以开启下一阶段的场景） CompletionStage 所有函数都是用于描述任务的时序关系\n4. CompleteableFutute 方法总览 0. 常用api 概述 参考资料: https://blog.51cto.com/zhangzhixi/5626288\n==提交任务==：\nsupplyAsync 需要返回值时使用\nrunAsync\n==接力处理==：\nthenRun thenRunAsync\nthenAccept thenAcceptAsync\nthenApply thenApplyAsync\nhandle handleAsync\napplyToEither applyToEitherAsync\nacceptEither acceptEitherAsync\nrunAfterEither runAfterEitherAsync\nthenCombine thenCombineAsync\nthenAcceptBoth thenAcceptBothAsync\n==获取结果==：\njoin 阻塞等待，不会抛异常 get 阻塞等待，会抛异常 complete(T value) 不阻塞，如果任务已完成，返回处理结果。如果没完成，则返回传参value。 ==总结分类==\n带run的方法，无入参，无返回值。 带accept的方法，有入参，无返回值。 带supply的方法，无入参，有返回值。 带apply的方法，有入参，有返回值。 带handle的方法，有入参，有返回值，并且带异常处理。 以Async结尾的方法，都是异步的，否则是同步的。 以Either结尾的方法，只需完成任意一个。 以Both/Combine结尾的方法，必须所有都完成 completeExceptionally(Throwable ex) 不阻塞，如果任务已完成，返回处理结果。如果没完成，抛异常。\n1. 串行关系 then 直译【然后】，也就是表示下一步，所以通常是一种串行关系体现, then 后面的单词（比如 run /apply/accept）就是上面说的函数式接口中的抽象方法名称了，它的作用和那几个函数式接口的作用是一样一样滴\n// run 是没有返回值的 CompletableFuture\u0026lt;Void\u0026gt; thenRun(Runnable action)\rCompletableFuture\u0026lt;Void\u0026gt; thenRunAsync(Runnable action)\rCompletableFuture\u0026lt;Void\u0026gt; thenRunAsync(Runnable action, Executor executor)\r\u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; thenApply(Function\u0026lt;? super T,? extends U\u0026gt; fn)\r\u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; thenApplyAsync(Function\u0026lt;? super T,? extends U\u0026gt; fn)\r\u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; thenApplyAsync(Function\u0026lt;? super T,? extends U\u0026gt; fn, Executor executor)\rCompletableFuture\u0026lt;Void\u0026gt; thenAccept(Consumer\u0026lt;? super T\u0026gt; action) CompletableFuture\u0026lt;Void\u0026gt; thenAcceptAsync(Consumer\u0026lt;? super T\u0026gt; action)\rCompletableFuture\u0026lt;Void\u0026gt; thenAcceptAsync(Consumer\u0026lt;? super T\u0026gt; action, Executor executor)\r\u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; thenCompose(Function\u0026lt;? super T, ? extends CompletionStage\u0026lt;U\u0026gt;\u0026gt; fn) \u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; thenComposeAsync(Function\u0026lt;? super T, ? extends CompletionStage\u0026lt;U\u0026gt;\u0026gt; fn)\r\u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; thenComposeAsync(Function\u0026lt;? super T, ? extends CompletionStage\u0026lt;U\u0026gt;\u0026gt; fn, Executor executor)\r2. 聚合关系 and combine... with... 和 both...and... 都是要求两者都满足，也就是 and 的关系了\n\u0026lt;U,V\u0026gt; CompletableFuture\u0026lt;V\u0026gt; thenCombine(CompletionStage\u0026lt;? extends U\u0026gt; other, BiFunction\u0026lt;? super T,? super U,? extends V\u0026gt; fn)\r\u0026lt;U,V\u0026gt; CompletableFuture\u0026lt;V\u0026gt; thenCombineAsync(CompletionStage\u0026lt;? extends U\u0026gt; other, BiFunction\u0026lt;? super T,? super U,? extends V\u0026gt; fn)\r\u0026lt;U,V\u0026gt; CompletableFuture\u0026lt;V\u0026gt; thenCombineAsync(CompletionStage\u0026lt;? extends U\u0026gt; other, BiFunction\u0026lt;? super T,? super U,? extends V\u0026gt; fn, Executor executor)\r\u0026lt;U\u0026gt; CompletableFuture\u0026lt;Void\u0026gt; thenAcceptBoth(CompletionStage\u0026lt;? extends U\u0026gt; other, BiConsumer\u0026lt;? super T, ? super U\u0026gt; action)\r\u0026lt;U\u0026gt; CompletableFuture\u0026lt;Void\u0026gt; thenAcceptBothAsync(CompletionStage\u0026lt;? extends U\u0026gt; other, BiConsumer\u0026lt;? super T, ? super U\u0026gt; action)\r\u0026lt;U\u0026gt; CompletableFuture\u0026lt;Void\u0026gt; thenAcceptBothAsync( CompletionStage\u0026lt;? extends U\u0026gt; other, BiConsumer\u0026lt;? super T, ? super U\u0026gt; action, Executor executor)\rCompletableFuture\u0026lt;Void\u0026gt; runAfterBoth(CompletionStage\u0026lt;?\u0026gt; other, Runnable action)\rCompletableFuture\u0026lt;Void\u0026gt; runAfterBothAsync(CompletionStage\u0026lt;?\u0026gt; other, Runnable action)\rCompletableFuture\u0026lt;Void\u0026gt; runAfterBothAsync(CompletionStage\u0026lt;?\u0026gt; other, Runnable action, Executor executor)\r3. or或 关系 Either...or... 表示两者中的一个，自然也就是 Or 的体现了\n\u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; applyToEither(CompletionStage\u0026lt;? extends T\u0026gt; other, Function\u0026lt;? super T, U\u0026gt; fn)\r\u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; applyToEitherAsync(、CompletionStage\u0026lt;? extends T\u0026gt; other, Function\u0026lt;? super T, U\u0026gt; fn)\r\u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; applyToEitherAsync(CompletionStage\u0026lt;? extends T\u0026gt; other, Function\u0026lt;? super T, U\u0026gt; fn, Executor executor)\rCompletableFuture\u0026lt;Void\u0026gt; acceptEither(CompletionStage\u0026lt;? extends T\u0026gt; other, Consumer\u0026lt;? super T\u0026gt; action)\rCompletableFuture\u0026lt;Void\u0026gt; acceptEitherAsync(CompletionStage\u0026lt;? extends T\u0026gt; other, Consumer\u0026lt;? super T\u0026gt; action)\rCompletableFuture\u0026lt;Void\u0026gt; acceptEitherAsync(CompletionStage\u0026lt;? extends T\u0026gt; other, Consumer\u0026lt;? super T\u0026gt; action, Executor executor)\rCompletableFuture\u0026lt;Void\u0026gt; runAfterEither(CompletionStage\u0026lt;?\u0026gt; other, Runnable action)\rCompletableFuture\u0026lt;Void\u0026gt; runAfterEitherAsync(CompletionStage\u0026lt;?\u0026gt; other, Runnable action)\rCompletableFuture\u0026lt;Void\u0026gt; runAfterEitherAsync(CompletionStage\u0026lt;?\u0026gt; other, Runnable action, Executor executor)\r4. 异常处理 CompletableFuture\u0026lt;T\u0026gt; exceptionally(Function\u0026lt;Throwable, ? extends T\u0026gt; fn)\rCompletableFuture\u0026lt;T\u0026gt; exceptionallyAsync(Function\u0026lt;Throwable, ? extends T\u0026gt; fn)\rCompletableFuture\u0026lt;T\u0026gt; exceptionallyAsync(Function\u0026lt;Throwable, ? extends T\u0026gt; fn, Executor executor)\rCompletableFuture\u0026lt;T\u0026gt; whenComplete(BiConsumer\u0026lt;? super T, ? super Throwable\u0026gt; action)\rCompletableFuture\u0026lt;T\u0026gt; whenCompleteAsync(BiConsumer\u0026lt;? super T, ? super Throwable\u0026gt; action)\rCompletableFuture\u0026lt;T\u0026gt; whenCompleteAsync(BiConsumer\u0026lt;? super T, ? super Throwable\u0026gt; action, Executor executor)\r\u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; handle(BiFunction\u0026lt;? super T, Throwable, ? extends U\u0026gt; fn)\r\u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; handleAsync(BiFunction\u0026lt;? super T, Throwable, ? extends U\u0026gt; fn)\r\u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; handleAsync(BiFunction\u0026lt;? super T, Throwable, ? extends U\u0026gt; fn, Executor executor)\r==exceptionally 和 whenComplete, handle 的区别==\n5. 性能 任务巨多, 如何保证性能 如何观察任务的调度情况 (存在线程复用的情况) 可以使用线程池. 设置核心线程数为0, 线程存活时间为0, 保证每个任务都在新线程上执行 ​\n","date":"2022-06-03T18:12:07+08:00","permalink":"https://mikeLing-qx.github.io/p/completeablefuture/","title":"CompleteableFuture"},{"content":"1. 基础 Elasticsearch 的 DSL（Domain-Specific Language）是用于查询、操作和分析数据的功能强大且灵活的查询语言。它基于 JSON 格式表达，分为两种主要类型的查询：精确查询（Leaf Query） 和 复合查询（Compound Query）。\n2. 用例 文章来源: https://blog.csdn.net/a_liuren/article/details/111938001\n创建index, type, 并新增一个ducument 记录, 1是索引\nPOST test1/_doc/1 { \u0026quot;uid\u0026quot;: \u0026quot;1234\u0026quot;, \u0026quot;phone\u0026quot;: \u0026quot;123456\u0026quot;, \u0026quot;message\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;msgcode\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;sendtime\u0026quot;: \u0026quot;2019-03-14 01:14:20\u0026quot; } 从数据库中查询 select * from table where id =1\nGET test1/_doc/1 定义索引库, mapping 指定数据结构, settings 主要的作用就是用来修改分片和副本数\n{ \u0026quot;settings\u0026quot;: { \u0026quot;number_of_shards\u0026quot;: 10, \u0026quot;number_of_replicas\u0026quot;: 1, \u0026quot;refresh_interval\u0026quot;: \u0026quot;1s\u0026quot; }, \u0026quot;mapping\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;uid\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;long\u0026quot; }, \u0026quot;phone\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;long\u0026quot; }, \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;msgcode\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;long\u0026quot; }, \u0026quot;sendtime\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;date\u0026quot;, \u0026quot;format\u0026quot;: \u0026quot;yyyy-MM-dd HH:mm:ss\u0026quot; } } } } 通过主键id 修改ES数据\nPOST jason/_doc/1 {\u0026quot;uid\u0026quot;:\u0026quot;175\u0026quot;,\u0026quot;phone\u0026quot;:\u0026quot;176590660\u0026quot;,\u0026quot;message\u0026quot;:\u0026quot;章美\u0026quot;,\u0026quot;msgcode\u0026quot;:\u0026quot;1\u0026quot;,\u0026quot;sendtime\u0026quot;:\u0026quot;2020-12-14 01:14:20\u0026quot;} 根据id 删除数据\ndelete jason/_doc/1 删除手机号是123456的数据\nPOST test1/_doc/_delete_by_query { \u0026quot;query\u0026quot;:{ \u0026quot;term\u0026quot;:{ \u0026quot;phone\u0026quot;: \u0026quot;123456\u0026quot; } } } 根据查询结果 更新\nPOST test1/_doc/_update_by_query { \u0026quot;script\u0026quot;:{ \u0026quot;lang\u0026quot;:\u0026quot;painless\u0026quot;, \u0026quot;inline\u0026quot;:\u0026quot;ctx._source.remove(\\\u0026quot;msgcode\\\u0026quot;)\u0026quot; } } 查询索引库中的数据\nGET _search { \u0026quot;query\u0026quot;:{ \u0026quot;match_all\u0026quot;: {} } } 查询指定index, type 中的数据\nGET jason/_doc/_search 多字段匹配\nGET jason/_doc/_search { \u0026quot;query\u0026quot;:{ \u0026quot;terms\u0026quot;:{ \u0026quot;uid\u0026quot;:[ 176, 12 ] } } } 范围查询; 需要的数据类型为数值\nGET jason/_doc/search { \u0026quot;query\u0026quot;: { \u0026quot;range\u0026quot;:{ \u0026quot;uid\u0026quot;: { \u0026quot;gte\u0026quot;: 165, \u0026quot;lte\u0026quot;: 177 } } } } exist 存在字段查询, 其实就是非null 的字段数据\nGET jason/_doc/_search { \u0026quot;query\u0026quot;:{ \u0026quot;exists\u0026quot;:{ \u0026quot;field\u0026quot;: \u0026quot;uid\u0026quot; } } } ==bool 可以用来合并多个过滤条件查询结果==\nGET jason/_doc/_search { \u0026quot;query\u0026quot;:{ \u0026quot;bool\u0026quot;:{ \u0026quot;must\u0026quot;:{ \u0026quot;term\u0026quot;:{ \u0026quot;phone\u0026quot;: \u0026quot;176206660\u0026quot; } }, \u0026quot;must_not\u0026quot;:{ \u0026quot;term\u0026quot;:{ \u0026quot;uid\u0026quot;: \u0026quot;7890\u0026quot; } }, \u0026quot;should\u0026quot;:[ { \u0026quot;term\u0026quot;:{ \u0026quot;uid\u0026quot;: \u0026quot;176\u0026quot; } } ], \u0026quot;adjust_pure_negative\u0026quot;: true, \u0026quot;boost\u0026quot;: 1 } } } 模糊查询 (比较消耗性能)\nGET jason/_doc/_search { \u0026quot;query\u0026quot;:{ \u0026quot;wildcard\u0026quot;:{ \u0026quot;message\u0026quot;: \u0026quot;ruan*\u0026quot; } } } 正则查询\n# 查询出信息为ruan到0-9的数据 GET jason/_doc/_search { \u0026quot;query\u0026quot;:{ \u0026quot;regexp\u0026quot;:{ \u0026quot;message\u0026quot;:\u0026quot;ruan[0-9]\u0026quot; } } } 匹配查询\n新增数据 PUT /questionnaire_record/_doc/9 { \u0026quot;id\u0026quot;: \u0026quot;6\u0026quot;, \u0026quot;uid\u0026quot;:\u0026quot;12\u0026quot;, \u0026quot;activityNo\u0026quot;: \u0026quot;AC20122403071370181\u0026quot;, \u0026quot;activityCustomType\u0026quot;:1, \u0026quot;elapsedTime\u0026quot;:12, \u0026quot;winPrize\u0026quot;: true, \u0026quot;browerFrom\u0026quot;:\u0026quot;wap\u0026quot; } 匹配查询 POSt /questionnaire_record/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;activityNo\u0026quot;: \u0026quot;AC20122403071370181\u0026quot; } } } 新增数据\nPUT /school/student/14 { \u0026quot;name\u0026quot;: \u0026quot;JetWu\u0026quot;, \u0026quot;age\u0026quot;: 22, \u0026quot;city\u0026quot;: \u0026quot;chengdu\u0026quot;, \u0026quot;interests\u0026quot;:[\u0026quot;run\u0026quot;,\u0026quot;qiicky_run\u0026quot;,\u0026quot;mountain climbing\u0026quot;], \u0026quot;introduction\u0026quot;: \u0026quot;i am form hengyang. i love my hometown\u0026quot; } PUT /school/student/8 { \u0026quot;name\u0026quot;: \u0026quot;jason\u0026quot;, \u0026quot;age\u0026quot;: 18, \u0026quot;city\u0026quot;: \u0026quot;hefei\u0026quot;, \u0026quot;interests\u0026quot;: [\u0026quot;swiming\u0026quot;,\u0026quot;sleep\u0026quot;], \u0026quot;introduction\u0026quot;: \u0026quot;阳光男孩\u0026quot; } PUT /school/student/4 { \u0026quot;name\u0026quot;: \u0026quot;hello\u0026quot;, \u0026quot;age\u0026quot;:15, \u0026quot;city\u0026quot;: \u0026quot;china\u0026quot;, \u0026quot;interests\u0026quot;:[\u0026quot;chinese\u0026quot;,\u0026quot;game\u0026quot;], \u0026quot;introduction\u0026quot;:\u0026quot;大家好，我是一个宅男\u0026quot; } #都是查询在age这个字段上面非空的数据，查询出age字段上非空的数据 POST /school/student/_search { \u0026quot;query\u0026quot;: { \u0026quot;exists\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;age\u0026quot; } } } #前缀查询，匹配具有包含带有指定前缀的术语的字段的文档 #查询出名称这个字段上为Jason的数据 POST /school/student/_search { \u0026quot;query\u0026quot;:{ \u0026quot;prefix\u0026quot;:{\u0026quot;name\u0026quot;: {\u0026quot;value\u0026quot;: \u0026quot;jason\u0026quot;} } } } #通配符查询,其中*代表是所有的，?代表的就是一个字符 #wildcard有点像like关键字，查询出name在jet?u的数据 POST /school/student/_search { \u0026quot;query\u0026quot;:{ \u0026quot;wildcard\u0026quot;: { \u0026quot;name\u0026quot;:{\u0026quot;value\u0026quot;: \u0026quot;jet?u\u0026quot;} } } } #模糊查询 #该fuzzy查询将生成在中指定的最大编辑距离内的匹配术语 POST /school/student/_search { \u0026quot;query\u0026quot;:{ \u0026quot;fuzzy\u0026quot;: { \u0026quot;name\u0026quot;:\u0026quot;jason\u0026quot; } } } ","date":"2022-05-06T14:22:04+08:00","permalink":"https://mikeLing-qx.github.io/p/elasticsearch_dsl%E8%AF%AD%E5%8F%A5%E7%A4%BA%E4%BE%8B%E4%B8%8E%E7%BB%83%E4%B9%A0/","title":"ElasticSearch_DSL语句示例与练习"},{"content":"1. 版本 elasticSearch\nhttps://www.elastic.co/cn/downloads/past-releases/elasticsearch-7-17-3\nkibana\nhttps://www.elastic.co/cn/downloads/past-releases/kibana-7-17-3\nik_smart\nhttps://github.com/medcl/elasticsearch-analysis-ik/releases/tag/v7.17.3\n2. 安装步骤 1. ES 解压 tar -xvf elasticsearch-7.17.3-linux-x86_64.tar.gz 创建es用户 ## ElasticSearch不能以Root身份运行， 需要单独创建一个用户， 并赋予目录权限 groupadd elsearch useradd elsearch -g elsearch -p elasticsearch chown -R elsearch:elsearch /opt/elasticsearch/elasticsearch-7.10.2 vi config/elasticsearch.yml 修改配置文件 # node名称 node.name: node-1 # 外网访问地址 network.host: 0.0.0.0 discovery.seed_hosts: [\u0026quot;node-1\u0026quot;] cluster.initial_master_nodes: [\u0026quot;node-1\u0026quot;] 启动 ## 切换用户 su elsearch ## 以后台常驻方式启动 bin/elasticsearch -d 启动错误\n需要配置\nvi /etc/sysctl.conf 添加\nvm.max_map_count=655360 vi /etc/security/limits.conf 末尾添加\nelsearch soft nproc 125535 elsearch hard nproc 125535 浏览器验证\nhttp://localhost:9200/_cat/health ik-smart 分词器放在 elasticsearch/plugins/ik/ 目录下, 解压重启就生效 3. kibana 解压 tar -xvf kibana-7.17.3-linux-x86_64.tar.gz Kibana启动不能使用root用户， 使用上面创建的elsearch用户， 进行赋权： chown -R elsearch:elsearch kibana-7.10.2 修改配置文件 # 服务端口 server.port: 5601 # 服务地址 server.host: \u0026quot;0.0.0.0\u0026quot; # elasticsearch服务地址 elasticsearch.hosts: [\u0026quot;http://localhost0:9200\u0026quot;] 启动 ./kibana -q 或者 nohup /bin/kibana \u0026amp; 访问 http://localhost:5601/app/home#/ ","date":"2022-04-27T14:27:14+08:00","permalink":"https://mikeLing-qx.github.io/p/elasticsearch_linux%E5%8D%95%E6%9C%BA%E9%83%A8%E7%BD%B2/","title":"ElasticSearch_Linux单机部署"},{"content":"Spring cloud Feign 1. 简介 配置nacos 配置中心\nspring: cloud: nacos: discovery: server-addr: 127.0.0.1:8848 1.1 Feign Feign是Spring Cloud组件中的一个轻量级RESTful的HTTP服务客户端 Feign==内置了Ribbon==，用来做客户端负载均衡，去调用服务注册中心的服 务。 Feign的使用方式是：==使用Feign的注解定义接口，调用这个接口，就可以调用服务注册中心的服务== Feign支持的注解和用法请参考官方文档：https://github.com/OpenFeig n/feign ==Feign本身不支持Spring MVC的注解，它有一套自己的注解==\n1.2 Feign 日志配置 Feign支持4中级别：\nNONE：不记录任何日志，默认值\nBASIC：仅记录请求的方法，URL以及响应状态码和执行时间\nHEADERS：在BASIC基础上，额外记录了请求和响应的头信息\nFULL：记录所有请求和响应的明细，包括头信息、请求体、元数据\nfeign: client: config: default: #这里default就是全局配置，如果是写服务名称，则是针对某个微服务的配置 LoggerLevel: FULL 1.3 Feign 数据压缩 feign: compression: request: enabled: true mime-types: text/html,application/xml,application/json # 设置压缩的数据类 型 min-request-size: 2048 # 触发压缩的大小下限 response: enabled: true # 响应压缩 1.2 OpenFeign ​\tOpenFeign是Spring Cloud 在Feign的基础上支持了Spring MVC的注解， 如@RequesMapping等，是一个轻量级的Http封装工具对象,大大简化了Http请 求，使得我们对服务的调用转换成了对本地接口方法的调用。 OpenFeign 的@FeignClient 可以解析SpringMVC的@RequestMapping 注解下的接口，==并通过动态代理的方式产生实现类，实现类中做负载均衡并调用其他服务。==\n集成了Ribbon的负载均衡功能 集成Hystrix的熔断器功能 支持请求压缩 大大简化了远程调用的代码，同时功能还增强啦 以更加优雅的方式编写远程调用代码，并简化重复代码 2. OpenFeign 应用 1:导入openfeign依赖 2:编写openfeign客户端接口-将请求地址写到该接口上 3:消费者启动引导类开启openfeign功能注解 @EnableFeignClients(basePackages = {\u0026quot;com.itheima.driver.feign\u0026quot;}) 4:访问接口测试 流程分析\nFeignClient 标注 对应的服务, 配置 fallback 降级 服务方法上通过spring mvc 的 注解 标注所要调用的方法 在启动类上标注EnableFeignClient 找到所有标注的了FeignClient 的接口生成代理 使用的时候通过@Autowired 自动注入 一个代理 依赖 \u0026lt;!--配置feign--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-openfeign\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.2.1.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 创建Feign客户端接口 @FeignClient(\u0026quot;hailtaxi-driver\u0026quot;) public interface DriverFeign { // http://ip:port/driver/status/{id}/{status} @PutMapping(value = \u0026quot;/driver/status/{id}/{status}\u0026quot;) public Driver status(@PathVariable(value = \u0026quot;id\u0026quot;)String id, @PathVariable(value = \u0026quot;status\u0026quot;)Integer status); } Feign会通过动态代理，帮我们生成实现类。 注解@FeignClient声明Feign的客户端，==注解value指明服务名称==\n接口定义的方法，采用SpringMVC的注解。Feign会根据注解帮我们生成URL 地址 注解@RequestMapping中的/driver，不要忘记。==因为Feign需要拼接可访问地址==\n3. 源码解析 问: 和 Gateway 有什么区别吗? Gateway 是通过 filter 转换成实际需要请求的地址\n1. 如何为接口生成代理并放入ioc 容器 (重点) 1. 注入FeignClientfactorybean 启动类上标注的 EnableFeignClients 为入口 @SpringBootApplication @EnableDiscoveryClient @EnableFeignClients(basePackages = {\u0026quot;com.itheima.driver.feign\u0026quot;}) public class OrderApplication { psvm } EnableFeignClients 源码 @Import @Import 注解支持==导入普通 java 类，并将其声明成一个bean==。主要用于将多个分散的 java config 配置类融合成一个更大的 config 类。\n@Import 注解在 4.2 之前只支持导入配置类。 在4.2之后 @Import 注解支持导入普通的 java 类,并将其声明成一个 bean。 @Retention(RetentionPolicy.RUNTIME) @Target({ElementType.TYPE}) @Documented @Import({FeignClientsRegistrar.class}) public @interface EnableFeignClients { .... } 启动扫描feign接口并注册 FeignClientsRegistrar#registerBeanDefinitions\npublic void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry) { this.registerDefaultConfiguration(metadata, registry); // 这里会注册所有@FeignClient 修饰的接口 this.registerFeignClients(metadata, registry); } FeignClientsRegistrar#registerFeignClient ==每个标注了 @FeignClient的接口, 真正向容器中注册的其实是一个 绑定了该接口信息的 FeignClientFactoryBean==\n1. 创建 FeignClientFactoryBean 的 BeanDefinition 1. 向BeanDefinition 中填充相关属性, 属性来源于接口上@FeignClient 的属性信息 private void registerFeignClient(BeanDefinitionRegistry registry, AnnotationMetadata annotationMetadata, Map\u0026lt;String, Object\u0026gt; attributes) { // 接口全路径 String className = annotationMetadata.getClassName(); /** * */ BeanDefinitionBuilder definition = BeanDefinitionBuilder .genericBeanDefinition(FeignClientFactoryBean.class); validate(attributes); ....\t} 向容器中注入 beanName: com.itheima.driver.feign.DriverFeign class: [org.springframework.cloud.openfeign.FeignClientFactoryBean] 2. FeignClientFactorybean 获取代理对象 代理对象是怎么生成的? 代理对象做了什么事? FeignClientFactoryBean#loadBalance 默认的LoadBalance 是 Hystrix\n代理对象Proxy 中\n总结\n@EnableFeignClient 注解包下的 所有@FeignClient 接口都会封装成 FeignClientFactoryBean FeignClientFactoryBean 的 getObject 方法会为 默认选择使用 Hrystrix 类型的 FeginLoadBalance 客户端 (可在配置文件中使用 Ribbon),使用jdk动态代理 生成代理对象, 在代理方法的 Handler 中 封装了Target (Feign就是实际需要调用的服务 名称 以及 url) 接口中所有方法 存储在 Dispatch 一个map中, key 是 method , value 是 该方法的处理器 2. 代理拦截后如何走网络调用的 1. 代理拦截 ReflectiveFeign.FeignInvocationHandler#invoke\n@Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { if (\u0026quot;equals\u0026quot;.equals(method.getName())) { try { Object otherHandler = args.length \u0026gt; 0 \u0026amp;\u0026amp; args[0] != null ? Proxy.getInvocationHandler(args[0]) : null; return equals(otherHandler); } catch (IllegalArgumentException e) { return false; } } else if (\u0026quot;hashCode\u0026quot;.equals(method.getName())) { return hashCode(); } else if (\u0026quot;toString\u0026quot;.equals(method.getName())) { return toString(); } // 从map 集合中根据方法Method 获取该方法的处理器 handler (SynchronousMethodHandler), 然后执行 return dispatch.get(method).invoke(args); } 2. 方法处理器执行请求 SynchronousMethodHandler#invoke\nclient.execute(request, options); // 使用feign客户端发送请求获取响应 3. 进行负载均衡调用 LoadBalancerFeignClient#execute\n4. Ribbon 中进行实际地址的替换 ","date":"2022-04-27T10:11:27+08:00","permalink":"https://mikeLing-qx.github.io/p/spring_cloud_feign/","title":"Spring_cloud_feign"},{"content":"1. 概述 ​ 传统阻塞的队列使用锁保证线程安全，而锁通过操作系统内核上下文切换实现，会暂停线程去等待 锁，直到锁释放。\n​\t执行这样的上下文切换，会丢失之前保存的数据和指令。由于消费者和生产者之间的速度差异，队 列总是接近满或者空的状态，这种状态会导致高水平的写入争用。\n==传统队列的==\nDisruptor其实就像一个队列一样，用于在不同的线程之间迁移数据，但是Disruptor也实现了一些\n其他队列没有的特性，如：\n同一个“事件”可以有多个消费者，==消费者之间既可以并行处理，也可以相互依赖形成处理的先后次序==(形成一个依赖图)； 预分配用于存储事件内容的内存空间； 针对极高的性能目标而实现的极度优化和无锁的设计； 2. 核心概念 1. RingBuffer Disruptor中的数据结构，用于存储生产者生产的数据\n环形的缓冲区，曾经 RingBuffer 是 Disruptor 中的最主要的对象，但从3.0版本开始，其职责被简化为仅仅负责对通过 Disruptor 进行交换的数据（事件）进行存储和更新。在一些更高级的应 用场景中，Ring Buffer 可以由用户的自定义实现来完全替代。\n2. Sequence 序号，在Disruptor框架中，任何地方都有序号\n==生产者生产的数据放在RingBuffer中的哪个位置，消费者应该消费哪个位置的数据==，RingBuffer中\n的某个位置的数据是什么，这些==都是由这个序号来决定的==。这个序号可以简单的理解为一个AtomicLong\n类型的变量。==其使用了padding的方法去消除缓存的伪共享问题==。\n3. Sequencer 序号生成器，这个类主要是用来协调生产者的\n在生产者生产数据的时候，Sequencer会产生一个可用的序号（Sequence），然后生产者就就知道\n数据放在环形队列的那个位置了。\n==Sequencer是Disruptor的真正核心，此接口有两个实现类 SingleProducerSequencer、 MultiProducerSequencer== ，它们定义在生产者和消费者之间快速、正确地传递数据的并发算法。\n4. Sequence Barrier 序号屏障\n我们都知道，消费者在消费数据的时候，需要知道消费哪个位置的数据。消费者总不能自己想取哪\n个数据消费，就取哪个数据消费吧。这个SequencerBarrier起到的就是这样一个“栅栏”般的阻隔作用。\n你消费者想消费数据，得，我告诉你一个序号（Sequence），你去消费那个位置上的数据。要是没有数\n据，就好好等着吧\n5. Wait Strategy Wait Strategy决定了==一个消费者怎么等待生产者==将事件（Event）放入Disruptor中。\n设想一种这样的情景：生产者生产的非常慢，而消费者消费的非常快。那么必然会出现数据不够的\n情况，这个时候消费者怎么进行等待呢？WaitStrategy就是为了解决问题而诞生的。\n6. Event ​\t从生产者到消费者传递的数据叫做Event。它不是一个被 Disruptor 定义的特定类型，而是由\nDisruptor 的使用者定义并指定\n7. EventHandler ​\tDisruptor 定义的事件处理接口，由用户实现，用于处理事件，是 Consumer 的真正实现。\n8. Producer ​\t即生产者，只是泛指调用 Disruptor 发布事件的用户代码，Disruptor 没有定义特定接口或类型。\n3. Disruptor 入门 添加依赖\n\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;com.lmax\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;disruptor\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;3.4.2\u0026lt;/version\u0026gt;\r\u0026lt;/dependency\u0026gt;\rEvent 事件 EventFactory 事件工厂 EventHandler 事件消费者 wait strategy 等待策略 1. 创建事件 首先创建一个 LongEvent 类，这个类将会被放入环形队列中作为消息内容。\n事件(Event)就是通过 Disruptor 进行交换的数据类型。\npublic class LongEvent {\rprivate long value;\rpublic void set(long value) {\rthis.value = value;\r}\rpublic long getValue() {\rreturn value;\r}\r}\r2. 定义事件工厂 ==为了使用Disruptor的内存预分配event，我们需要定义一个EventFactory==\n事件工厂(Event Factory)定义了==如何实例化前面第1步中定义的事件(Event)==，需要实现接口 com.lmax.disruptor.EventFactory。\nDisruptor 通过 EventFactory 在 RingBuffer 中预创建 Event 的实例。\n一个 Event 实例实际上被用作一个“数据槽”，发布者发布前，先从 RingBuffer 获得一个 Event 的实\n例，然后往 Event 实例中填充数据，之后再发布到 RingBuffer 中，之后由 Consumer 获得该 Event 实\n例并从中读取数据\npublic class LongEventFactory implements EventFactory\u0026lt;LongEvent\u0026gt; {\rpublic LongEvent newInstance() {\rreturn new LongEvent();\r}\r}\r3. 定义事件处理的 handler 为了让消费者处理这些事件，所以我们这里定义一个事件处理器，负责打印event\n通过实现接口 com.lmax.disruptor.EventHandler 定义事件处理的具体实现。\npublic class LongEventHandler implements EventHandler\u0026lt;LongEvent\u0026gt; {\rpublic void onEvent(LongEvent event, long sequence, boolean endOfBatch) {\rCommonUtils.calculation();\r// System.out.println(\u0026quot;consumer:\u0026quot; + Thread.currentThread().getName() + \u0026quot; Event: value=\u0026quot; + event.getValue() + \u0026quot;,sequence=\u0026quot; + sequence + \u0026quot;,endOfBatch=\u0026quot; + endOfBatch);\r}\r}\r4. 等待策略 Disruptor 定义了 com.lmax.disruptor.WaitStrategy 接口用于抽象 Consumer 如何等待新事件， 这是策略模式的应用\nWaitStrategy YIELDING_WAIT = new YieldingWaitStrategy();\r5. 启动Disruptor RingBuffer 必须是2的n次方\npublic static void main(String[] args) {\r// 指定事件工厂\rLongEventFactory factory = new LongEventFactory();\r// 指定 ring buffer字节大小, 必须是2的N次方\rint bufferSize = 1024;\r//单线程模式，获取额外的性能\rDisruptor\u0026lt;LongEvent\u0026gt; disruptor = new Disruptor\u0026lt;LongEvent\u0026gt;(factory,\rbufferSize, Executors.defaultThreadFactory(),\rProducerType.SINGLE,\rnew YieldingWaitStrategy());\r//设置事件业务处理器---消费者\rdisruptor.handleEventsWith(new LongEventHandler());\r//启动disruptor线程\rdisruptor.start();\r// 获取 ring buffer环，用于接取生产者生产的事件\rRingBuffer\u0026lt;LongEvent\u0026gt; ringBuffer = disruptor.getRingBuffer();\r//为 ring buffer指定事件生产者\rLongEventProducerWithTranslator producer = new LongEventProducerWithTranslator(ringBuffer);\r//循环遍历\rfor (int i = 0; i \u0026lt; 100; i++) {\r//获取一个随机数\rlong value = (long) ((Math.random() * 1000000) + 1);\r//发布数据\rproducer.onData(value);\r}\r//停止disruptor线程\rdisruptor.shutdown();\r}\r6. 使用Translators发布事件 由于加入了丰富的Lambda风格的API，可以用来帮组开发人员简化流 程。所以在3.0版本后==首选使用Event Publisher/Event Translator来发布事件==\nEvent Publisher 负责将事件发布到 Disruptor 的 RingBuffer 中。它的主要任务是将新的事件放入 RingBuffer 中，以供消费者线程进行处理。 Event Translator 是一个用于将外部数据转换为 Disruptor 事件的接口。它负责将传入的数据转换为 Disruptor 内部的事件对象，并将其放入 RingBuffer 中进行处理。 EventTranslator 是一个接口，通常通过实现 EventTranslatorOneArg, EventTranslatorTwoArg, 或 EventTranslatorThreeArg 接口来创建。不同的接口支持不同数量的参数传递。 在发布事件时，==Event Publisher 会使用 Event Translator 来进行数据转换==。 public class LongEventProducerWithTranslator {\rprivate final RingBuffer\u0026lt;LongEvent\u0026gt; ringBuffer;\rpublic LongEventProducerWithTranslator(RingBuffer\u0026lt;LongEvent\u0026gt; ringBuffer) {\rthis.ringBuffer = ringBuffer;\r}\rprivate static final EventTranslatorOneArg\u0026lt;LongEvent, Long\u0026gt; TRANSLATOR =\rnew EventTranslatorOneArg\u0026lt;LongEvent, Long\u0026gt;() {\rpublic void translateTo(LongEvent event, long sequence, Long data) {\revent.set(data);\r}\r};\rpublic void onData(Long data) {\rringBuffer.publishEvent(TRANSLATOR, data);\r}\r}\r4. 高性能原理 引入环形的数组结构：数组元素不会被回收，避免频繁的GC， 无锁的设计：采用CAS无锁方式，保证线程的安全性 属性填充：通过添加额外的无用信息，避免伪共享问题 元素位置的定位：采用跟一致性哈希一样的方式，一个索引，进行自增 0. MESI 协议 ​\tMESI 协议（Modified, Exclusive, Shared, Invalid）是一种缓存一致性协议，用于确保多核处理器系统中的缓存数据的一致性。它确保各个核心的缓存中保存的数据保持同步，避免数据冲突和不一致。下面是 MESI 协议的四种状态及其简单解释：\nModified（修改）：数据在该缓存中是修改过的，并且是唯一的拷贝。其他缓存没有这块数据。这个缓存中的数据已被更改，但尚未写回主内存。 Exclusive（独占）：数据在该缓存中是唯一的拷贝，并且与主内存中的数据一致。该缓存中的数据没有被修改。 Shared（共享）：数据可能存在于一个或多个缓存中，并且与主内存中的数据一致。这意味着其他缓存也有这块数据的拷贝。 Invalid（无效）：数据在该缓存中是无效的，不可靠。缓存中的数据可能已被其他缓存修改或更新，不能再使用。 ==类比解释==\n想象你和你的朋友在一个图书馆里读书，你们都有一个笔记本记录书中的内容。这就像是缓存的工作，每个人都有自己的“缓存”（笔记本）。\nModified（修改）： 你在笔记本上写下了一些新的笔记，并且这些笔记是你自己写的，没有别人写过。你笔记本上的内容与图书馆的原书（主内存）不一致，因为你做了修改。 Exclusive（独占）： 你有一本独特的书，只有你拥有。这本书的内容完全和图书馆的原书一致，没有人修改过它。 Shared（共享）： 你和你的朋友都有相同的笔记内容，你们都在各自的笔记本上记录了这些内容。这些内容和图书馆的原书一致，但有多个人都有这些笔记。 Invalid（无效）： 你发现你的笔记本上有一些过时的内容，这些内容已经不再准确了，因为图书馆的原书已经更新了，你的笔记需要更新 1. 缓存行 缓存行（Cache Line）是现代计算机体系结构中缓存的基本单位。它是处理器缓存（如 L1、L2、L3 缓存）中的一个数据块，用于提高内存访问的效率。\n处理器缓存中的数据块，通常大小为 64 字节\n2. 伪共享 当多个==线程操作的变量位于同一个缓存行中时，即使这些变量并不相互干扰，也可能导致性能问题==。这种现象称为伪共享。伪共享会导致==处理器频繁地更新缓存行，增加缓存一致性协议的开销，从而降低性能==。 ​\t解决伪共享的一种方法是将每个线程的变量放在不同的缓存行中，从而避免它们位于同一个缓存行中。另一个方法是==使用缓存行填充技术==，将==相关的数据放置在缓存行的不同部分==。\n3. 无锁设计 在多线程竞争下，加锁、释放锁会导致比较多的上下文切换和调度延时，引起性能问题，而且在上 下文切换的时候，cpu之前缓存的指令和数据都将失效，对性能有很大的损失，用户态的锁虽然避 免了这些问题，但是其实它们只是在没有真实的竞争时才有效。\n一个线程持有锁会导致其它所有需要此锁的线程挂起直至该锁释放。\n如果一个优先级高的线程等待一个优先级低的线程释放锁会导致导致优先级反转(Priority\nInversion)，引起性能风险。\n1. CAS 比较与交换 CAS的语义是“我认为V的值应该为A，如果是，那么将V的值更新为B， 否则不修改并告诉V的值实际为多少”, 乐观锁 技术，当多个线程尝试使用CAS同时更新同一 个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，失败的线程并不会被挂起，而是被 告知这次竞争中失败，并可以再次尝试\n2. Disruptor 的无锁设计 多线程环境下，多个生产者通过do/while循环的条件CAS，来判断每次申请的空间是否已经被其他 生产者占据。假如已经被占据，该函数会返回失败，While循环重新执行，申请写入空间。\ndo {\rcurrent = cursor.get();\rnext = current + n;\rif (!hasAvailableCapacity(gatingSequences, n, current)) {\rthrow InsufficientCapacityException.INSTANCE;\r}\r}\rwhile (!cursor.compareAndSet(current, next));\r//next 类比于ArrayBlockQueue的数组索引index\rreturn next;\r3. 环形数组 ​\t为数组预先分配内存，使得数组对象一直存在, 这样就 不需要花大量的时间用于垃圾回收。 数组的大小为2的n次方，这样==元素定位可以通过位运算效率会更高==\n5. 等待策略 BlockingWaitStrategy Disruptor的==默认策略==是BlockingWaitStrategy，在BlockingWaitStrategy内部是使用锁和condition 来控制线程的唤醒 ==BlockingWaitStrategy是最低效的策略==，==但其对CPU的消耗最小并且在各种不同部署环境中能提供 更加一致的性能表现==。\nSleepingWaitStrategy SleepingWaitStrategy 的性能表现跟 BlockingWaitStrategy 差不多，对 CPU 的消耗也类似，但其 对生产者线程的影响最小，通过使用 LockSupport.parkNanos(1) 来实现循环等待，适合用于异步日志\nYieldingWaitStrategy YieldingWaitStrategy是可以使用在低延迟系统的策略之一，YieldingWaitStrategy将自旋以等待序 列增加到适当的值。在循环体内，将调用 Thread.yield() 以允许其他排队的线程运行。在要求极高性 能且事件处理线数小于 CPU 逻辑核心数的场景中，推荐使用此策略；\nBusySpinWaitStrategy 性能最好，适合用于低延迟的系统，在要求极高性能且事件处理线程数小于CPU逻辑核心数的场景 中，推荐使用此策略；\nPhasedBackoffWaitStrategy 自旋 + yield + 自定义策略，CPU资源紧缺，吞吐量和延迟并不的场景。\n6. 生产和消费模式 ==在Disruptor中生产者分为单生产者和多生产者，而消费者并没有区分==\n单生产者情况下，就是普通的生产者向RingBuffer中放置数据，消费者获取最大可消费的位置，并 进行消费。\n多生产者时候，又多出了一个跟RingBuffer同样大小的Buffer，==称为AvailableBuffer==。 在多生产者中，每个生产者==首先通过CAS竞争获取可以写的空间，然后再进行慢慢往里放数据==，如 果正好这个时候消费者要消费数据，那么==每个消费者都需要获取最大可消费的下标==，这个下标是在 AvailableBuffer进行获取得到的最长连续的序列下标。\n1. 单生产者生产数据 申请写入m个元素；\n若是有m个元素可以入，则返回最大的序列号。这儿主要判断是否会覆盖未读的元素；\n若是返回的正确，则生产者开始写入元素。\n2. 多生产者生产数据 ​\t多个生产者的情况下，会遇到“如何防止多个线程重复写同一个元素”的问题。Disruptor的解决方法 是，==每个线程获取不同的一段数组空间进行操作==。这个通过CAS很容易达到。==只需要在分配元素的时候，通过CAS判断一下这段空间是否已经分配出去==即可。\n但是会遇到一个新问题：\n==如何防止读取的时候，读到还未写的元素==。Disruptor在多个生产者的情况\n下，引入了一个与Ring Buffer大小相同的buffer：==available Buffer。当某个位置写入成功的时候，便把 availble Buffer相应的位置置位，标记为写入成功。读取的时候，会遍历available Buffer，来判断元素 是否已经就绪==。\n申请写入m个元素；\n若是有m个元素可以写入，则返回最大的序列号。每个生产者会被分配一段独享的空间；\n生产者写入元素，写入元素的同时设置available Buffer里面相应的位置，以标记自己哪些位置是已 经写入成功的。\n如下图所示，Writer1和Writer2两个线程写入数组，都申请可写的数组空间。Writer1被分配了下 标3到下表5的空间，Writer2被分配了下标6到下标9的空间。\nWriter1写入下标3位置的元素，同时把available Buffer相应位置置位，标记已经写入成功，往后 移一位，开始写下标4位置的元素。Writer2同样的方式。最终都写入完成。\n3. 多生产者消费数据 ​ 假设三个生产者在写中，==还没有置位AvailableBuffer，那么消费者可获取的消费下标只能获取到 6==然后等生产者都写OK后，通知到消费者，消费者继续重复上面的步骤\n==消费流程==\n申请读取到序号n；\n若writer cursor \u0026gt;= n，这时仍然无法确定连续可读的最大下标。从reader cursor开始读取\navailable Buffer，一直查到第一个不可用的元素，然后返回最大连续可读元素的位置；\n消费者读取元素 如下图所示，读线程读到下标为2的元素，三个线程Writer1/Writer2/Writer3正在向RingBuffer相\n应位置写数据，==写线程被分配到的最大元素下标是11==。\n==读线程申请读取到下标从3到11的元素，判断writer cursor\u0026gt;=11。然后开始读取 availableBuffer，从3开始，往后读取，发现下标为7的元素没有生产成功，于是WaitFor(11)返回 6==。\n然后，消费者读取下标从3到6共计4个元素。 ==也就是只能读到写线程最小的下标==\n7. 高级使用 1. 单一写者模式 在并发系统中==提高性能最好的方式之一就是单一写者原则==，对Disruptor也是适用的。如果在你的代\n码中仅仅有一个事件生产者，那么可以设置为单一生产者模式来提高系统的性能。\n2. 串行消费 比如：现在触发一个注册Event，需要有一个Handler来存储信息，一个Hanlder来发邮件等等\n/**\r* 串行依次执行\r* \u0026lt;br/\u0026gt;\r* p --\u0026gt; c11 --\u0026gt; c21\r* @param disruptor\r*/\rpublic static void serial(Disruptor\u0026lt;LongEvent\u0026gt; disruptor){\rdisruptor.handleEventsWith(new C11EventHandler()).then(new\rC21EventHandler());\rdisruptor.start();\r}\r3. 菱形消费 public static void diamond(Disruptor\u0026lt;LongEvent\u0026gt; disruptor){\rdisruptor.handleEventsWith(new C11EventHandler(),new\rC12EventHandler()).then(new C21EventHandler());\rdisruptor.start();\r}\r4. 链式并行 public static void chain(Disruptor\u0026lt;LongEvent\u0026gt; disruptor){\rdisruptor.handleEventsWith(new C11EventHandler()).then(new\rC12EventHandler());\rdisruptor.handleEventsWith(new C21EventHandler()).then(new\rC22EventHandler());\rdisruptor.start();\r}\r5. 相互隔离 public static void parallelWithPool(Disruptor\u0026lt;LongEvent\u0026gt; disruptor){\rdisruptor.handleEventsWithWorkerPool(new C11EventHandler(),new\rC11EventHandler());\rdisruptor.handleEventsWithWorkerPool(new C21EventHandler(),new\rC21EventHandler());\rdisruptor.start();\r}\r6. 航道模式 /**\r* 串行依次执行,同时C11，C21分别有2个实例\r* \u0026lt;br/\u0026gt;\r* p --\u0026gt; c11 --\u0026gt; c21\r* @param disruptor\r*/\rpublic static void serialWithPool(Disruptor\u0026lt;LongEvent\u0026gt; disruptor){\rdisruptor.handleEventsWithWorkerPool(new C11EventHandler(),new\rC11EventHandler()).then(new C21EventHandler(),new C21EventHandler());\rdisruptor.start();\r}\r","date":"2022-03-23T18:14:03+08:00","permalink":"https://mikeLing-qx.github.io/p/disruptor/","title":"Disruptor"},{"content":"1. filter 参考资料: https://segmentfault.com/a/1190000040755445\ndubbo filter是dubbo提供的一项扩展的功能，dubbo在做调用的过程中，会先经过一层filter，顾名思义，也就是一层拦截过滤。通过dubbo filter功能，我们可以记录一些额外的操作日志、传递一些公共的信息、做一些自定义校验和权限控制等。\ndubbo 提供了Filter接口类，我们只要定义一个自己的类，然后继承该接口即可实现自己的逻辑。 ​\t以实现一个传递链路唯一追踪号为案例来看下如何实现，比如现在我有一个A服务，还有一个B服务，现在A服务有自己的逻辑链路追踪号，需要传递到B服务中去，使得两个服务能够用同一个追踪号来关联某一次请求的所有链路。 A服务先定义一个自己的Filter类并实现dubbo的Filter接口：\n//对于服务提供端，Activate改为Constants.PROVIDER\r@Activate(group = {Constants.CONSUMER})\rpublic class UniqIdTraceFilter implements Filter {\r@Override\rpublic Result invoke(Invoker\u0026lt;?\u0026gt; invoker, Invocation invocation) throws RpcException {\r//此处逻辑可以自定义\rString traceId = UUID.randomUUID().toString().replace(\u0026quot;-\u0026quot;, \u0026quot;\u0026quot;);\rRpcContext.getContext().setAttachment(\u0026quot;traceId\u0026quot;, traceId);\rreturn invoker.invoke(invocation);\r}\r}\r再在resources目录下新建一个META-INF文件夹，在其下再创建一个dubbo目录。在META-INF\\dubbo目录下创建一个名称为org.apache.dubbo.rpc.Filter（如果你用的是2.7之前的版本，则名称为com.alibaba.dubbo.rpc.Filter），如图\n#在文件里写上你定义的filter全名\runiqIdTraceFilter=com.example.dubboanalyze.filter.UniqIdTraceFilter\r使用内置的 Filter 实现 (默认是不开启的)\n配置文件使用方式\n注解使用方式\n2. 泛化调用 ​\t泛化接口调用方式主要用于客户端没有 API 接口及模型类元的情况，这是官方的说法，对于新人我认为太抽象了，这里解释一下。我们在做正常的dubbo调用时，==服务端会提供一个所有接口的jar包让客户端引入==，此时客户端就知道服务端提供了哪些接口并进行使用，参考 如何搭建并进行调用，而对于泛化调用，我们还是需要知道服务端提供了哪些接口，只不过是程序不知道而已，此时我们==在程序中使用泛化调用，显示填入需要调用的接口名称，dubbo会进行匹配并进行调用后返回==\n服务端接口\npackage com.example.dubboprovider.rpc;\rpublic interface CityService {\rString getCityName();\r}\r泛化调用最重要的就是 GenericService，首先我们定义一个服务引用\n@Reference(interfaceName = \u0026quot;com.example.dubboprovider.rpc.CityService\u0026quot;, generic = true)\rGenericService genericService;\r注意上面的注解 `@Reference`，在之前的文章中说到过这个注解表明是一个dubbo引用，注解里面 `interfaceName` 用来表明你要使用的接口类，`generic` 为true用来表明是泛化调用，这样一个泛化接口就完成了。\r再来看下调用\nObject name = genericService.$invoke(\u0026quot;getCityName\u0026quot;, new String[]{}, new Object[]{});\r原理: 两个filter分别叫做 GenericImplFilter、GenericFilter。\n这两个filter就是对泛化调用的处理，其中 GenericImplFilter 是消费端的处理，而 GenericFilter 是服务端的处理。消费端的处理就是对参数做一些预处理（非内置的进行序列化、以及获得结果之后的反序列化，内置的进行校验以及传递generic值），而服务端的处理就是对内置的泛化调用进行处理（序列化以及返序列化），从逻辑中我们可以看到其实 generic 可以有三种赋值方式，分别为true、nativejava以及bean，其实就是对应的对于我们传递的参数的解析方式，下面分别介绍一下： true 这个是最简单的处理方式，dubbo内置的PojoUtils工具类就会进行解析。 nativejava 如果参数是byte数组类型的，则我们可以通过实现 Serialization 接口来自定义实现序列化和反序列化。 bean 如果你的参数类继承JavaBeanDescriptor，则可以通过该种方式来转换对象。 dubbo内置的类型转换已经很全了，如果你有自己的转换需求，可以考虑在这一块进行改造。 从源码中我们也可以看出来，其实泛化调用就是通过filter过滤对传递的参数进行了一层转换，然后找到匹配的接口以及方法进行调用。 3. 隐式传参 RpcContext.getContext().setAttachment(\u0026quot;traceId\u0026quot;, traceId); ","date":"2021-10-13T16:23:12+08:00","permalink":"https://mikeLing-qx.github.io/p/dubbo/","title":"Dubbo"},{"content":"Arthas 使用入门 1 安装启动 下载并启动 curl -O https://alibaba.github.io/arthas/arthas-boot.jar java -jar arthas-boot.jar 选择需要attach的程序 2 常用命令 2.1. dashboard dashboard 命令可以查看当前系统的实时数据面板。可以查看到CPU、内存、GC、运行环境等信息。\nctrl + c 可以退出dashboard 命令\n2.2. Thread thread {id} 命令会打印线程ID 的栈。用 thread 1 | grep \u0026lsquo;main(\u0026rsquo; 查找到main class。\nthread -b : 显示当前阻塞的线程\n目前只支持找出synchronized关键字阻塞住的线程， 如果是java.util.concurrent.Lock， 目前还不支持 thread -i 1000 -n 3: 每过 1000 毫秒进行采样，显示最占 CPU 时间的前 3 个线程\nthread \u0026ndash;state WAITING 查看处于等待状态的线程\n2.3. trace trace dh.webapi.form.controller.TemplateController getDetail 使用 trace 命令可以跟踪统计方法耗时。\n继续跟踪耗时高的方法，然后再次访问。\n比如使用一个 Springboot 项目（当然，不想 Springboot 的话，你也可以直接在 UserController 里 main 方法启动）控制层 getUser 方法调用了 userService.get(uid);，这个方法中分别进行 check、service、redis、mysql 等操作操作。就可以根据这个命令跟踪出来哪里的耗时最长。 2.4. jad 反编译指定已加载类的源码 jad dh.webapi.form.biz.AreaBiz 2.5. monitor 每 2秒统计一次 dh.webapi.form.controller.TemplateController 类的 getDetail 方法执行情况：\nmonitor -c 2 dh.webapi.form.controller.TemplateController getDetail 2.6 sc/ sm 查找JVM里已加载的类 sc 命令来查找JVM里已加载的类,通过-d参数，可以打印出类加载的具体信息，很方便查找类加载问题。并且支持统配 例如搜索 所有的 StringUtils , sc *StringUtils\nsm 命令是查找类的具体函数; -d参数可以打印函数的具体属性\nsm -d java.math.RoundingMode 查找特定的函数，比如查找构造函数\nsm java.math.RoundingMode \u0026lt;init\u0026gt; 2.8. watch watch dh.webapi.evidence.biz.EvidenceBiz doPostHashEvidence \u0026quot;{params,returnObj}\u0026quot; -x 5 -b -s watch dh.framework.starter.shiro.filter.ShiroAuthcFilter onAccessDenied \u0026quot;{params,returnObj}\u0026quot; -x 5 -b -s dh.framework.starter.shiro.session.RedisSessionDao doReadSession dh.webapi.zhidun.dao.OwnerEvidenceMapper listEvidence dh.framework.common.util.Requests post(java.lang.String, java.lang.String) dh.webapi.evidence.biz.EvidenceBiz doPostHashEvidence ","date":"2021-08-22T16:04:14+08:00","permalink":"https://mikeLing-qx.github.io/p/arthas/","title":"Arthas"},{"content":"概览 1.介绍一下 java 吧\n2.java 有哪些数据类型？\n3.接口和抽象类有什么区别？\n4.重载和重写什么区别？\n5.常见的异常有哪些？\n6.异常要怎么解决？\n7.arrayList 和 linkedList 的区别？\n8.hashMap 1.7 和 hashMap 1.8 的区别？\n9.hashMap 线程不安全体现在哪里？\n10.那么 hashMap 线程不安全怎么解决？\n11.concurrentHashMap 1.7 和 1.8 有什么区别\n12.介绍一下 hashset 吧\n13.什么是泛型？\n14.泛型擦除是什么？\n15.说说进程和线程的区别？\n16.volatile 有什么作用？\n17.什么是包装类？为什么需要包装类？\n18.Integer a = 1000，Integer b = 1000，a==b 的结果是什么？那如果 a，b 都为1，结果又是什么？\n19.JMM 是什么？\n20.创建对象有哪些方式\n21.讲讲单例模式懒汉式吧\n22.volatile 有什么作用\n23.怎么保证线程安全？\n24.synchronized 锁升级的过程\n25.cas 是什么？\n26.聊聊 ReentrantLock 吧\n27.多线程的创建方式有哪些？\n28.线程池有哪些参数？\n29.线程池的执行流程？\n30.线程池的拒绝策略有哪些？\n31.介绍一下四种引用类型?\n32.深拷贝、浅拷贝是什么？\n33.聊聊 ThreadLocal 吧\n34.一个对象的内存布局是怎么样的?\n35.方法参数传递\n1. 介绍一下java java 是一门==「开源的跨平台的面向对象的」==计算机语言\n跨平台是因为 java 的 class 文件是运行在虚拟机上的,其实跨平台的,而**「虚拟机是不同平台有不同版本」**,所以说 java 是跨平台的.\n面向对象特点\n1.「封装」\n两层含义：一层含义是把对象的属性和行为看成一个密不可分的整体，将这两者\u0026rsquo;封装\u0026rsquo;在一个不可分割的**「独立单元」**(即对象)中 另一层含义指\u0026rsquo;信息隐藏，把不需要让外界知道的信息隐藏起来，有些对象的属性及行为允许外界用户知道或使用，但不允许更改，而另一些属性或行为，则不允许外界知晓，或只允许使用对象的功能，而尽可能**「隐藏对象的功能实现细节」**。 1.良好的封装能够**「减少耦合」，符合程序设计追求\u0026rsquo;高内聚，低耦合\u0026rsquo; 2.「类内部的结构可以自由修改」** 3.可以对成员变量进行更**「精确的控制」** 4.**「隐藏信息」**实现细节\n2.「继承」\n继承就是子类继承父类的特征和行为，使得子类对象（实例）具有父类的实例域和方法，或子类从父类继承方法，使得子类具有父类相同的行为。 1.提高类代码的**「复用性」** 2.提高了代码的**「维护性」**\n3.「多态」\n1.「方法重载」：在一个类中，允许多个方法使用同一个名字，但方法的参数不同，完成的功能也不同。 2.「对象多态」：子类对象可以与父类对象进行转换，而且根据其使用的子类不同完成的功能也不同（重写父类的方法）。 多态是同一个行为具有多个不同表现形式或形态的能力。Java语言中含有方法重载与对象多态两种形式的多态： 「消除类型之间的耦合关系」 「可替换性」 「可扩充性」 「接口性」 「灵活性」 「简化性」 2. Java基础数据类型 3. 接口和抽象类的区别 1.接口是抽象类的变体，「接口中所有的方法都是抽象的」。而抽象类是声明方法的存在而不去实现它的类。 2.接口可以多继承，抽象类不行。 3.接口==定义方法，不能实现==，默认是 ==「public abstract」==，而抽象类可以实现部分方法。 ==接口中基本数据类型为 「public static final」== 并且需要给出初始值，而抽类象不是的 4. 重载和重写的区别 重写：\n1.参数列表必须**「完全与被重写的方法」**相同，否则不能称其为重写而是重载. 2.「返回的类型必须一直与被重写的方法的返回类型相同」，否则不能称其为重写而是重载。 3.访问**「修饰符的限制一定要大于被重写方法的访问修饰符」** 4.重写方法一定**「不能抛出新的检查异常或者比被重写方法申明更加宽泛的检查型异常」**。 重载：\n1.必须具有**「不同的参数列表」**； 2.可以有不同的返回类型，只要参数列表不同就可以了； 3.可以有**「不同的访问修饰符」**； 4.可以抛出**「不同的异常」** 5. 异常的处理方法 Java标准库内建了一些通用的异常，这些类以Throwable为顶层父类\nThrowable又派生出**「Error类和Exception类」**\n错误：Error类以及他的子类的实例，代表了JVM本身的错误。错误不能被程序员通过代码处理，Error很少出现。因此，程序员应该关注Exception为父类的分支下的各种异常类\n异常：Exception以及他的子类，代表程序运行时发送的各种不期望发生的事件。可以被Java异常处理机制使用，是异常处理的核心\ntry() catch()\nthrow\n作用是抛出一个异常, 抛出 方法throws\n表示此方法不处理异常, 而交给方法调用处进行处理 6. ArrayList 和 LinkedList 的区别 数据结构不同\nArrayList 是基于数组的, 存储空间是连续的 LinkedList 是基于链表的, 双向链表 , 存储空间不连续 随机访问\n对于get 和 set , ArrayList 比较好, 因为不需要移动指针 新增和删除\nLinkList 占优势, 因为 ArrayList 需要移动数据 空间大小\n同样数据量, LinkedList 所占的空间会更小, 因为 ArrayList ==需要预留空间== 方便后面的数据增加; LinkedList 增加数据只要 ==增加一个节点== 7. HashMap 1.7 和 1.8 的区别 不同点 hashMap 1.7 hashMap 1.8 数据结构 数组+链表 数组+链表+红黑树 插入数据的方式 头插法 尾插法 hash 值计算方式 9次扰动处理(4次位运算+5次异或) 2次扰动处理(1次位运算+1次异或) 扩容策略 插入前扩容 插入后扩容 9. HashMap 线程不安全的体现 在 **「hashMap1.7 中扩容」**的时候，因为采用的是头插法，所以会可能会有循环链表产生，导致数据有问题，在 1.8 版本已修复，改为了尾插法\n在任意版本的 hashMap 中，如果在**「插入数据时多个线程命中了同一个槽」**，可能会有数据覆盖的情况发生，导致线程不安全。\n10. HashMap 线程不安全怎么解决 直接加锁 使用hashTable, 其实就是在方法上加了synchronized 使用concurrentHashMap, 不管是1.7 和 1.8, 本质都是==减小了锁的力度, 减少锁竞争== 11. concurrentHashMap 1.7和 1.8的区别 不同点 concurrentHashMap 1.7 concurrentHashMap 1.8 锁粒度 基于segment 基于entry节点 锁 reentrantLock synchronized 底层结构 Segment + HashEntry + Unsafe Synchronized + CAS + Node + Unsafe 12. HashSet set 继承于 Collection 接口, 是一个 ==无序不重复集合==\n==基于 HashMap 实现的, 底层采用HashMap 来保存元素==\n元素的哈希值是通过元素的 hashcode 方法 来获取的, HashSet 首先判断两个元素的哈希值，如果哈希值一样，接着会比较 ==equals 方法 如果 equls 结果为 true== ，HashSet 就视为同一个元素。如果 equals 为 false 就不是同一个元素。\n13. 什么是泛型 把==类型明确的工作, 推迟==到 ==创建对象== 或 ==调用方法==的时候 才去明确的 特殊 类型\n14. 泛型擦除 因为泛型其实只是在编译器中实现的而虚拟机并不认识泛型类项，所以要在虚拟机中将泛型类型进行擦除。也就是说，==「在编译阶段使用泛型，运行阶段取消泛型，即擦除」==。擦除是将泛型类型以其父类代替，如String 变成了Object等。其实在使用的时候还是进行带强制类型的转化，只不过这是比较安全的转换，因为在编译阶段已经确保了数据的一致性\n15. 进程和线程的区别 「进程是系统资源分配和调度的基本单位」，它能并发执行较高系统资源的利用率. **「线程」是「比进程更小」**的能独立运行的基本单位, 创建、销毁、切换==成本要小==于进程,可以减少程序并发执行时的时间和空间开销，使得操作系统具有更好的并发性。 16. volatile 的作用 保证共享变量的可见性和有序性\n「1.保证内存可见性」\n可见性是指线程之间的可见性，一个线程修改的状态对另一个线程是可见的。也就是一个线程修改的结果，另一个线程马上就能看到。 「2.禁止指令重排序」\n17. 简述包装类, 为什么需要包装类 八种基本类型, 对应8 种包装类\n「为什么需要包装类」:\n基本数据类型方便、简单、高效，==但泛型不支持、集合元素不支持== 不符合面向对象思维 包装类提供很多方法，方便使用，如 Integer 类 toHexString(int i)、parseInt(String s) 方法等等 18. 包装类的缓存 Integer a = 1000，Integer b = 1000，a==b 结果为**「false」**\nInteger a = 1，Integer b = 1，a==b 结果为**「true」**\n这道题主要考察 Integer 包装类缓存的范围,「在-128~127之间会缓存起来」,比较的是直接缓存的数据,在此之外比较的是对象\n19. JMM JMM 就是 ==「Java内存模型」==(java memory model)。因为在不同的硬件生产商和不同的操作系统下，内存的访问有一定的差异，所以会造成相同的代码运行在不同的系统上会出现各种问题。所以java内存模型(JMM)「屏蔽掉各种硬件和操作系统的内存访问差异，以实现让java程序在各种平台下都能达到一致的并发效果」。\nJava内存模型规定所有的变量都存储在主内存中，包括实例变量，静态变量，但是不包括局部变量和方法参数。每个线程都有自己的工作内存，线程的工作内存保存了该线程用到的变量和主内存的副本拷贝，线程对变量的操作都在工作内存中进行。==「线程不能直接读写主内存中的变量」==。\n每个线程的工作内存都是==独立的==，==**「线程操作数据只能在工作内存中进行，然后刷回到主存」==。这是 Java 内存模型定义的线程基本工作方式。\n20. 创建对象的方式 1. new 关键字\rPerson person = new Person();\r2. Class.newInstance\rPerson p1 = Person.class.newInstance();\r3. Constructor.newInstance\rConstructor\u0026lt;Person\u0026gt; constructor = Person.class.getConstructor();\r4. clone\rPerson person = new Person();\rPerson person2 = person.clone();\r5. 反序列化\rPerson p1 = new Person();\rbyte[] bytes = SerializationUtils.serialize(p1);\rPerson p2 = (Person)SerializationUtils.deserialize(bytes);\r21. 讲讲单例懒汉模式 // 懒汉式\rpublic class Singleton {\r// 延迟加载保证多线程安全\rPrivate volatile static Singleton singleton;\rprivate Singleton(){\r}\rpublic static Singleton getInstance(){\rif(singleton == null){\rsynchronized(Singleton.class){\rif(singleton == null){\rsingleton = new Singleton();\r}\r}\r}\rreturn singleton;\r}\r}\r使用 volatile 是**「防止指令重排序，保证对象可见」**，防止读到半初始化状态的对象 第一层if(singleton == null) 是为了防止有多个线程同时创建 synchronized 是加锁防止多个线程同时进入该方法创建对象 第二层if(singleton == null) 是防止有多个线程同时等待锁，一个执行完了后面一个又继续执行的情况 22. 如何保证线程安全? synchronized 关键字 代码块, 方法 (静态方法, 同步锁是当前字节码对象, 实例方法, 同步锁是实例对象) lock 锁机制 (必须要在finally 中进行释放)\nLock lock = new ReentrantLock();\rlock. lock();\rtry {\rSystem. out. println(\u0026quot;获得锁\u0026quot;);\r} catch (Exception e) {\r} finally {\rSystem. out. println(\u0026quot;释放锁\u0026quot;);\rlock. unlock();\r}\r23. synchronized 锁升级的过程 在 Java1.6 之前的版本中，synchronized 属于重量级锁，效率低下，「锁是」 cpu 一个**「总量级的资源」**，每次获取锁都要和 cpu 申请，非常消耗性能。\n在 「jdk1.6 之后」 Java 官方对从 JVM 层面对 synchronized 较大优化，所以现在的 synchronized 锁效率也优化得很不错了，Jdk1.6 之后，为了减少获得锁和释放锁所带来的性能消耗，引入了偏向锁和轻量级锁，「增加了锁升级的过程」，由无锁-\u0026gt;偏向锁-\u0026gt;自旋锁-\u0026gt;重量级锁\n==增加锁升级的过程主要是**「减少用户态到核心态的切换，提高锁的效率，从 jvm 层面优化锁**==\n25. CAS 是什么 cas 叫做 CompareAndSwap，「比较并交换」，很多地方使用到了它，比如锁升级中自旋锁就有用到，主要是**「通过处理器的指令来保证操作的原子性」**，它主要包含三个变量：\n「1.变量内存地址」 「2.旧的预期值 A」 「3.准备设置的新值 B」 当一个线程需要修改一个共享变量的值，完成这个操作需要先取出共享变量的值，赋给 A，基于 A 进行计算，得到新值 B，在用预期原值 A 和内存中的共享变量值进行比较，「如果相同就认为其他线程没有进行修改」，而将新值写入内存\n缺点\n「CPU开销比较大」：在并发量比较高的情况下，如果许多线程反复尝试更新某一个变量，却又一直更新不成功，又因为自旋的时候会一直占用CPU，如果CAS一直更新不成功就会一直占用，造成CPU的浪费。 「ABA 问题」：比如线程 A 去修改 1 这个值，修改成功了，但是中间 线程 B 也修改了这个值，但是修改后的结果还是 1，所以不影响 A 的操作，这就会有问题。可以用**「版本号」**来解决这个问题。 「只能保证一个共享变量的原子性」 26. ReentrantLock 可重入锁 ==底层就是使用AQS==\nReentrantLock有两种模式，一种是公平锁，一种是非公平锁。\n公平模式下等待线程入队列后会严格==按照队列顺序==去执行 非公平模式下等待线程入队列后有可能会出现插队情况 1. 公平锁 第一步: ==获取状态的state 的值==\n如果 state = ==0 代表锁没有被其他线程占用==, 执行第二步 如果 state != 0 则代表锁正在被其他线程占用, 执行第三步 第二步: ==判断队列中是否有线程在排队==\n如果不存在, 则将锁的所有者设置成当前线程, 且更新 state\n如果存在就入队\n第三步: ==判断锁的所有者是不是当前线程==\n如果是则 更新状态 state的 值 如果不是, 线程进入队列排队等待 2. 非公平锁 获取状态的 state 的值\n如果 state=0 即代表锁没有被其它线程占用，==则设置当前锁的持有者为当前线程，该操作用 CAS 完成==。 如果不为0或者设置失败，代表锁被占用进行下一步。 此时**「获取 state 的值」**\n如果是，则给state+1，获取锁 如果不是，则进入队列等待 如果是0，代表刚好线程释放了锁，此时将锁的持有者设为自己 如果不是0，则查看线程持有者是不是自己 27. 多线程的创建方式 继承Thread 类, 重写 run() 方法\npublic class Demo extends Thread{\r//重写父类Thread的run()\rpublic void run() {\r}\rpublic static void main(String[] args) {\rDemo d1 = new Demo();\rDemo d2 = new Demo();\rd1.start();\rd2.start();\r}\r}\r实现Runnable 接口, 重写run()\npublic class Demo2 implements Runnable{\r//重写Runnable接口的run()\rpublic void run() {\r}\rpublic static void main(String[] args) {\rThread t1 = new Thread(new Demo2());\rThread t2 = new Thread(new Demo2());\rt1.start();\rt2.start();\r}\r}\r实现Callable 接口\npublic class Demo implements Callable\u0026lt;String\u0026gt;{\rpublic String call() throws Exception {\rSystem.out.println(\u0026quot;正在执行新建线程任务\u0026quot;);\rThread.sleep(2000);\rreturn \u0026quot;结果\u0026quot;;\r}\rpublic static void main(String[] args) throws InterruptedException, ExecutionException {\rDemo d = new Demo();\rFutureTask\u0026lt;String\u0026gt; task = new FutureTask\u0026lt;\u0026gt;(d);\rThread t = new Thread(task);\rt.start();\r//获取任务执行后返回的结果\rString result = task.get();\r}\r}\r使用线程池创建\npublic class Demo {\rpublic static void main(String[] args) {\rExecutor threadPool = Executors.newFixedThreadPool(5);\rfor(int i = 0 ;i \u0026lt; 10 ; i++) {\rthreadPool.execute(new Runnable() {\rpublic void run() {\r//todo\r}\r});\r}\r}\r}\r28. 线程池有哪些参数 corePoolSize: 核心线程函数, 线程池中始终存活的线程数 maximumPoolSize: 最大线程数, 线程池中允许的 最大线程数 keepAliveTime: 存活时间, 线程没有执行任务时 最多保持多久时间会终止 unit: danwei 参数keepAliveTime的时间单位，7种可选 workQueue: 一个阻塞队列, 用来存储等待执行的任务, 均为线程安全, 7 种可选 threadFactory: 线程工厂, 主要是用来创建线程, 正常优先级、非守护线程 handler: 拒绝策略, 拒绝处理任务时的策略, 4 种可选 29. 线程池的执行流程 判断线程池中的线程数**「是否大于设置的核心线程数」**\n如果**「没有满」，则「放入队列」**，等待线程空闲时执行任务 如果队列已经**「满了」，则判断「是否达到了线程池设置的最大线程数」** 如果**「没有达到」，就「创建新线程」**来执行任务 如果已经**「达到了」最大线程数，则「执行指定的拒绝策略」** 如果**「小于」，就「创建」**一个核心线程来执行任务 如果**「大于」，就会「判断缓冲队列是否满了」** 30. 线程池的拒绝策略 「AbortPolicy」：直接丢弃任务，抛出异常，这是默认策略 「CallerRunsPolicy」：只用调用者所在的线程来处理任务 「DiscardOldestPolicy」：丢弃等待队列中最旧的任务，并执行当前任务 「DiscardPolicy」：直接丢弃任务，也不抛出异常 31. 四种引用类型 强引用 垃圾回收器不会回收被引用的对象，哪怕内存不足时，JVM 也会直接抛出 OutOfMemoryError，除非赋值为 null。 软引用 软引用是用来描述一些==非必需但仍有用的对象==。在内存足够的时候，软引用对象不会被回收，只有在内存==不足时==，系统则会==回收==软引用对象，如果回收了软引用对象之后仍然没有足够的内存，才会抛出内存溢出异常。 弱引用 弱引用的引用强度比软引用要更弱一些，无论内存是否足够，只要 JVM 开始进行垃圾回收，那些被弱引用关联的对象都会被回收 虚引用 最弱的一种引用关系 32. 深拷贝, 浅拷贝 浅拷贝并不是真的拷贝，只是==「复制指向某个对象的指针」==，而不复制对象本身，新旧对象还是共享同一块内存。 深拷贝会另外==「创造一个一模一样的对象」==，新对象跟原对象不共享内存，修改新对象不会改到原对象。 33. ThreadLocal ThreadLocal其实就是==「线程本地变量」==，他会在每个线程都创建一个副本，那么在线程之间访问内部副本变量就行了，做到了线程之间互相隔离。 ThreadLocal 有一个**「静态内部类 ThreadLocalMap」，ThreadLocalMap 又包含了一个 Entry 数组，「Entry 本身是一个弱引用」，他的 key 是指向 ThreadLocal 的弱引用，「弱引用的目的是为了防止内存泄露」**,如果是强引用那么除非线程结束,否则无法终止,可能会有内存泄漏的风险\n但是这样还是会存在内存泄露的问题，假如 key 和 ThreadLocal 对象被回收之后，entry 中就存在 key 为 null ，但是 value 有值的 entry 对象，但是永远没办法被访问到，同样除非线程结束运行。「解决方法就是调用 remove 方法删除 entry 对象」。\n34. 一个对象的内存布局 「1.对象头」: 对象头又分为 「MarkWord」 和 「Class Pointer」 两部分。\n「MarkWord」:包含一系列的标记位，比如==轻量级锁的标记位，偏向锁标记位,gc记录信息==等等。 「ClassPointer」:用来==指向对象对应的 Class 对象（其对应的元数据对象）的内存地址==。在 32 位系统占 4 字节，在 64 位系统中占 8 字节。 「2.Length」:只在数组对象中存在，用来记录数组的长度，占用 4 字节\n「3.Instance data」: ==对象实际数据==，对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定。(这里不包括==静态成员变量，因为其是在方法区==维护的)\n「4.Padding」:Java 对象占用空间是 8 字节对齐的，即所有 Java 对象占用 bytes 数必须是 8 的倍数,是因为当我们从磁盘中取一个数据时，不会说我想取一个字节就是一个字节，都是按照一块儿一块儿来取的，这一块大小是 8 个字节，所以为了完整，==padding 的作用就是补充字节，「保证对象是 8 字节的整数倍」==。\n35. 方法参数传递 如果形参是基本数据类型，那么实参（实际数据）向形参传递的时候，就是直接传递值，把实际值复制给形参 如果形参是对象，那么实参（实际对象），向形参传递参数时，也是把值传递给形参，这个值是==实参在栈的值，也就是引用对象在堆内存中的地址== ","date":"2020-12-21T19:21:43+08:00","permalink":"https://mikeLing-qx.github.io/p/java%E5%9F%BA%E7%A1%80/","title":"Java基础"},{"content":"MySQL数据库设计规范 目录 1. 规范背景与目的\t2. 设计规范 2.1 数据库设计\t2.1.1 库名\t2.1.2 表结构\t2.1.3 列数据类型优化\t2.1.4 索引设计\t2.1.5 分库分表、分区表\t2.1.6 字符集\t2.1.7 程序DAO层设计建议\t2.1.8 一个规范的建表语句示例\t2.2 SQL编写\t2.2.1 DML语句\t2.2.2 多表连接\t2.2.3 事务\t2.2.4 排序和分组\t2.2.5 线上禁止使用的SQL语句 1. 规范背景与目的 MySQL数据库与 Oracle、 SQL Server 等数据库相比，有其内核上的优势与劣势。我们在使用MySQL数据库的时候需要遵循一定规范，扬长避短。本规范旨在帮助或指导RD、QA、OP等技术人员做出适合线上业务的数据库设计。在数据库变更和处理流程、数据库表设计、SQL编写等方面予以规范，从而为公司业务系统稳定、健康地运行提供保障。\n2. 设计规范 2.1 数据库设计 以下所有规范会按照【高危】、【强制】、【建议】三个级别进行标注，遵守优先级从高到低。\n对于不满足【高危】和【强制】两个级别的设计，DBA会强制打回要求修改。\n2.1.1 库名 【强制】库的名称必须控制在32个字符以内，相关模块的表名与表名之间尽量提现join的关系，如user表和user_login表。 【强制】库的名称格式：业务系统名称_子系统名，同一模块使用的表名尽量使用统一前缀。 【强制】一般分库名称命名格式是库通配名_编号，编号从0开始递增，比如wenda_001以时间进行分库的名称格式是“库通配名_时间” 【强制】创建数据库时必须显式指定字符集，并且字符集只能是utf8或者utf8mb4。创建数据库SQL举例：create database db1 default character set utf8;。 2.1.2 表结构 【强制】表和列的名称必须控制在32个字符以内，表名只能使用字母、数字和下划线，一律小写。 【强制】表名要求模块名强相关，如师资系统采用”sz”作为前缀，渠道系统采用”qd”作为前缀等。 【强制】创建表时必须显式指定字符集为utf8或utf8mb4。 【强制】创建表时必须显式指定表存储引擎类型，如无特殊需求，一律为InnoDB。当需要使用除InnoDB/MyISAM/Memory以外的存储引擎时，必须通过DBA审核才能在生产环境中使用。因为Innodb表支持事务、行锁、宕机恢复、MVCC等关系型数据库重要特性，为业界使用最多的MySQL存储引擎。而这是其他大多数存储引擎不具备的，因此首推InnoDB。 【强制】建表必须有comment 【建议】建表时关于主键：(1)强制要求主键为id，类型为int或bigint，且为auto_increment(2)标识表里每一行主体的字段不要设为主键，建议设为其他字段如user_id，order_id等，并建立unique key索引（可参考cdb.teacher表设计）。因为如果设为主键且主键值为随机插入，则会导致innodb内部page分裂和大量随机I/O，性能下降。 【建议】核心表（如用户表，金钱相关的表）必须有行数据的创建时间字段create_time和最后更新时间字段update_time，便于查问题。 【建议】表中所有字段必须都是NOT NULL属性，业务可以根据需要定义DEFAULT值。因为使用NULL值会存在每一行都会占用额外存储空间、数据迁移容易出错、聚合函数计算结果偏差等问题。 【建议】建议对表里的blob、text等大字段，垂直拆分到其他表里，仅在需要读这些对象的时候才去select。 【建议】反范式设计：把经常需要join查询的字段，在其他表里冗余一份。如user_name属性在user_account，user_login_log等表里冗余一份，减少join查询。 【强制】中间表用于保留中间结果集，名称必须以tmp_开头。备份表用于备份或抓取源表快照，名称必须以bak_开头。中间表和备份表定期清理。 【强制】对于超过100W行的大表进行alter table，必须经过DBA审核，并在业务低峰期执行。因为alter table会产生表锁，期间阻塞对于该表的所有写入，对于业务可能会产生极大影响。 2.1.3 列数据类型优化 【建议】表中的自增列（auto_increment属性），推荐使用bigint类型。因为无符号int存储范围为-2147483648~2147483647（大约21亿左右），溢出后会导致报错。 【建议】业务中选择性很少的状态status、类型type等字段推荐使用tinytint或者smallint类型节省存储空间。 【建议】业务中IP地址字段推荐使用int类型，不推荐用char(15)。因为int只占4字节，可以用如下函数相互转换，而char(15)占用至少15字节。一旦表数据行数到了1亿，那么要多用1.1G存储空间。 SQL：select inet_aton('192.168.2.12'); select inet_ntoa(3232236044); PHP: ip2long(‘192.168.2.12’); long2ip(3530427185); 【建议】不推荐使用enum，set。 因为它们浪费空间，且枚举值写死了，变更不方便。推荐使用tinyint或smallint。 【建议】不推荐使用blob，text等类型。它们都比较浪费硬盘和内存空间。在加载表数据时，会读取大字段到内存里从而浪费内存空间，影响系统性能。建议和PM、RD沟通，是否真的需要这么大字段。Innodb中当一行记录超过8098字节时，会将该记录中选取最长的一个字段将其768字节放在原始page里，该字段余下内容放在overflow-page里。不幸的是在compact行格式下，原始page和overflow-page都会加载。 【建议】存储金钱的字段，建议用int，程序端乘以100和除以100进行存取。因为int占用4字节，而double占用8字节，空间浪费。 【建议】文本数据尽量用varchar存储。因为varchar是变长存储，比char更省空间。MySQL server层规定一行所有文本最多存65535字节，因此在utf8字符集下最多存21844个字符，超过会自动转换为mediumtext字段。而text在utf8字符集下最多存21844个字符，mediumtext最多存2^24/3个字符，longtext最多存2^32个字符。一般建议用varchar类型，字符数不要超过2700。 【建议】时间类型尽量选取timestamp。因为datetime占用8字节，timestamp仅占用4字节，但是范围为1970-01-01 00:00:01到2038-01-01 00:00:00。更为高阶的方法，选用int来存储时间，使用SQL函数unix_timestamp()和from_unixtime()来进行转换。 详细存储大小参加下图：\n2.1.4 索引设计 【强制】InnoDB表必须主键为id int/bigint auto_increment,且主键值禁止被更新。 【建议】主键的名称以“pk_”开头，唯一键以“uk_”或“uq_”开头，普通索引以“idx_”开头，一律使用小写格式，以表名/字段的名称或缩写作为后缀。 【强制】InnoDB和MyISAM存储引擎表，索引类型必须为BTREE；MEMORY表可以根据需要选择HASH或者BTREE类型索引。 【强制】单个索引中每个索引记录的长度不能超过64KB。 【建议】单个表上的索引个数不能超过7个。 【建议】在建立索引时，多考虑建立联合索引，并把区分度最高的字段放在最前面。如列userid的区分度可由select count(distinct userid)计算出来。 【建议】在多表join的SQL里，保证被驱动表的连接列上有索引，这样join执行效率最高。 【建议】建表或加索引时，保证表里互相不存在冗余索引。对于MySQL来说，如果表里已经存在key(a,b)，则key(a)为冗余索引，需要删除。 2.1.5 分库分表、分区表 【强制】分区表的分区字段（partition-key）必须有索引，或者是组合索引的首列。 【强制】单个分区表中的分区（包括子分区）个数不能超过1024。 【强制】上线前RD或者DBA必须指定分区表的创建、清理策略。 【强制】访问分区表的SQL必须包含分区键。 【建议】单个分区文件不超过2G，总大小不超过50G。建议总分区数不超过20个。 【强制】对于分区表执行alter table操作，必须在业务低峰期执行。 【强制】采用分库策略的，库的数量不能超过1024 【强制】采用分表策略的，表的数量不能超过4096 【建议】单个分表不超过500W行，ibd文件大小不超过2G，这样才能让数据分布式变得性能更佳。 【建议】水平分表尽量用取模方式，日志、报表类数据建议采用日期进行分表。 2.1.6 字符集 【强制】数据库本身库、表、列所有字符集必须保持一致，为utf8或utf8mb4。 【强制】前端程序字符集或者环境变量中的字符集，与数据库、表的字符集必须一致，统一为utf8。 2.1.7 程序层DAO设计建议 【建议】新的代码不要用model，推荐使用手动拼SQL+绑定变量传入参数的方式。因为model虽然可以使用面向对象的方式操作db，但是其使用不当很容易造成生成的SQL非常复杂，且model层自己做的强制类型转换性能较差，最终导致数据库性能下降。 【建议】前端程序连接MySQL或者redis，必须要有连接超时和失败重连机制，且失败重试必须有间隔时间。 【建议】前端程序报错里尽量能够提示MySQL或redis原生态的报错信息，便于排查错误。 【建议】对于有连接池的前端程序，必须根据业务需要配置初始、最小、最大连接数，超时时间以及连接回收机制，否则会耗尽数据库连接资源，造成线上事故。 【建议】对于log或history类型的表，随时间增长容易越来越大，因此上线前RD或者DBA必须建立表数据清理或归档方案。 【建议】在应用程序设计阶段，RD必须考虑并规避数据库中主从延迟对于业务的影响。尽量避免从库短时延迟（20秒以内）对业务造成影响，建议强制一致性的读开启事务走主库，或更新后过一段时间再去读从库。 【建议】多个并发业务逻辑访问同一块数据（innodb表）时，会在数据库端产生行锁甚至表锁导致并发下降，因此建议更新类SQL尽量基于主键去更新。 【建议】业务逻辑之间加锁顺序尽量保持一致，否则会导致死锁。 【建议】对于单表读写比大于10:1的数据行或单个列，可以将热点数据放在缓存里（如mecache或redis），加快访问速度，降低MySQL压力。 2.1.8\t一个规范的建表语句示例 一个较为规范的建表语句为：\nCREATE TABLE user ( `id` bigint(11) NOT NULL AUTO_INCREMENT, `user_id` bigint(11) NOT NULL COMMENT ‘用户id’ `username` varchar(45) NOT NULL COMMENT '真实姓名', `email` varchar(30) NOT NULL COMMENT ‘用户邮箱’, `nickname` varchar(45) NOT NULL COMMENT '昵称', `avatar` int(11) NOT NULL COMMENT '头像', `birthday` date NOT NULL COMMENT '生日', `sex` tinyint(4) DEFAULT '0' COMMENT '性别', `short_introduce` varchar(150) DEFAULT NULL COMMENT '一句话介绍自己，最多50个汉字', `user_resume` varchar(300) NOT NULL COMMENT '用户提交的简历存放地址', `user_register_ip` int NOT NULL COMMENT ‘用户注册时的源ip’, `create_time` timestamp NOT NULL COMMENT ‘用户记录创建的时间’, `update_time` timestamp NOT NULL COMMENT ‘用户资料修改的时间’, `user_review_status` tinyint NOT NULL COMMENT ‘用户资料审核状态，1为通过，2为审核中，3为未通过，4为还未提交审核’, PRIMARY KEY (`id`), UNIQUE KEY `idx_user_id` (`user_id`), KEY `idx_username`(`username`), KEY `idx_create_time`(`create_time`,`user_review_status`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='网站用户基本信息'; 2.2 SQL编写 2.2.1 DML语句 【强制】SELECT语句必须指定具体字段名称，禁止写成*。因为select *会将不该读的数据也从MySQL里读出来，造成网卡压力。且表字段一旦更新，但model层没有来得及更新的话，系统会报错。 【强制】insert语句指定具体字段名称，不要写成insert into t1 values(…)，道理同上。 【建议】insert into…values(XX),(XX),(XX)…。这里XX的值不要超过5000个。值过多虽然上线很很快，但会引起主从同步延迟。 【建议】SELECT语句不要使用UNION，推荐使用UNION ALL，并且UNION子句个数限制在5个以内。因为union all不需要去重，节省数据库资源，提高性能。 【建议】in值列表限制在500以内。例如select… where userid in(….500个以内…)，这么做是为了减少底层扫描，减轻数据库压力从而加速查询。 【建议】事务里批量更新数据需要控制数量，进行必要的sleep，做到少量多次。 【强制】事务涉及的表必须全部是innodb表。否则一旦失败不会全部回滚，且易造成主从库同步终端。 【强制】写入和事务发往主库，只读SQL发往从库。 【强制】除静态表或小表（100行以内），DML语句必须有where条件，且使用索引查找。 【强制】生产环境禁止使用hint，如sql_no_cache，force index，ignore key，straight join等。因为hint是用来强制SQL按照某个执行计划来执行，但随着数据量变化我们无法保证自己当初的预判是正确的，因此我们要相信MySQL优化器！ 【强制】where条件里等号左右字段类型必须一致，否则无法利用索引。 【建议】SELECT|UPDATE|DELETE|REPLACE要有WHERE子句，且WHERE子句的条件必需使用索引查找。 【强制】生产数据库中强烈不推荐大表上发生全表扫描，但对于100行以下的静态表可以全表扫描。查询数据量不要超过表行数的25%，否则不会利用索引。 【强制】WHERE 子句中禁止只使用全模糊的LIKE条件进行查找，必须有其他等值或范围查询条件，否则无法利用索引。 【建议】索引列不要使用函数或表达式，否则无法利用索引。如where length(name)='Admin'或where user_id+2=10023。 【建议】减少使用or语句，可将or语句优化为union，然后在各个where条件上建立索引。如where a=1 or b=2优化为where a=1… union …where b=2, key(a),key(b)。 【建议】分页查询，当limit起点较高时，可先用过滤条件进行过滤。如select a,b,c from t1 limit 10000,20;优化为: select a,b,c from t1 where id\u0026gt;10000 limit 20;。 2.2.2 多表连接 【强制】禁止跨db的join语句。因为这样可以减少模块间耦合，为数据库拆分奠定坚实基础。 【强制】禁止在业务的更新类SQL语句中使用join，比如update t1 join t2…。 【建议】不建议使用子查询，建议将子查询SQL拆开结合程序多次查询，或使用join来代替子查询。 【建议】线上环境，多表join不要超过3个表。 【建议】多表连接查询推荐使用别名，且SELECT列表中要用别名引用字段，数据库.表格式，如select a from db1.table1 alias1 where …。 【建议】在多表join中，尽量选取结果集较小的表作为驱动表，来join其他表。 2.2.3 事务 【建议】事务中INSERT|UPDATE|DELETE|REPLACE语句操作的行数控制在2000以内，以及WHERE子句中IN列表的传参个数控制在500以内。 【建议】批量操作数据时，需要控制事务处理间隔时间，进行必要的sleep，一般建议值5-10秒。 【建议】对于有auto_increment属性字段的表的插入操作，并发需要控制在200以内。 【强制】程序设计必须考虑“数据库事务隔离级别”带来的影响，包括脏读、不可重复读和幻读。线上建议事务隔离级别为repeatable-read。 【建议】事务里包含SQL不超过5个（支付业务除外）。因为过长的事务会导致锁数据较久，MySQL内部缓存、连接消耗过多等雪崩问题。 【建议】事务里更新语句尽量基于主键或unique key，如update … where id=XX; 否则会产生间隙锁，内部扩大锁定范围，导致系统性能下降，产生死锁。 【建议】尽量把一些典型外部调用移出事务，如调用webservice，访问文件存储等，从而避免事务过长。 【建议】对于MySQL主从延迟严格敏感的select语句，请开启事务强制访问主库。 2.2.4 排序和分组 【建议】减少使用order by，和业务沟通能不排序就不排序，或将排序放到程序端去做。order by、group by、distinct这些语句较为耗费CPU，数据库的CPU资源是极其宝贵的。 【建议】order by、group by、distinct这些SQL尽量利用索引直接检索出排序好的数据。如where a=1 order by可以利用key(a,b)。 【建议】包含了order by、group by、distinct这些查询的语句，where条件过滤出来的结果集请保持在1000行以内，否则SQL会很慢。 2.2.5 线上禁止使用的SQL语句 【高危】禁用update|delete t1 … where a=XX limit XX; 这种带limit的更新语句。因为会导致主从不一致，导致数据错乱。建议加上order by PK。 【高危】禁止使用关联子查询，如update t1 set … where name in(select name from user where…);效率极其低下。 【强制】禁用procedure、function、trigger、views、event、外键约束。因为他们消耗数据库资源，降低数据库实例可扩展性。推荐都在程序端实现。 【强制】禁用insert into …on duplicate key update…在高并发环境下，会造成主从不一致。 【强制】禁止联表更新语句，如update t1,t2 where t1.id=t2.id…。 ","date":"2020-10-03T15:00:10+08:00","permalink":"https://mikeLing-qx.github.io/p/mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1%E8%A7%84%E8%8C%83/","title":"MySql数据库设计规范"},{"content":"方式一: linux 搭建 redis集群 效果: 三个主节点;每个主节点有1个从节点\nredis 和哨兵 集群的搭建 (用原生的cluster; ==实现数据的分片==)\n可选的三个插件: 官方的cluster 和 豌豆荚 ; trainProxy 一个redis ==gem 包== ; 一个redis==源码包== 上传到服务器; 版本号要匹配\n然后服务器上需要准备好 c++ ; ruby; rubyBem 环境\n安装好 redis 的 gem;\n进到redis 源码包里面 用make命令进行安装Redis\n编辑配置文件 ; 开启集群 ; 配置端口; 允许后台启动\n创建一个 redis- cluster 文件夹; 里面 存放6个文件夹 ; redis1 到 6\n在 安装好的单机用make install 指定要安装Redis在 1到6 的文件夹下面\n拷贝redis.conf 配置文件到 各个文件夹下\n再编辑每个配置文件的端口\n启动所有的redis服务\n在redis 源码包src目录下 用 redis.trip.rb 脚本 去 创建集群; 设置一个节主点有两个从节点 连接的 ./redis-cli -c -p 7002 sentinel 哨兵集群的搭建( 为每一个主节点配置一个哨兵)\nsentinel .conf 包里面\n设置其要监控的主节点的ip 地址 和端口号 ; 有多少个从节点\nredis 集群搭建 详细步骤 (1) 安装gcc\nRedis 是 c 语言开发的。安装 redis 需要 c 语言的编译环境\nyum install gcc-c++ (2) 使用yum命令安装 ruby （我们需要使用ruby脚本来实现集群搭建）\nyum install ruby yum install rubygems (3) 将redis 源码包上传到linux, 解压; 进入源码文件夹; 执行命令\n// 解压 tar tar -xzvf redis.tar.gz make 安装 (4) 创建目录/usr/local/redis-cluster目录， 安装6个redis实例 ; 用以下命令根据不同的文件夹 , 执行6 次, 注意\nmake install PREFIX=/usr/local/redis-cluster/redis-1 (5 修改配置文件\ndaemonize yes # 后台启动 cluster-enabled yes # 开启集群功能 (5) 复制配置文件到 各个实例中\n(6) 进入各个实例的配置文件中修改端口\nport 7001 # 端口 (7) 在redis-cluster 文件夹 创建脚本文件start-all.sh; 以便同时启动redis服务\ncd redis-1/bin/ ./redis-server redis.conf cd ../.. cd redis-2/bin/ ./redis-server redis.conf cd ../.. cd redis-3/bin/ ./redis-server redis.conf cd ../.. cd redis-4/bin/ ./redis-server redis.conf cd ../.. cd redis-5/bin/ ./redis-server redis.conf cd ../.. cd redis-6/bin/ ./redis-server redis.conf cd ../.. (8) 执行脚本文件 ; 查看进程\nps -ef | grep redis (9) 加入集群环境; 复制redis源码包 下的src 目录下的ruby脚本 redis-trib.rb 到redis-cluster 包下\n![复制ruby脚本到 redis-cluster文件夹下](images/复制ruby脚本到 redis-cluster文件夹下-16342818506617.png)\n(10) 执行ruby 脚本 (ip 为服务器ip)\n# 先赋权 chomd u+x redis-trib.rb ./redis-trib.rb create --replicas 1 192.168.182.129:7001 192.168.182.129:7002 192.168.182.129:7003 192.168.182.129:7004 192.168.182.129:7005 192.168.182.129:7006 ==可以看到 7001, 7002, 7003 是主节点; 7004是 7001的从节点; 7005是 7002的从节点; 7006是 7003的从节点;==\n(11) 测试redis 集群 (正常运行)\n进入任意redis文件夹执行菜单 ./redis-cli -p ip地址 -p 端口 -c -c 代表连接的是集群 (12) springboot 配置redis集群\nredis: cluster: nodes: - 192.168.182.129:7001 - 192.168.182.129:7002 - 192.168.182.129:7003 - 192.168.182.129:7004 - 192.168.182.129:7005 - 192.168.182.129:7006 2. 方式二: docker 安装redis集群 2.1 通过docker compose搭建 安装依赖项 yum install -y epel-release yum install -y python-pip python-devel gcc pip install --upgrade pip -vvv 安装 docker-compose pin install docker-compose 查看docker 的 多个容器的ip\ndocker inspect containerName XXX XXX | grep IPA 2.2 直接搭建 2.2.1 环境准备; 1. 安装docker： 2. 下载reids镜像：docker pull redis 3. 下载ruby镜像：docker pull ruby 2.2.2 创建多个redis容器并启动 在/usr/local/src目录下创建redis-cluster文件夹，并创建配置文件redis-cluster.conf cd /usr/local/src \u0026amp;\u0026amp; mkdir redis-cluster \u0026amp;\u0026amp; cd ./redis-cluster \u0026amp;\u0026amp; touch redis-cluster.conf redis-cluster.conf的内容如下： port ${PORT} cluster-enabled yes cluster-config-file nodes.conf cluster-node-timeout 5000 #对外ip cluster-announce-ip 写你的ip (用ifconfig 查看宿主机ip) cluster-announce-port ${PORT} cluster-announce-bus-port 1${PORT} appendonly yes 使用脚本创建 集群 (==待优化完善==) 192.168.182.129\nredis-cli --cluster create 172.18.0.2:6000 172.18.0.3:6001 172.18.0.4:6002 172.18.0.5:6003 172.18.0.6:6004 172.18.0.7:6005 --cluster-replicas 1 redis-cli --cluster create 192.168.182.129:6000 192.168.182.129:6001 192.168.182.129:6002 192.168.182.129:6003 192.168.182.129:6004 192.168.182.129:6005 --cluster-replicas 1 redis-cli --cluster create ip:6000 ip:6001 ip:6002 ip:6003 ip:6004 ip:6005 --cluster-replicas 1 ","date":"2020-09-09T14:35:17+08:00","permalink":"https://mikeLing-qx.github.io/p/redis%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/","title":"Redis集群搭建"},{"content":"1. 常见图标含义 参考文档: https://www.cnblogs.com/lsgxeva/p/13386238.html\n2. 目录列表 自动定位到当前打开的文件\n3. debug ctrl + f8 添加或删除断点\nctrl + shift + f8 查看所有断点\nShow Excuteion Point \u0026ndash; Alt + F10 \u0026ndash; 跳转到当前所在执行代码位置\nStep Over \u0026ndash; F8 \u0026ndash; 单步执行，从当前执行代码开始执行下一条语句。\nStep Into \u0026ndash; F7 \u0026ndash; 进入，如果当前行 有方法，则进入方法内；一般用于进入自定义方法，不会进入官方类库的方法。\nForce Step Into \u0026ndash; Alt+Shift+F7 \u0026ndash; 强制进入，能进入任何方法；一般用于查看源码时，进入官方类库的方法。\nStep Out \u0026ndash; Shift F8 \u0026ndash; 跳出，立即执行完当前正在执行的方法，返回方法的调用处。\nDrop Frame \u0026ndash; 删除栈帧，从当前 执行方法 回退到 方法的调用处。\nRun to Cursor \u0026ndash; Alt+F9 \u0026ndash; 执行到光标处。\n4. 书签 f11: 在代码所在行添加书签\nctrl + shift + 数字{1, 9}: 添加助记书签\nshift + f11: 查看所有书签\ndelete: 删除书签\nAlt + 方向键上下: 上下移动书签。\nctrl+数字: 快速跳转\n5. 查找使用处 Alt + f7\n6. 抽取变量 ctrl + alt +m\n7. 远程debug Test-NetConnection -ComputerName 172.29.8.144 -Port 8889\r","date":"2020-09-01T19:21:43+08:00","permalink":"https://mikeLing-qx.github.io/p/idea/","title":"Idea"},{"content":"1. Spring事务的7种传播行为 PROPAGATION_REQUIRED ，==默认==的spring事务传播级别，==如果上下文中已经存在事务，那么就加入到事务中执行，如果当前上下文中不存在事务，则新建事务执行==。所以这个级别通常能满足处理大多数的业务场景。\nPROPAGATION_SUPPORTS ，==如果上下文存在事务，则支持事务加入事务，如果没有事务，则使用非事务的方式执行==。所以说，并非所有的包在transactionTemplate.execute中的代码都会有事务支持。这个通常是用来处理那些并非原子性的非核心业务逻辑操作。应用场景较少。\nPROPAGATION_MANDATORY（强制） ， ==该级别的事务要求上下文中必须要存在事务，否则就会抛出异常！==配置该方式的传播级别是有效的控制上下文调用代码遗漏添加事务控制的保证手段。比如一段代码不能单独被调用执行，但是一旦被调用，就必须有事务包含的情况，就可以使用这个传播级别。\nPROPAGATION_REQUIRES_NEW ，==每次都会新建一个事务，并且同时将上下文中的事务挂起，执行当前新建事务完成以后，上下文事务恢复再执行==\n这是一个很有用的传播级别，举一个应用场景：现在有一个发送100个红包的操作，在发送之前，要做一些系统的初始化、验证、数据记录操作，然后发送100封红包，然后再记录发送日志，发送日志要求100%的准确，如果日志不准确，那么整个父事务逻辑需要回滚。 怎么处理整个业务需求呢？就是通过这个PROPAGATION_REQUIRES_NEW 级别的事务传播控制就可以完成。发送红包的子事务不会直接影响到父事务的提交和回滚。 PROPAGATION_NOT_SUPPORTED ，当前级别的特点就是上下文中存在事务，==则挂起事务，执行当前逻辑，结束后恢复上下文的事务==。\nPROPAGATION_NEVER ，该事务更严格，就抛出runtime异常，强制停止执行！\nPROPAGATION_NESTED ，字面也可知道，nested，嵌套级别事务。该传播级别特征是，如果上下文中存在事务，则==嵌套事务==执行，如果不存在事务，则新建事务。\n还是子事务先提交，父事务再提交, 子事务是父事务的一部分，由父事务统一提交。 回滚特性 主事务和嵌套事务属于同一个事务 嵌套事务出错回滚不会影响到主事务 主事务回滚会将嵌套事务一起回滚了 2. 多线程事务\u0026ndash;二阶段提交 这两个方法的两种写法, 第一种是可以正常的进行事务的提交和回滚的, 第二种执行的时候线程会一直阻塞, 不会退出, 分析原因\n原因分析过程:\n第一种方式 是根据线程池大小来划分任务, 第二种方式是根据数据列表来划分任务, 第二种方式会出现一个线程需要处理多个事务的情况, 然而在updateStudentsTransaction 方法里面 ==事务的隔离级别 PROPAGATION_REQUIRES_NEW 会导致 线程新开任务==, ==这会导致 主线程等待子线程全部完成后再进行事务的提交或回滚==, 需要修改为 ==PROPAGATION_REQUIRES== 第一种\r@Transactional(propagation = Propagation.REQUIRED, rollbackFor = {Exception.class})\rpublic void updateStudentWithThreadsAndTrans() throws InterruptedException {\r//查询总数据\rList\u0026lt;UserDemo\u0026gt; allUser = userMapper.selectAll();\r// 线程数量\rfinal int threadCount = 2;\r//每个线程处理的数据量\rfinal int dataPartionLength = (allUser.size() + threadCount - 1) / threadCount;\r// 创建多线程处理任务\rExecutorService studentThreadPool = Executors.newFixedThreadPool(threadCount);\rCountDownLatch threadLatchs = new CountDownLatch(threadCount);\rAtomicBoolean isError = new AtomicBoolean(false);\rtry {\rfor (int i = 0; i \u0026lt; threadCount; i++) {\r// 每个线程处理的数据\rList\u0026lt;UserDemo\u0026gt; threadDatas = allUser.stream()\r.skip((long) i * dataPartionLength).limit(dataPartionLength).collect(Collectors.toList());\rstudentThreadPool.execute(() -\u0026gt; {\rtry {\rtry {\ruserService.updateStudentsTransaction(transactionManager, transactionStatusList, threadDatas);\r} catch (Throwable e) {\risError.set(true);\rthrow e;\r}finally {\rthreadLatchs.countDown();\r}\r} catch (Exception e) {\risError.set(true);\rthrow e;\r}\r});\r}\r// 倒计时锁设置超时时间 30s\rboolean await = threadLatchs.await(30, TimeUnit.SECONDS);\r// 判断是否超时\rif (!await) {\risError.set(true);\r}\r} catch (Throwable e) {\risError.set(true);\rthrow e;\r}\rif (!transactionStatuses.isEmpty()) {\rif (isError.get()) {\rtransactionStatuses.forEach(transactionManager::rollback);\r} else {\rtransactionStatuses.forEach(transactionManager::commit);\r}\r}\rSystem.out.println(\u0026quot;主线程完成\u0026quot;);\r}\r第二种\r@Transactional(propagation = Propagation.REQUIRED, rollbackFor = {Exception.class})\rpublic void updateStudentWithThreadsAndTrans() throws InterruptedException {\r//查询总数据\rList\u0026lt;UserDemo\u0026gt; allUser = userMapper.selectAll();\r// 线程数量\rfinal int threadCount = 5;\r//每个线程处理的数据量\rfinal int dataPartionLength = (allUser.size() + threadCount - 1) / threadCount;\rExecutorService studentThreadPool = Executors.newFixedThreadPool(threadCount);\rCountDownLatch threadLatchs = new CountDownLatch(threadCount);\rAtomicBoolean isError = new AtomicBoolean(false);\r// 创建CompletableFuture列表\rList\u0026lt;CompletableFuture\u0026lt;Void\u0026gt;\u0026gt; futures = new ArrayList\u0026lt;\u0026gt;();\rint batchSize = 100;\rint size = allUser.size();\rint batchNum = size / batchSize;\rif (size % batchSize != 0) {\rbatchNum++;\r}\rfor (int i = 0; i \u0026lt; batchNum; i++) {\rint start = i * batchSize;\rint end = Math.min((i + 1) * batchSize, size);\r// 使用CompletableFuture执行异步任务\rCompletableFuture\u0026lt;Void\u0026gt; future = CompletableFuture.runAsync(() -\u0026gt; {\rtry {\ruserService.updateStudentsTransaction(transactionManager, transactionStatusList, allUser.subList(start, end));\r} catch (Throwable e) {\rthrow new CompletionException(e);\r} finally {\rthreadLatchs.countDown();\r}\r}, studentThreadPool).exceptionally(e -\u0026gt; {\risError.set(true);\rreturn null;\r});\rfutures.add(future);\r}\r// 等待所有任务完成\rCompletableFuture.allOf(futures.toArray(new CompletableFuture[0])).join();\rif (!transactionStatusList.isEmpty()) {\rif (isError.get()) {\rtransactionStatusList.forEach(transactionManager::rollback);\r} else {\rtransactionStatusList.forEach(transactionManager::commit);\r}\r}\rSystem.out.println(\u0026quot;主线程完成\u0026quot;);\r}\r对于updateStudentsTransaction的调用里面是一样的代码\n@Transactional(propagation = Propagation.REQUIRED, rollbackFor = {Exception.class})\rpublic void updateStudentsTransaction(PlatformTransactionManager transactionManager, List\u0026lt;TransactionStatus\u0026gt; transactionStatuses, List\u0026lt;UserDemo\u0026gt; userList) {\r// 使用这种方式将事务状态都放在同一个事务里面\rDefaultTransactionDefinition def = new DefaultTransactionDefinition();\rdef.setPropagationBehavior(TransactionDefinition.PROPAGATION_REQUIRES_NEW); // 事物隔离级别，开启新事务，这样会比较安全些。\rTransactionStatus status = transactionManager.getTransaction(def); // 获得事务状态\rtransactionStatuses.add(status);\ruserList.forEach(s -\u0026gt; {\r// if (\u0026quot;mike\u0026quot;.equals(s.getUserName())) {\r// throw new RuntimeException(\u0026quot;故意抛出异常\u0026quot;);\r// }\r// 更新教师信息\r// String teacher = s.getTeacher();\rString newTeacher = \u0026quot;TNO_\u0026quot; + new Random().nextInt(100);\rs.setUserName(newTeacher);\ruserMapper.updateByPrimaryKey(s);\r});\rSystem.out.println(\u0026quot;子线程：\u0026quot; + Thread.currentThread().getName());\r}\r","date":"2020-07-09T19:21:43+08:00","permalink":"https://mikeLing-qx.github.io/p/spring%E4%BA%8B%E5%8A%A1/","title":"Spring事务"},{"content":"1. 概览 1.什么是 redis？它能做什么？ 2.redis 有哪八种数据类型？有哪些应用场景？ 3.redis为什么这么快？ 4.听说 redis 6.0之后又使用了多线程，不会有线程安全的问题吗？ 5.redis 的持久化机制有哪些？优缺点说说 \\6. Redis的过期键的删除策略有哪些？ \\7. Redis的内存满了怎么办？ 8.Redis 的热 key 问题怎么解决？ 9.缓存击穿、缓存穿透、缓存雪崩是什么？怎么解决呢？ 10.Redis 有哪些部署方式？ 11.哨兵有哪些作用？ 12.哨兵选举过程是怎么样的？ 13.cluster集群模式是怎么存放数据的？ 14.cluster的故障恢复是怎么做的？ 15.主从同步原理是怎样的？ 16.无硬盘复制是什么？ 2. Redis 的数据类型 基本\nstring: 存储计数器，粉丝数; ==分布式锁也会用到该类型==\nhashmap: key - value 形式的，value 是一个map\nlist: 基本的数据类型，列表。在 Redis 中可以把 list 用作栈、队列、阻塞队列\nset: 集合，不能有重复元素，可以做点赞，收藏等\nzsat: 有序集合，不能有重复元素，有序集合中的每个元 素都需要指定一个分数，根据分数对元素进行升序排序。可以做排行榜\n特殊数据类型\n1.geospatial: Redis 在 3.2 推出 ==Geo 类型==，该功能==可以推算出地理位置信息，两地之间的距离==。 2.hyperloglog: 基数：数学上集合的元素个数，是不能重复的。这个数据结构常用于统计网站的 UV。 3.bitmap: bitmap 就是通过最小的单位 bit 来进行0或者1的设置，表示某个元素对应的值或者状态。一个 bit 的值，或者是0，或者是1；也就是说一个 bit 能存储的最多信息是2。bitmap 常用于统计用户信息比如活跃粉丝和不活跃粉丝、登录和未登录、是否打卡等 3. redis 为什么快 1. 完全基于内存操作\r2. 使用单线程模型来处理客户端的请求, 避免上下文切换\r3. IO 多路复用机制\r4. 自身使用C语言编写, 有很多优化机制\r4. redis 6.0 后用多线程, 会线程不安全吗? 不会\n其实 redis 还是使用单线程模型来处理客户端的请求，只是使用多线程来处理数据的读写和协议解析，执行命令还是使用单线程，所以是不会有线程安全的问题。\n==就相当与还是只有一个医生就诊, 但是会有多个护士将病人带给医生==\n之所以加入了多线程因为 redis 的性能瓶颈在于网络IO而非CPU，使用多线程能提升IO读写的效率，从而整体提高redis的性能。\n5. redis 的持久化机制有哪些? 优缺点 一. AOF redis ==每次执行一个命令时==,都会把这个「命令原本的语句记录到一个.aod的文件当中,然后通过fsync策略,将命令执行后的数据==持久化到磁盘==中」(不包括读命令)， 优点\n1.AOF可以「更好的保护数据不丢失」，==一般AOF会以每隔1秒==，通过后台的一个线程去执行一次fsync操作，如果redis进程挂掉，==最多丢失1秒的数据== 2.AOF是将命令直接==追加在文件末尾==的,==「写入性能非常高」== 3.AOF日志文件的命令通过非常==可读的方式==进行记录，这个非常「==适合做灾难性的误删除紧急恢复」==，如果某人不小心用 flushall 命令清空了所有数据，只要这个时候还没有执行 rewrite，那么就可以将日志文件中的 flushall 删除，进行恢复 二. RDB (快照) ==优点==\n将某一时间点redis 内的所有数据保存, 大型数据的恢复, RDB 的恢复速度会很快 对读写影响较小 ==缺点==\n==「有可能会产生长时间的数据丢失」== 可能会有长时间停顿:我们前面讲了,fork 子进程这个过程是和 redis 的数据量有很大关系的,==如果「数据量很大,那么很有可能会使redis暂停几秒」== 6. Redis 的过期键的删除策略 定时过期：每个设置过期时间的key都需要创建一个定时器，到过期时间就会立即清除 惰性过期：只有当访问一个key时，才会判断该key是否已过期，过期则清除。 定期过期：每隔一定的时间，会扫描一定数量的数据库的expires字典中一定数量的key，并清除其中已过期的key。 8. 热key问题 某一时刻, 有非常多的请求访问某个key, 流量过大, 导致该redis 服务器宕机\n方案\n可以将结果缓存到本地内存 将热key 分散到不同的服务器 设置永不过期 9. 缓存击穿; 缓存穿透; 缓存雪崩 缓存穿透: 没有的数据, 不存在的数据, 每次 都请求到达数据库\n存一个空对象, 设置一个比较短的过期时间 布隆过滤器 访问缓存层和存储层之前，将存在的key用布隆过滤器提前保存起来 缓存击穿: 一个key 非常热门, 失效的瞬间, 持续的请求就到了数据库\n永不过期\n缓存雪崩: 不同的数据大批量到过期时间\n设置不同的过期时间\n10. Redis 部署方式 单机模式: 哨兵模式: cluster集群模式: 3.0版本; 自动将数据进行分片, 每个master 放一部分数据 主从复制: 主数据库负责 ==读写操作==, 从数据库 负责==读操作== 13. Cluster 集群存放数据方式 一个cluster集群中总共有16384个节点，集群会将这16384个节点平均分配给每个==主节点==\n14. cluster 的故障恢复过程 每个节点会向其他节点发送ping命令, 通过有没有收到回复来判断其他节点是否已经下线;\n如果长时间没有收到回复, 那么发起ping命令的节点就会认为目标节点疑似下线,\n1.当A节点发现目标节点疑似下线，就会向==集群中的其他节点散播消息==，其他节点就会向目标节点发送命令，判断目标节点是否下线 2.如果集群中==半数以上的节点都认为目标节点下线==，就会对目标节点标记为下线，从而告诉其他节点，让目标节点在整个集群中都下线 15. 主从同步原理是怎样的? 1.当一个从数据库启动时，它会向==主数据库发送一个SYNC命令==，master收到后，在后台保存==快照==，也就是我们说的==RDB持久化==，当然保存快照是需要消耗时间的，并且redis是单线程的，在==保存快照期间redis受到的命令会缓存起来== 2.快照完成后会==将缓存的命令以及快照一起打包发给slave节点==，从而保证主从数据库的一致性。 3.从数据库接受到快照以及缓存的命令后会将这部分数据==写入到硬盘上的临时文件当中==，写入完成后会用这份文件去替换掉RDB快照文件，当然，这个操作是不会阻塞的，可以继续接收命令执行，具体原因其实就是fork了一个子进程，用子进程去完成了这些功能。 16. 无硬盘辅助 我们刚刚说了主从之间是通过RDB快照来交互的，虽然看来逻辑很简单，但是还是会存在一些问题，但是会存在着一些问题。\n1.master禁用了RDB快照时，发生了主从同步(复制初始化)操作，也会生成RDB快照，但是之后如果master发成了重启，就会用RDB快照去恢复数据，这份数据可能已经很久了，中间就会丢失数据 2.在这种一主多从的结构中，master每次和slave同步数据都要进行一次快照，从而在硬盘中生成RDB文件，会影响性能 为了解决这种问题，redis在后续的更新中也加入了无硬盘复制功能，也就是说直接通过网络发送给slave，避免了和硬盘交互，但是也是有io消耗\n17. Redis 的槽 槽的分配与迁移 槽分配： Redis 集群启动时，所有的槽需要分配给集群中的节点。 例如： 节点 A 管理槽 0-5460。 节点 B 管理槽 5461-10922。 节点 C 管理槽 10923-16383。 槽的迁移： 当集群中新增或移除节点时，Redis 会重新分配槽。 数据迁移仅发生在重新分配的槽对应的数据上，减少迁移量。 槽是 Redis 集群用来分布和管理数据的==逻辑分区==，数量固定为 16384 个。 每个键通过哈希算法映射到一个槽上，但槽本身不是存储数据的实体，而是一个==分区标识==。 每个槽可以存储任意数量的键值对：\n槽相当于一个“桶”，每个槽可以包含非常多的键。 数据的实际存储仍然由分配该槽的 Redis 节点负责。 例如： 槽 0 可能存储 1 万个键； 槽 1 可能存储 100 万个键。 键到槽的映射：\nRedis 使用哈希算法（CRC16(key) % 16384）计算每个键对应的槽。 同一个槽中的键分配到相同的 Redis 节点管理。 18. redission 分布式数据结构 提供对 Redis 中数据结构的封装，如：\n基本类型（Map、Set、List、Queue、Deque 等） 高级数据结构（BloomFilter、Geo 等） 锁相关数据结构（BitSet、CountDownLatch 等） 分布式锁和同步器 Redisson 提供强大的分布式锁功能，包括：\n公平锁 读写锁 可重入锁 联锁（MultiLock） 红锁（RedLock） 在分布式环境中，确保在多个 Redis 实例上获取锁的一致性，防止单点故障和锁的竞争问题。 分布式服务\n对象存储服务：可以直接将对象映射到 Redis。 远程服务调用（RRemoteService）：通过 Redis 实现微服务间的远程调用。 ","date":"2020-07-07T19:21:43+08:00","permalink":"https://mikeLing-qx.github.io/p/redis/","title":"Redis"},{"content":"JUC（java.util.current）多线程 (一) 1. 多线程基础 1.1 目标 ​\t理解多线程技术的一些基本概念。\n1.2 路径 多线程的概念及作用 线程和进程的介绍 多线程的创建 用户线程和守护线程 1.3 讲解 1.3.1 多线程的概念及作用 ​\t一个采用了多线程技术的应用程序可以更好地利用系统资源。其主要优势在于充分利用了CPU的空闲时间片，可以用尽可能少的时间来对用户的要求做出响应，使得进程的整体运行效率得到较大提高，同时增强了应用程序的灵活性。\n​\t更为重要的是，由于同一进程的所有线程是共享同一内存，所以不需要特殊的数据传送机制，不需要建立共享存储区或共享文件，从而使得不同任务之间的协调操作与运行、数据的交互、资源的分配等问题更加易于解决。\n​\t简单说，多线程就是以空间（cpu的占用）换时间(响应时间)，期望以更多的资源消耗（cpu）来减少响应时间，提高执行速度的技术。\n1.3.2 线程和进程 进程： ​\t是指一个内存中运行的应用程序，每个进程都有一个独立的内存空间，一个应用程序可以同时运行多个进程；进程也是程序的一次执行过程，是系统运行程序的基本单位；系统运行一个程序即是一个进程从创建、运行到消亡的过程。\n线程： ​\t进程内部的一个独立执行单元；一个进程可以同时并发的运行多个线程，可以理解为一个进程便相当于一个单 CPU 操作系统，而线程便是这个系统中运行的多个任务。\n进程与线程的区别： ​\t进程：有独立的内存空间，进程中的数据存放空间（堆空间和栈空间）是独立的，至少有一个线程。\n​\t线程：堆空间是共享的，栈空间是独立的，线程消耗的资源比进程小的多。\n堆空间: 存储对象实例和数组的内存区域 栈空间: 用于存储局部变量、函数参数和返回地址, 栈的大小通常在线程创建时确定，并在线程生命周期内保持不变 注意： 因为一个进程中的多个线程是并发运行的，那么从微观角度看也是有先后顺序的，哪个线程执行完全取决于 CPU 的调度，程序员是不能完全控制的（可以设置线程优先级）。而这也就造成的多线程的随机性。 Java 程序的进程里面至少包含两个线程，==主线程也就是 main()方法线程，另外一个是垃圾回收机制线程==。每 当使用 java 命令执行一个类时，实际上都会启动一个 JVM，每一个 JVM 实际上就是在操作系统中启动了一个 线程，java 本身具备了垃圾的收集机制，所以在 Java 运行时至少会启动两个线程。 由于创建一个线程的开销比创建一个进程的开销小的多，那么我们在开发多任务运行的时候，通常考虑创建 多线程，而不是创建多进程。 1.3.3 多线程的创建 创建Maven工程，编写测试类\n1.继承Thread类 ​\t第一种继承Thread类 重写run方法\npublic class Demo1CreateThread extends Thread {\rpublic static void main(String[] args) throws InterruptedException {\rSystem.out.println(\u0026quot;-----多线程创建开始-----\u0026quot;);\r// 1.创建一个线程\rCreateThread createThread1 = new CreateThread();\rCreateThread createThread2 = new CreateThread();\r// 2.开始执行线程 注意 开启线程不是调用run方法，而是start方法\rSystem.out.println(\u0026quot;-----多线程创建启动-----\u0026quot;);\rcreateThread1.start();\rcreateThread2.start();\rSystem.out.println(\u0026quot;-----多线程创建结束-----\u0026quot;);\r}\rstatic class CreateThread extends Thread {\rpublic void run() {\rString name = Thread.currentThread().getName();\rfor (int i = 0; i \u0026lt; 5; i++) {\rSystem.out.println(name + \u0026quot;打印内容是:\u0026quot; + i);\r}\r}\r}\r}\r2.实现Runnable接口 ​\t实现Runnable接口,重写run方法\n​\t实际上所有的多线程代码都是通过运行Thread的start()方法来运行的。因此，不管是继承Thread类还是实现Runnable接口来实现多线程，最终还是通过Thread的对象的API来控制线程的。\npublic class Demo2CreateRunnable {\rpublic static void main(String[] args) {\rSystem.out.println(\u0026quot;-----多线程创建开始-----\u0026quot;);\r// 1.创建线程\rCreateRunnable createRunnable = new CreateRunnable();\rThread thread1 = new Thread(createRunnable);\rThread thread2 = new Thread(createRunnable);\r// 2.开始执行线程 注意 开启线程不是调用run方法，而是start方法\rSystem.out.println(\u0026quot;-----多线程创建启动-----\u0026quot;);\rthread1.start();\rthread2.start();\rSystem.out.println(\u0026quot;-----多线程创建结束-----\u0026quot;);\r}\rstatic class CreateRunnable implements Runnable {\rpublic void run() {\rString name = Thread.currentThread().getName();\rfor (int i = 0; i \u0026lt; 5; i++) {\rSystem.out.println(name + \u0026quot;的内容:\u0026quot; + i);\r}\r}\r}\r}\r实现Runnable接口比继承Thread类所具有的优势：\n==适合多个相同的程序代码的线程去共享同一个资源==。 ==可以避免java中的单继承的局限性==。 增加程序的健壮性，实现解耦操作，==代码可以被多个线程共享，代码和数据独立==。 3.匿名内部类方式 ​\t使用线程的内匿名内部类方式，可以方便的实现每个线程执行不同的线程任务操作\npublic class Demo3Runnable {\rpublic static boolean exit = true;\rpublic static void main(String[] args) throws InterruptedException {\rnew Thread(new Runnable() {\rpublic void run() {\rString name = Thread.currentThread().getName();\rfor (int i = 0; i \u0026lt; 5; i++) {\rSystem.out.println(name + \u0026quot;执行内容：\u0026quot; + i);\r}\r}\r}).start();\rnew Thread(new Runnable() {\rpublic void run() {\rString name = Thread.currentThread().getName();\rfor (int i = 0; i \u0026lt; 5; i++) {\rSystem.out.println(name + \u0026quot;执行内容：\u0026quot; + i);\r}\r}\r}).start();\rThread.sleep(1000l);\r}\r}}\r上面我们介绍了创建多线程的两种方式实现Runnable接口和继承Thread类，接下来看这个demo\nstatic Integer num = 0;\rpublic static void main(String[] args) {\rRunnable runnable = new Runnable() {\r@Override\rpublic void run() {\rwhile (num\u0026lt;10000){\rnum++;\r}\r}\r};\rnew Thread(runnable).start();\r//TODO。。。。。。。\r}\r我们想要获取该线程执行完毕之后num的值，但是我们如何确定线程已经执行完毕了呢,当然我们可以等待一段时间让线程执行完毕，但是等待多久呢，很难估计一个准确的时间，那么有没有更好的办法。当然有，可以使用Callable接口和FutureTask来解决。\n4.实现Callable接口，使用FutureTask 改造上面的案例\nstatic Integer num = 0;\rpublic static void main(String[] args) {\rCallable\u0026lt;String\u0026gt; callableTask = new Callable(){\r@Override\rpublic String call() throws Exception {\rwhile (num\u0026lt;10000){\rnum++;\r}\rreturn \u0026quot;ok\u0026quot;;\r}\r};\rFutureTask\u0026lt;String\u0026gt; stringFutureTask = new FutureTask(callableTask);\rnew Thread(stringFutureTask).start();\rtry {\rif(\u0026quot;ok\u0026quot;.equals(stringFutureTask.get())){\r}\r} catch (InterruptedException e) {\re.printStackTrace();\r} catch (ExecutionException e) {\re.printStackTrace();\r}\rSystem.out.println(num);\r}\r现在我们可以优雅的拿到一个线程执行的结果了，但是如果是多个线程呢，又该如何解决线程执行完毕的通知行为呢？\nstatic Integer num = 0;\rpublic static void main(String[] args) {\rCallable\u0026lt;String\u0026gt; callableTask = new Callable(){\r@Override\rpublic String call() throws Exception {\rwhile (num\u0026lt;10000){\r//System.out.println(Thread.currentThread().getName());\rnum++;\r}\rreturn \u0026quot;ok\u0026quot;;\r}\r};\rFutureTask\u0026lt;String\u0026gt; stringFutureTask = new FutureTask(callableTask);\rfor (int i = 0; i \u0026lt; 10; i++) {\rnew Thread(stringFutureTask).start();\r}\rtry {\rif(\u0026quot;ok\u0026quot;.equals(stringFutureTask.get())){\r}\r} catch (InterruptedException e) {\re.printStackTrace();\r} catch (ExecutionException e) {\re.printStackTrace();\r}\rSystem.out.println(num);\r}\r我们用10个线程模拟获取多线程情况下的执行结果，看起来代码好像没有问题，但是如果取消System.out.println(Thread.currentThread().getName());的注释，你会发现，一直是同一个线程在执行，说好的多线程呢？那么为什么会出现这种情况，我们翻开FutureTask的源码，能够作为Thread的参数，那么它必是一个runnable接口的实现类，我们直接找它的run方法：\npublic void run() {\rif (state != NEW ||\r//这里是一个cas方法，意味着同时只有一个线程会执行callable的call方法。\r!UNSAFE.compareAndSwapObject(this, runnerOffset,\rnull, Thread.currentThread()))\rreturn;\rtry {\rCallable\u0026lt;V\u0026gt; c = callable;\rif (c != null \u0026amp;\u0026amp; state == NEW) {\rV result;\rboolean ran;\rtry {\rresult = c.call();\rran = true;\r} catch (Throwable ex) {\rresult = null;\rran = false;\rsetException(ex);\r}\rif (ran)\rset(result);\r}\r} finally {\r// runner must be non-null until state is settled to\r// prevent concurrent calls to run()\rrunner = null;\r// state must be re-read after nulling runner to prevent\r// leaked interrupts\rint s = state;\rif (s \u0026gt;= INTERRUPTING)\rhandlePossibleCancellationInterrupt(s);\r}\r}\r所以我们发现futureTask并不能支持多个线程同时执行，它只能支持一个线程等待另一个线程执行的结果时使用，那么我们又该如何解决多线程的异步通知问题呢，这个我们到之后学习了juc的CAS，AQS以及一些工具类之后再回来思考，先挖一个坑在这里。\n1.3.4 用户线程和守护线程 Java中有两种线程，一种是用户线程，另一种是守护线程。\n用户线程是指用户自定义创建的线程，主线程停止，用户线程不会停止。\n守护线程当进程不存在或主线程停止，守护线程也会被停止。\npublic class Demo4Daemon {\rpublic static void main(String[] args) {\rThread thread = new Thread(new Runnable() {\rpublic void run() {\rfor (int i = 0; i \u0026lt; 10; i++) {\rtry {\rThread.sleep(10);\r} catch (Exception e) {\r}\rSystem.out.println(\u0026quot;子线程...\u0026quot; + i);\r}\r}\r});\r// 设置线程为守护线程\r//thread.setDaemon(true);\rthread.start();\rfor (int i = 0; i \u0026lt; 5; i++) {\rtry {\rThread.sleep(10);\rSystem.out.println(\u0026quot;主线程\u0026quot; + i);\r} catch (Exception e) {\r}\r}\rSystem.out.println(\u0026quot;主线程执行完毕!\u0026quot;);\r}\r}\r1.4 小结 多线程：通过空间（cpu的时间片的利用）换取响应时间\n线程和进程的概念\n进程：程序的一次执行，进程之间内存是独立的，无法共享内存空间，至少有一个线程。 线程：进程内部的最小执行单元，线程之间是共享堆内存，栈内存是独立的。 创建线程的方式\n继承Thread类，重写run方法 实现Runnable接口，实现里面的run方法 匿名内部类实现Runnable接口，New Thread(()-\u0026gt;执行内容) 线程的执行：一定要用Start方法执行线程，如果run方法执行是直接执行类的方法，不会以线程的方式执行\n用户线程和守护线程：\n用户线程：一般是用户创建的，不会随着主线程的终止而终止\n守护线程：一般是系统创建的，会随着主线的终止而终止，垃圾回收线程就是守护线程，可以使用Thread::setDaemon方法将用户线程转化为守护线程\n​\n2 线程安全 1.1 目标 ​\t理解多线程技术的一些基本概念。\n1.2 路径 通过案例演示理解线程安全的重要性 保证线程安全的方案 死锁 1.3 讲解 线程安全：当多个线程执行一段程序的时候，如果可能发生和预期结果不一致的情况，就是线程不安全的。如果一致就是线程安全，一般线程安全的问题都是伴随着共享变量发生的，只有代码满足了原子性，可见性，有序性才是线程安全的，有一个不满足就不是线程安全。\n1.3.1.卖票案例 ​\t如果有多个线程在同时运行，而这些线程可能会同时运行这段代码。程序每次运行结果和单线程运行的结果是一样的，而且其他的变量的值也和预期的是一样的，就是线程安全的，反之则是线程不安全的。\npublic class Demo5Ticket {\rpublic static void main(String[] args) {\r//创建线程任务对象\rTicket ticket = new Ticket();\r//创建三个窗口对象\rThread t1 = new Thread(ticket, \u0026quot;窗口1\u0026quot;);\rThread t2 = new Thread(ticket, \u0026quot;窗口2\u0026quot;);\rThread t3 = new Thread(ticket, \u0026quot;窗口3\u0026quot;);\r//卖票\rt1.start();\rt2.start();\rt3.start();\r}\rstatic class Ticket implements Runnable {\r//Object lock = new Object();\rReentrantLock lock = new ReentrantLock();\rprivate int ticket = 10;\rpublic void run() {\rString name = Thread.currentThread().getName();\rwhile (true) {\rsell(name);\rif (ticket \u0026lt;= 0) {\rbreak;\r}\r}\r}\rprivate void sell(String name) {\rtry {\rThread.sleep(10);\r} catch (InterruptedException e) {\re.printStackTrace();\r}\rif (ticket \u0026gt; 0) {\rSystem.out.println(name + \u0026quot;卖票：\u0026quot; + ticket);\rticket--;\r}\r}\r}\r}\r​\t线程安全问题都是由全局变量及静态变量引起的。若每个线程中对全局变量、静态变量只有读操作，而无写 操作，一般来说，这个全局变量是线程安全的；若有多个线程同时执行写操作，一般都需要考虑线程同步， 否则的话就可能影响线程安全。\n1.3.2.线程同步 当我们使用多个线程访问同一资源的时候，且多个线程中对资源有写的操作，就容易出现线程安全问题。 要解决上述多线程并发访问一个资源的安全问题，Java中提供了同步机制(synchronized)来解决。\nsynchronized都是对象锁\n同步代码块\nObject lock = new Object(); //创建锁\rsynchronized(lock){\r//可能会产生线程安全问题的代码\r}\r同步方法\n//同步方法 this对象\rpublic synchronized void method(){\r//可能会产生线程安全问题的代码 }\r同步方法使用的是this锁\n证明方式: 一个线程使用同步代码块(this明锁),另一个线程使用同步函数。如果两个线程抢票不能实现同步，那么会出现数据错误。\n//使用this锁的同步代码块\rsynchronized(this){\r//需要同步操作的代码\r}\r1.synchronized(lock)，对lock加锁\n2.同步方法：this锁\n3.静态同步方法：当前类的class对象\nLock锁\nLock lock = new ReentrantLock();\rlock.lock();\r//需要同步操作的代码\rlock.unlock();\r1.3.3.死锁 多线程死锁：同步中嵌套同步,导致锁无法释放。\n死锁解决办法：不要在同步中嵌套同步\npublic class Demo6DeadLock {\rpublic static void main(String[] args) {\r//创建线程任务对象\rTicket ticket = new Ticket();\r//创建三个窗口对象\rThread t1 = new Thread(ticket, \u0026quot;窗口1\u0026quot;);\rThread t2 = new Thread(ticket, \u0026quot;窗口2\u0026quot;);\rThread t3 = new Thread(ticket, \u0026quot;窗口3\u0026quot;);\r//卖票\rt1.start();\rt2.start();\rt3.start();\r}\rstatic class Ticket implements Runnable {\rObject lock = new Object();\rprivate int ticket = 100;\rpublic void run() {\rString name = Thread.currentThread().getName();\rwhile (true) {\rif (\u0026quot;窗口1\u0026quot;.equals(name)) {\rsynchronized (lock) {\rsell(name);\r}\r} else {\rsell(name);\r}\rif (ticket \u0026lt;= 0) {\rbreak;\r}\r}\r}\rprivate synchronized void sell(String name) {\rsynchronized (lock) {\rif (ticket \u0026gt; 0) {\rSystem.out.println(name + \u0026quot;卖票：\u0026quot; + ticket);\rticket--;\r}\r}\r}\r}\r死锁案例二\npublic class DeadLockDemo {\rstatic Object obj1 = new Object();\rstatic Object obj2 = new Object();\rpublic static void fun01(){\rSystem.out.println(Thread.currentThread().getId()+\u0026quot;尝试获取obj1的锁\u0026quot;);\rsynchronized (obj1){\rSystem.out.println(Thread.currentThread().getId()+\u0026quot;获取到了ob1的锁\u0026quot;);\rtry {\rThread.sleep(30);\r} catch (InterruptedException e) {\re.printStackTrace();\r}\rfun02();\r}\r}\rpublic static void fun02(){\rSystem.out.println(Thread.currentThread().getId()+\u0026quot;尝试获取obj2的锁\u0026quot;);\rsynchronized (obj2){\rSystem.out.println(Thread.currentThread().getId()+\u0026quot;获取到了ob2的锁\u0026quot;);\rtry {\rThread.sleep(30);\r} catch (InterruptedException e) {\re.printStackTrace();\r}\rfun01();\r}\r}\rpublic static void main(String[] args) {\rThread thread1 = new Thread(() -\u0026gt; fun01());\rThread thread2 = new Thread(() -\u0026gt; fun02());\rthread1.start();\rthread2.start();\r}\r}\r1.4 小结 线程安全：当多线程执行同一段程序的时候，如果发生了和预期结果不一致的情况，就是线程不安全的，如果和预期结果一致就是线程安全的，可以加锁解决(把并行运行的线程变成串行化执行)。\n同步锁的几种方式(锁对象):\n同步代码块加锁：sync\u0026hellip;.(obj) 同步方法加锁：等价于sync\u0026hellip;.(this) 静态同步方法加锁:等价于sync\u0026hellip;.(this.getClass()) 死锁：线程之间互相等待对方释放锁，就产生了死锁，尽量不要同步中嵌套同步。\n3 线程状态 1.1 目标 ​\t掌握线程在运行期间状态的变化。\n1.2 路径 线程状态介绍 wait()和notify() wait()和sleep的区别 线程停止 1.3 讲解 1.3.1.线程状态介绍 查看Thread源码，能够看到java的线程有六种状态：\npublic enum State {\rNEW,\rRUNNABLE,\rBLOCKED,\rWAITING,\rTIMED_WAITING,\rTERMINATED;\r}\r**NEW(新建) ** 线程刚被创建，但是并未启动。 RUNNABLE(可运行) 线程可以在java虚拟机中运行的状态，可能正在运行自己代码，也可能没有，这取决于操作系统处理器。 BLOCKED(锁阻塞) 当一个线程试图获取一个对象锁，而该对象锁被其他的线程持有，则该线程进入Blocked状态；当该线程持有锁时，该线程将变成Runnable状态。 WAITING(无限等待) 一个线程在等待另一个线程执行一个（唤醒）动作时，该线程进入Waiting状态。进入这个状态后是不能自动唤醒的，必须等待另一个线程调用notify或者notifyAll方法才能够唤醒。 TIMED_WAITING(计时等待) 同waiting状态，有几个方法有超时参数，调用他们将进入Timed Waiting状态。这一状态将一直保持到超时期满或者接收到唤醒通知。带有超时参数的常用方法有Thread.sleep 、Object.wait。 TERMINATED(被终止) 因为run方法正常退出而死亡，或者因为没有捕获的异常终止了run方法而死亡。\n线程状态图 1.3.2.wait()、notify() wait()、notify()、notifyAll()是三个定义在Object类里的方法，可以用来控制线程的状态。\nwait\t方法会使持有该对象的线程把该对象的控制权交出去，然后处于等待状态。 notify\t方法会通知某个正在等待这个对象的控制权的线程继续运行，能否继续运行取决于是否获取到锁。 notifyAll\t方法会通知所有正在等待这个对象的控制权的线程继续运行，能否继续运行取决于是否获取到锁。\n注意：一定要在线程同步中使用,并且是同一个锁的资源\nwait和notify方法例子，打开关闭开关：\npublic class DemoSwitch {\rstatic class Switch{\r//开关状态\rBoolean state = false;\rpublic Boolean getState() {\rreturn state;\r}\rpublic void setState(Boolean state) throws Exception {\rString s = state?\u0026quot;打开\u0026quot;:\u0026quot;关闭\u0026quot;;\rSystem.out.println(s);\rif(this.state==state){\rthrow new Exception(\u0026quot;开关不能连续\u0026quot;+s);\r}\rthis.state = state;\r}\r}\rstatic abstract class ActionThread implements Runnable{\r//开关\rSwitch aSwitch;\rpublic ActionThread(Switch aSwitch) {\rthis.aSwitch = aSwitch;\r}\r@Override\rpublic void run() {\r//循环\rwhile (true){\r//同步\rsynchronized (aSwitch){\rtry {\raction();\r} catch (Exception e) {\re.printStackTrace();\rbreak;\r}\r}\r}\r}\rprotected abstract void action() throws Exception;\r}\rpublic static void main(String[] args) {\r//1.创建开关\rSwitch aSwitch = new Switch();\r//2.关闭线程\rThread thread1 = new Thread(new ActionThread(aSwitch){\r@Override\rprotected void action() throws Exception {\r//RUNNABLE\rSystem.out.println(\u0026quot;thread1获取到aSwitch对象的锁后:\u0026quot;+Thread.currentThread().getState().name());\r//如果开关是关闭的\rif(!this.aSwitch.getState()){\rtry {\r//让当前线程释放锁，进入等待\rthis.aSwitch.wait();\r} catch (InterruptedException e) {\re.printStackTrace();\r}\r}\r//关闭开关\rthis.aSwitch.setState(false);\r//唤醒等待中的一个线程\rthis.aSwitch.notify();\r}\r});\r//new\rSystem.out.println(\u0026quot;创建thread1未运行时的状态是:\u0026quot;+thread1.getState().name());\r//2.打开线程\rThread thread2 = new Thread(new ActionThread(aSwitch){\r@Override\rprotected void action() throws Exception {\r//BLOCKED\rSystem.out.println(\u0026quot;thread2获取到aSwitch对象的锁后:\u0026quot;+thread1.getState().name());\r//如果开关是打开的\rif(this.aSwitch.getState()){\rtry {\r//让当前线程释放锁，进入等待\rthis.aSwitch.wait();\r} catch (InterruptedException e) {\re.printStackTrace();\r}\r}\r//打开开关\rthis.aSwitch.setState(true);\r//wait\rSystem.out.println(\u0026quot;thread2线程在调用notify之前:\u0026quot;+thread1.getState().name());\r//唤醒等待中的一个线程\rthis.aSwitch.notify();\r}\r});\rthread1.start();\rthread2.start();\r}\r}\r1.3.3.wait与sleep区别 对于sleep()方法，首先要知道该方法是属于Thread类中的。而wait()方法，则是属于Object类中的。\nsleep()方法导致了程序暂停执行指定的时间，让出cpu调度其他线程，但是他的监控状态依然保持者，当指定的时间到了又会自动恢复运行状态。\nwait()是把控制权交出去，然后进入等待此对象的等待锁定池处于等待状态，只有针对此对象调用notify()方法后本线程才进入对象锁定池准备获取对象锁进入运行状态。\n在调用sleep()方法的过程中，线程不会释放对象锁。而当调用wait()方法的时候，线程会放弃对象锁。\n1.3.4.线程停止 结束线程有以下三种方法： （1）自定义退出标志，使线程正常退出。 （2）使用interrupt()方法中断线程，使用线程内部的退出标志。 （3）使用stop方法强行终止线程（不推荐使用Thread.stop, 这种终止线程运行的方法已经被废弃，使用它们是极端不安全的！）\n1.5.1 自定义退出标志 一般run()方法执行完，线程就会正常结束，然而，常常有些线程是伺服线程。它们需要长时间的运行，只有在外部某些条件满足的情况下，才能关闭这些线程。使用一个变量来控制循环，例如：最直接的方法就是设一个boolean类型的标志，并通过设置这个标志为true或false来控制while循环是否退出，代码示例：\npublic class Demo8Exit {\rpublic static boolean exit = true;\rpublic static void main(String[] args) throws InterruptedException {\rThread t = new Thread(new Runnable() {\rpublic void run() {\rwhile (exit) {\rtry {\rSystem.out.println(\u0026quot;线程执行！\u0026quot;);\rThread.sleep(100l);\r} catch (InterruptedException e) {\re.printStackTrace();\r}\r}\r}\r});\rt.start();\rThread.sleep(1000l);\rexit = false;\rSystem.out.println(\u0026quot;退出标识位设置成功\u0026quot;);\r}\r}\r1.5.2 使用interrupt()方法 使用interrupt()方法来中断线程有两种情况：\n1)线程处于阻塞状态\n​\t如使用了sleep,同步锁的wait,socket中的receiver,accept等方法时，会使线程处于阻塞状态。当调用线程的interrupt()方法时，会抛出InterruptException异常。阻塞中的那个方法抛出这个异常，通过代码捕获该异常，然后break跳出循环状态，从而让我们有机会结束这个线程的执行。\n2)线程未处于阻塞状态\n​\t使用isInterrupted()判断线程的中断标志来退出循环。当使用interrupt()方法时，中断标志就会置true，和使用自定义的标志来控制循环是一样的道理。\npublic class Demo9Interrupt {\rpublic static boolean exit = true;\rpublic static void main(String[] args) throws InterruptedException {\rThread t = new Thread(new Runnable() {\rpublic void run() {\rwhile (exit) {\rtry {\rSystem.out.println(\u0026quot;线程执行！\u0026quot;);\r//判断线程的中断标志来退出循环\rif (Thread.currentThread().isInterrupted()) {\rbreak;\r}\rThread.sleep(100l);\r} catch (InterruptedException e) {\re.printStackTrace();\r//线程处于阻塞状态,当调用线程的interrupt()方法时，\r//会抛出InterruptException异常,跳出循环\rbreak;\r}\r}\r}\r});\rt.start();\rThread.sleep(1000l);\r//中断线程\rt.interrupt();\rSystem.out.println(\u0026quot;线程中断了\u0026quot;);\r}\r}\r1.4 小结 线程状态：\nNEW：线程被创建但是没有start运行 RUNABLE:线程可以运行，是否运行取决于cpu是否调度该线程，如果没有调度就是ready，如调度到就是running WAITING：当锁对象调用wait方法，会让持有该锁对象的线程进入无限等待状态，这个状态只有被同一个锁对象的notify才能解除，解除后进入RUNABLE状态。 TIMED_WAITING：sleep(time),wait(time)的时候进入计时等待，当时间到了，继续运行 BLOCKED：在线程获取不到锁对象的时候，就会进入阻塞状态，当其他线程释放锁，本线程获取到锁才能够继续运行。 TERMINATED：run方法执行完毕之后，进入终止状态。 wait和sleep的区别：\nwait是属于object对象的方法，sleep是属于Thread类的方法 wait会让当前持有该对象锁的线程停止运行，进入waiting状态，并且会释放锁，只有同一个对象的notify/notifyAll方法才能够唤醒,sleep不会释放锁，让线程等待一段时间继续运行。 线程终止：\n自定义退出标识：缺点是线程在sleep的时候是无法立刻退出，必须等线程执行到了判断标识的地方才能够停止\n使用线程自己的退出标识：\n如果线程正常运行，可以通过Thread.currentThread().isInterrupted()判断退出\n如果当前线程正在sleep，通过‘t.interrupt();’会抛出异常，可以通过捕获异常立刻线程的运行。\n4 线程优先级 1.1 目标 ​\t理解多线程技术的一些基本概念。\n1.2 路径 设置并理解线程的优先级priority 理解并掌握join()方法的使用 理解yield()方法的概念 1.3 讲解 1.优先级priority ​\t现今操作系统基本采用分时的形式调度运行的线程，线程分配得到时间片的多少决定了线程使用处理器资源的多少，也对应了线程优先级这个概念。\n​\t在JAVA线程中，通过一个int priority来控制优先级，范围为1-10，其中10最高，默认值为5。\npublic class Demo10Priorityt {\rpublic static void main(String[] args) {\rPrioritytThread prioritytThread = new PrioritytThread();\r// 如果8核CPU处理3线程，无论优先级高低，每个线程都是单独一个CPU执行，就无法体现优先级\r// 开启10个线程，让8个CPU处理，这里线程就需要竞争CPU资源，优先级高的能分配更多的CPU资源\rfor (int i = 0; i \u0026lt; 10; i++) {\rThread t = new Thread(prioritytThread, \u0026quot;线程\u0026quot; + i);\rif (i == 1) {\rt.setPriority(10);\r}\rif (i == 2) {\rt.setPriority(1);\r}\rt.setDaemon(true);\rt.start();\r}\rtry {\rThread.sleep(1000l);\r} catch (InterruptedException e) {\re.printStackTrace();\r}\rSystem.out.println(\u0026quot;线程1总计：\u0026quot; + PrioritytThread.count1);\rSystem.out.println(\u0026quot;线程2总计：\u0026quot; + PrioritytThread.count2);\r}\rstatic class PrioritytThread implements Runnable {\rpublic static Integer count1 = 0;\rpublic static Integer count2 = 0;\rpublic void run() {\rwhile (true) {\rif (\u0026quot;线程1\u0026quot;.equals(Thread.currentThread().getName())) {\rcount1++;\r}\rif (\u0026quot;线程2\u0026quot;.equals(Thread.currentThread().getName())) {\rcount2++;\r}\rif (Thread.currentThread().isInterrupted()) {\rbreak;\r}\r}\r}\r}\r}\r2.join()方法 join作用是让其他线程变为等待。thread.Join把指定的线程加入到当前线程，可以将两个交替执行的线程合并为顺序执行的线程。比如在线程B中调用了线程A的Join()方法，直到线程A执行完毕后，才会继续执行线程B。\npublic class Demo11Join {\rpublic static void main(String[] args) {\rThread thread1 = new Thread(new Runnable() {\r@Override\rpublic void run() {\rfor (int i = 0; i \u0026lt; 10; i++) {\rString name = Thread.currentThread().getName();\rSystem.out.println(name+\u0026quot;执行\u0026quot;+i);\r}\r}\r}, \u0026quot;线程1\u0026quot;);\rThread thread2 = new Thread(new Runnable() {\r@Override\rpublic void run() {\rtry {\rthread1.join();\r} catch (InterruptedException e) {\re.printStackTrace();\r}\rfor (int i = 0; i \u0026lt; 10; i++) {\rString name = Thread.currentThread().getName();\rSystem.out.println(name+\u0026quot;执行\u0026quot;+i);\r}\r}\r}, \u0026quot;线程2\u0026quot;);\rThread thread3 = new Thread(new Runnable() {\r@Override\rpublic void run() {\rtry {\rthread2.join();\r} catch (InterruptedException e) {\re.printStackTrace();\r}\rfor (int i = 0; i \u0026lt; 10; i++) {\rString name = Thread.currentThread().getName();\rSystem.out.println(name+\u0026quot;执行\u0026quot;+i);\r}\r}\r}, \u0026quot;线程3\u0026quot;);\rthread1.start();\rthread2.start();\rthread3.start();\r/* try {\rthread3.join();\r} catch (Exception e) {\r}*/\rfor (int i = 0; i \u0026lt; 5; i++) {\rSystem.out.println(\u0026quot;main ---i:\u0026quot; + i);\r}\r}\r}\r3.yield方法（running-\u0026gt;ready） Thread.yield()方法的作用：暂停当前正在执行的线程，并执行其他线程。（可能没有效果） yield()让当前正在运行的线程回到可运行状态，以允许具有相同优先级的其他线程获得运行的机会。因此，使用yield()的目的是让具有相同优先级的线程之间能够适当的轮换执行。但是，实际中无法保证yield()达到让步的目的，因为，让步的线程可能被线程调度程序再次选中。\n查看源码介绍：\n结论：大多数情况下，yield()将导致线程从运行状态转到可运行状态，但有可能没有效果。\n1.4 小结 优先级：从1-10，默认值是5，表示线程被cpu调度的概率，值越大优先级越高就越有可能被cpu调度到去执行，从宏观角度来说，不同优先级的线程在运行一段时间后，它们的优先级表示的就是线程占用cpu时间片的多少。\njoin方法：线程a里面调用线程b.join方法，就是让线程b先执行，b执行完了在执行线程a\nyield方法：就是让当前线程让出cpu资源（running-\u0026gt;ready切换）,可能没有效果，因为cpu有可能再度调度到该线程执行\n5. 多线程并发的3个特性 1.1 目标 ​\t理解多线程并发的3个特性。\n1.2 路径 原子性 可见性 有序性 1.3 讲解 ​\t多线程并发开发中，要知道什么是多线程的原子性，可见性和有序性，以避免相关的问题产生。\n1.3.1 原子性（不可分割的最小单位） 原子性：即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行\n一个很经典的例子就是银行账户转账问题：\n比如从账户A向账户B转1000元，那么必然包括2个操作：从账户A减去1000元，往账户B加上1000元。\n试想一下，如果这2个操作不具备原子性，会造成什么样的后果。假如从账户A减去1000元之后，操作突然中止。这样就会导致账户A虽然减去了1000元，但是账户B没有收到这个转过来的1000元。\n所以这2个操作必须要具备原子性才能保证不出现一些意外的问题。\n回顾之前的买票案例。\n1.3.2 可见性 可见性：当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值\n前面讲过多线程的内存可见性，现在我们写一个内存不可见的问题。\n案例如下：\npublic class DemoVisible {\rpublic static void main(String[] args) throws InterruptedException {\rJmmDemo demo = new JmmDemo();\rThread t = new Thread(demo);\rt.start();\rThread.sleep(100);\rdemo.flag = false;\rSystem.out.println(\u0026quot;已经修改为false\u0026quot;);\rSystem.out.println(demo.flag);\r}\rstatic class JmmDemo implements Runnable {\rpublic boolean flag = true;\rpublic void run() {\rSystem.out.println(\u0026quot;子线程执行。。。\u0026quot;);\rwhile (flag) {\r}\rSystem.out.println(\u0026quot;子线程结束。。。\u0026quot;);\r}\r}\r}\r执行结果\n​\t按照main方法的逻辑，我们已经把flag设置为false，那么从逻辑上讲，子线程就应该跳出while死循环，因为这个时候条件不成立，但是我们可以看到，程序仍旧执行中，并没有停止。\n​\t原因:\n​\t1.和java内存模型有关，多线程访问共享变量时会创建副本变量，修改后再写回主变量，其他线程副本变量的值没有及时更新，导致可见性的问题。\n//线程1执行的代码\rint i = 0;\ri = 10;\r//线程2执行的代码\rj = i;\r当线程1执行int i = 0这句时，i的初始值0加载到内存中，然后再执行i = 10，那么在内存中i的值变为10了。\n如果当线程1执行到int i = 0这句时，此时线程2执行 j = i，它读取i的值并加载到内存中，注意此时内存当中i的值是0，那么就会使得j的值也为0，而不是10。\n这就是可见性问题，线程1对变量i修改了之后，线程2没有立即看到线程1修改的值。\n​\t解决方案：一般使用volatile或同步锁保证其内存的可见性\n1.3.3 有序性 有序性：程序执行的顺序按照代码的先后顺序执行\nboolean flag = false;\rint count = 0;\rcount = 1; //语句1\rflag = true; //语句2\r​\t以上代码定义了一个int型变量，定义了一个boolean类型变量，然后分别对两个变量进行赋值操作。从代码顺序上看，语句1是在语句2前面的，那么JVM在真正执行这段代码的时候会保证语句1一定会在语句2前面执行吗？不一定，为什么呢？这里可能会发生指令重排序（Instruction Reorder）。\n​\t什么是重排序？一般来说，处理器为了提高程序运行效率，可能会对输入代码进行优化，它不保证程序中各个语句的执行先后顺序同代码中的顺序一致。\n​\tas-if-serial:无论如何重排序，程序最终执行结果和代码顺序执行的结果是一致的。Java编译器、运行时和处理器都会保证Java在单线程下遵循as-if-serial语意）\n​\t上面的代码中，语句1和语句2谁先执行对最终的程序结果并没有影响，那么就有可能在执行过程中，语句2先执行而语句1后执行。但是要注意，虽然处理器会对指令进行重排序，但是它会保证程序最终结果会和代码顺序执行结果相同，那么它靠什么保证的呢？\n再看下面一个例子：\nint a = 10; //语句1\rint b = 2; //语句2\ra = a + 3; //语句3\rb = a*a; //语句4\r这段代码有4个语句，那么可能的一个执行顺序是： 语句2 语句1 语句3 语句4\n不可能是这个执行顺序： 语句2 语句1 语句4 语句3\n因为处理器在进行重排序时是会考虑指令之间的数据依赖性，如果一个指令Instruction 2必须用到Instruction 1的结果，那么处理器会保证Instruction 1会在Instruction 2之前执行。==虽然重排序不会影响单个线程内程序执行的结果，但是多线程会有影响==\n下面看一个例子：\n//线程1:\rinit = false\rcontext = loadContext(); //语句1\rinit = true; //语句2\r//线程2:\rwhile(!init){//如果初始化未完成，等待\rsleep();\r}\rexecute(context);//初始化完成，执行逻辑\r上面代码中，==由于语句1和语句2没有数据依赖性，因此可能会被重排序==。假如发生了重排序，在线程1执行过程中先执行语句2，而此是线程2会以为初始化工作已经完成，那么就会跳出while循环，去执行execute(context)方法，而此时context并没有被初始化，就会导致程序出错。\n从上面可以看出，重排序不会影响单个线程的执行，但是会影响到线程并发执行的正确性。\n1.4 小结 ​\t要想并发程序正确地执行，必须要保证原子性、可见性以及有序性。只要有一个没有被保证，就有可能会导致程序运行不正确。\n​\t原子性：不可分割，说明这一段程序要么都执行，要么不执行，并且不能被其他线程影响，可以使用同步方法,同步块，锁解决。\n​\t可见性：多线程对于共享变量的访问，必须每次都获取到主变量的值。可以使用volatile或者同步方法，同步块，锁解决。\n​\t原因1：和内存模型有关，每次去读取内存中的值会加载到cpu缓存。\n​\t原因2：jvm有时候会认为程序是单线程执行，或者执行的逻辑对变量没有影响，就会重复读取cpu缓存的变量，导致可见性的问题。\n​\t有序性:由jvm指令重排导致的，指令重排的结果对于单线程来说是一致性，没有什么影响（在指令重排的过程中会考虑数据依赖的问题），多线程情况下避免指令重排带来程序执行错误的危害。有些情况可以使用volatile，或者使用synchronized同步块或者同步方法或者同步锁。\n6. Java内存可见性 1.1 目标 了解java内存模型\n理解java内存可见性\n1.2 路径 java内存模型介绍 java内存可见性的介绍 1.3 讲解 3.1 了解Java内存模型 ​\tJVM内存结构、Java对象模型和Java内存模型，这就是三个截然不同的概念，而这三个概念很容易混淆。这里详细区别一下\n3.1.1 JVM内存结构; 运行时数据区 ​\t我们都知道，Java代码是要运行在虚拟机上的，而虚拟机在执行Java程序的过程中会把所管理的内存划分为若干个不同的数据区域，这些区域都有各自的用途。其中有些区域随着虚拟机进程的启动而存在，而有些区域则依赖用户线程的启动和结束而建立和销毁。\n在《Java虚拟机规范（Java SE 8）》中描述了JVM运行时内存区域结构如下：\n​\tJVM内存结构，由Java虚拟机规范定义。描述的是Java程序执行过程中，由JVM管理的不同数据区域。各个区域有其特定的功能。\n3.1.2 Java对象模型 ​\tJava是一种面向对象的语言，而Java对象在JVM中的存储也是有一定的结构的。而这个关于Java对象自身的存储模型称之为Java对象模型。\n​\tHotSpot虚拟机中（Sun JDK和OpenJDK中所带的虚拟机，也是目前使用范围最广的Java虚拟机），设计了一个OOP-Klass Model。OOP（Ordinary Object Pointer）指的是普通对象指针，而Klass用来描述对象实例的具体类型。\n​\t每一个Java类，在被JVM加载的时候，JVM会给这个类创建一个instanceKlass对象，保存在方法区，用来在JVM层表示该Java类。当我们在Java代码中，使用new创建一个对象的时候，JVM会创建一个instanceOopDesc对象，这个对象中包含了对象头以及实例数据。\n这就是一个简单的Java对象的OOP-Klass模型，即Java对象模型。\n3.1.3 内存模型 ​\tJava内存模型就是一种符合内存模型规范的，屏蔽了各种硬件和操作系统的访问差异的，保证了Java程序在各种平台下对内存的访问都能保证效果一致的机制及规范。\n有兴趣详细了解Java内存模型是什么，为什么要有Java内存模型，Java内存模型解决了什么问题的学员，参考：https://www.hollischuang.com/archives/2550。\n​\tJava内存模型是根据英文Java Memory Model（JMM）翻译过来的。其实JMM并不像JVM内存结构一样是真实存在的。他只是一个抽象的概念。JSR-133: Java Memory Model and Thread Specification中描述了，JMM是和多线程相关的，他描述了一组规则或规范，这个规范定义了一个线程对共享变量的写入时对另一个线程是可见的。\n​\t简单总结下，Java的多线程之间是通过共享内存进行通信的，而由于采用共享内存进行通信，在通信过程中会存在一系列如可见性、原子性、顺序性等问题，而JMM就是围绕着多线程通信以及与其相关的一系列特性而建立的模型。JMM定义了一些语法集，这些语法集映射到Java语言中就是volatile、synchronized等关键字。\nJMM线程操作内存的基本的规则：\n==第一条关于线程与主内存：线程对共享变量的所有操作都必须在自己的工作内存（本地内存）中进行，不能直接从主内存中读写==\n==第二条关于线程间本地内存：不同线程之间无法直接访问其他线程本地内存中的变量，线程间变量值的传递需要经过主内存来完成。==\n主内存\n主要存储的是Java实例对象，所有线程创建的实例对象都存放在主内存中，不管该实例对象是成员变量还是方法中的本地变量(也称局部变量)，当然也包括了共享的类信息、常量、静态变量。由于是共享数据区域，多条线程对同一个变量进行访问可能会发现线程安全问题。\n本地内存\n主要存储当前方法的所有本地变量信息(本地内存中存储着主内存中的变量副本拷贝)，每个线程只能访问自己的本地内存，即线程中的本地变量对其它线程是不可见的，就算是两个线程执行的是同一段代码，它们也会各自在自己的工作内存中创建属于当前线程的本地变量，当然也包括了字节码行号指示器、相关Native方法的信息。注意由于工作内存是每个线程的私有数据，线程间无法相互访问工作内存，因此存储在工作内存的数据不存在线程安全问题。\n3.1.4 小结 ​\tJVM内存结构，和Java虚拟机的运行时区域有关。\n​\tJava对象模型，和Java对象如何在jvm中存储的描述。\n​ Java内存模型，和Java的并发编程有关。\n3.2 内存可见性 3.2.1 内存可见性介绍 可见性：一个线程对共享变量值的修改，能够及时的被其他线程看到\n共享变量：如果一个变量在多个线程的工作内存中都存在副本，那么这个变量就是这几个线程的共享变量\n线程 A 与线程 B 之间如要通信的话，必须要经历下面 2 个步骤：\n首先，线程 A 把本地内存 A 中更新过的共享变量刷新到主内存中去。 然后，线程 B 到主内存中去读取线程 A 之前已更新过的共享变量。 ​\t如上图所示，本地内存 A 和 B 有主内存中共享变量 x 的副本。假设初始时，这三个内存中的 x 值都为 0。线程 A 在执行时，把更新后的 x 值（假设值为 1）临时存放在自己的本地内存 A 中。当线程 A 和线程 B 需要通信时，线程 A 首先会把自己本地内存中修改后的 x 值刷新到主内存中，此时主内存中的 x 值变为了 1。随后，线程 B 到主内存中去读取线程 A 更新后的 x 值，此时线程 B 的本地内存的 x 值也变为了 1。\n​\t从整体来看，这两个步骤实质上是线程 A 在向线程 B 发送消息，而且这个通信过程必须要经过主内存。JMM 通过控制主内存与每个线程的本地内存之间的交互，来为 java 程序员提供内存可见性保证。\n​\t回顾上面多线程并发的可见性问题的案例\n7 Volatile 7.1 目标 掌握volatile的使用方法和场景 理解volatile的作用原理 7.2 路径 解决内存可见性的问题 原子性的问题 volatile的适用场景 synchronized和volatile的对比 7.3 讲解 如果一个变量使用volatile，则它比使用synchronized的成本更加低，因为它不会引起线程上下文的切换和调度。\nJava语言规范对volatile的定义如下：\nJava允许线程访问共享变量，为了确保共享变量能被准确和一致地更新，线程应该确保通过排他锁单独获得这个变量。\n​\t通俗点讲就是说一个变量如果用volatile修饰了，则Java可以确保所有线程看到这个变量的值是一致的，如果某个线程对volatile修饰的共享变量进行更新，那么其他线程可以立马看到这个更新，这就是内存可见性。\nvolatile虽然看起来比较简单，使用起来无非就是在一个变量前面加上volatile即可，但是要用好并不容易。\n1 解决内存可见性问题 1.1 缓存一致协议保证读到最新值 设置-XX:+UnlockDiagnosticVMOptions -XX:+PrintAssembly 可以查看执行过程中的汇编码\n汇编代码：\r0x01a3de1d: movb $0x0,0x1104800(%esi);\r0x01a3de24: lock addl $0x0,(%esp);\r有volatile变量修饰的共享变量进行写操作的时候会多第二行汇编代码，通过查IA-32架构软件开发者手册可知，lock前缀的指令在多核处理器下会引发了两件事情。\n将当前处理器缓存行的数据会写回到系统内存。 这个写回内存的操作会引起在其他CPU里缓存了该内存地址的数据无效。 处理器为了提高处理速度，不直接和内存进行通讯，而是先将系统内存的数据读到内部缓存（L1,L2或其他）后再进行操作，但操作完之后不知道何时会写到内存，如果对声明了Volatile变量进行写操作，JVM就会向处理器发送一条Lock前缀的指令，将这个变量所在缓存行的数据写回到系统内存。但是就算写回到内存，如果其他处理器缓存的值还是旧的，再执行计算操作就会有问题，所以在多处理器下，为了保证各个处理器的缓存是一致的，就会实现缓存一致性协议，每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器要对这个数据进行修改操作的时候，会强制重新从系统内存里把数据读到处理器缓存里。\n这两件事情在IA-32软件开发者架构手册的第三册的多处理器管理章节（第八章）中有详细阐述。\nLock前缀指令会引起处理器缓存回写到内存 。Lock前缀指令导致在执行指令期间，声言处理器的 LOCK# 信号。在多处理器环境中，LOCK# 信号确保在声言该信号期间，处理器可以独占使用任何共享内存。（因为它会锁住总线，导致其他CPU不能访问总线，不能访问总线就意味着不能访问系统内存），但是在最近的处理器里，LOCK＃信号一般不锁总线，而是锁缓存，毕竟锁总线开销比较大。在8.1.4章节有详细说明锁定操作对处理器缓存的影响，对于Intel486和Pentium处理器，在锁操作时，总是在总线上声言LOCK#信号。但在P6和最近的处理器中，如果访问的内存区域已经缓存在处理器内部，则不会声言LOCK#信号。相反地，它会锁定这块内存区域的缓存并回写到内存，并使用缓存一致性机制来确保修改的原子性，此操作被称为“缓存锁定”，缓存一致性机制会阻止同时修改被两个以上处理器缓存的内存区域数据 。\n一个处理器的缓存回写到内存会导致其他处理器的缓存无效 。IA-32处理器和Intel 64处理器使用MESI（修改，独占，共享，无效）控制协议去维护内部缓存和其他处理器缓存的一致性。在多核处理器系统中进行操作的时候，IA-32 和Intel 64处理器能嗅探其他处理器访问系统内存和它们的内部缓存。它们使用嗅探技术保证它的内部缓存，系统内存和其他处理器的缓存的数据在总线上保持一致。例如在Pentium和P6 family处理器中，如果通过嗅探一个处理器来检测其他处理器打算写内存地址，而这个地址当前处理共享状态，那么正在嗅探的处理器将无效它的缓存行，在下次访问相同内存地址时，强制执行缓存行填充。\n1.2 内存屏障防止指令重排（解决指令重排对volatile修饰的变量不会产生影响） 写操作时，通过在写操作指令后加入一条store屏障指令，让本地内存中变量的值能够刷新到主内存中\n读操作时，通过在读操作前加入一条load屏障指令，及时读取到变量在主内存的值\nPS: 内存屏障（Memory Barrier）是一种CPU指令，用于控制特定条件下的重排序和内存可见性问题。Java编译器也会根据内存屏障的规则禁止重排序\n==volatile的底层实现是通过插入内存屏障==，但是对于编译器来说，发现一个最优布置来最小化插入内存屏障的总数几乎是不可能的，所以，JMM采用了保守策略。如下：\nStoreStore屏障可以保证在volatile写之前，其前面的所有普通写操作都已经刷新到主内存中。\nStoreLoad屏障的作用是避免volatile写与后面可能有的volatile读/写操作重排序。\nLoadLoad屏障用来禁止处理器把上面的volatile读与下面的普通读重排序。\nLoadStore屏障用来禁止处理器把上面的volatile读与下面的普通写重排序。\n2 原子性的问题 虽然Volatile 关键字可以让变量在多个线程之间可见，但是Volatile不能保证对修饰变量的操作是原子性的。\npublic class Demo3Volatile {\rpublic static void main(String[] args) throws InterruptedException {\rVolatileDemo demo = new VolatileDemo();\rfor (int i = 0; i \u0026lt; 5; i++) {\rThread t = new Thread(demo);\rt.start();\r}\rThread.sleep(1000);\rSystem.out.println(demo.count);\r}\rstatic class VolatileDemo implements Runnable {\rpublic volatile int count;\r//public volatile AtomicInteger count = new AtomicInteger(0);\rpublic void run() {\raddCount();\r}\rpublic void addCount() {\rfor (int i = 0; i \u0026lt; 10000; i++) {\rcount++;\r}\r}\r}\r}\r以上出现原子性问题的原因是count++并不是原子性操作。\ncount = 5 开始，流程分析：\n线程1读取count的值为5 线程2读取count的值为5 线程2加1操作 线程2最新count的值为6 线程2写入值到主内存的最新值为6 这个时候，线程1的count为5，线程2的count为6\n如果切换到线程1执行，那么线程1得到的结果是6，写入到主内存的值还是6\n现在的情况是对count进行了两次加1操作，但是主内存实际上只是加1一次\n解决方案：\n使用synchronized 使用ReentrantLock（可重入锁） 使用AtomicInteger（原子操作） 使用synchronized\npublic synchronized void addCount() {\rfor (int i = 0; i \u0026lt; 10000; i++) {\rcount++;\r}\r}\r使用ReentrantLock（可重入锁）\n//可重入锁\rprivate Lock lock = new ReentrantLock();\rpublic void addCount() {\rfor (int i = 0; i \u0026lt; 10000; i++) {\rlock.lock();\rcount++;\rlock.unlock();\r}\r}\r使用AtomicInteger（原子操作）\npublic static AtomicInteger count = new AtomicInteger(0);\rpublic void addCount() {\rfor (int i = 0; i \u0026lt; 10000; i++) {\r//count++;\rcount.incrementAndGet();\r}\r}\r3 Volatile 适合使用场景（能够解决可见性，和一定程度的有序性，但是不能保证原子性） a）对变量的写入操作不依赖其当前值\n​ 不满足：number++、count=count*5等\n​ 满足：boolean变量、直接赋值的变量等\nb）该变量没有包含在具有其他变量的不变式中\n​\t不满足：不变式 low\u0026lt;up\n总结：对变量的操作本身是原子性的操作，就可以使用volatile,如果不是原子操作使用了volitale也不能保证线程安全\n扩展：\nJava中的原子操作包括：\r1）除long和double之外的基本类型的赋值操作\r2）所有引用reference的赋值操作\r3）java.util.concurrent.Atomic.* 包中所有类的一切操作。\r4）cas操作是原子操作。\r但是java对long和double的赋值操作是非原子操作！！long和double占用的字节数都是8，也就是64bits。在32位操作系统上对64位的数据的读写要分两步完成，每一步取32位数据。这样对double和long的赋值操作就会有问题：如果有两个线程同时写一个变量内存，一个进程写低32位，而另一个写高32位，这样将导致获取的64位数据是失效的数据。因此需要使用volatile关键字来防止此类现象。volatile本身不保证获取和设置操作的原子性，仅仅保持修改的可见性。但是java的内存模型保证声明为volatile的long和double变量的get和set操作是原子的。\r7.4 小结 8. synchronized 8.1 目标 理解synchronized实现可见性的过程 理解同步原理 理解jvm是如何进行锁优化的 8.2 路径 解决可见性问题 同步原理 锁优化 8.3 讲解 ​\tsynchronized可以保证方法或者代码块在运行时，同一时刻只有一个线程执行synchronized声明的代码块。还可以保证共享变量的内存可见性。同一时刻只有一个线程执行，这部分代码块的重排序也不会影响其执行结果。也就是说使用了synchronized可以保证并发的原子性，可见性，有序性。\n8.3.1 解决可见性问题 JMM关于synchronized的两条规定：\n==线程加锁时（进入同步代码块时）：将清空本地内存中共享变量的值，从而使用共享变量时需要从主内存中重新读取最新的值（加锁与解锁是同一把锁==）\n线程解锁前（退出同步代码块时）：必须把自己工作内存中共享变量的最新值刷新到主内存中\n做如下修改，在死循环中添加同步代码块\nwhile (flag) {\rsynchronized (this) {\r}\r}\rsynchronized实现可见性的过程\n获得互斥锁（同步获取锁） 清空本地内存 执行代码 将更改后的共享变量的值刷新到主内存 释放互斥锁 重新循环，从主内存拷贝变量的最新副本到本地内存 8.3.2 同步原理 synchronized的同步可以解决原子性、可见性和有序性的问题，那是如何实现同步的呢？\nJava中每一个对象都可以作为锁，这是synchronized实现同步的基础：\n普通同步方法，锁是当前实例对象this 静态同步方法，锁是当前类的class对象 同步方法块，锁是括号里面的对象 当一个线程访问同步代码块时，它首先是需要得到锁才能执行同步代码，当退出或者抛出异常时必须要释放锁。\nsynchronized的同步操作主要是monitorenter和monitorexit这两个jvm指令实现的，先写一段简单的代码：\npublic class Demo2Synchronized {\rpublic void test2() {\rsynchronized (this) {\r}\r}\r}\r在cmd命令行执行javac编译和javap -c Java 字节码的指令\njavac Demo2Synchronized.java\rjavap -c Demo2Synchronized.class\r从结果可以看出，同步代码块是使用monitorenter和monitorexit这两个jvm指令实现的：\n​\t注意，本小节是解释synchronized性能低效的原因，只要能理解synchronized同步过程其实还需要做很多事，这些逻辑的执行都需要占用资源，从而导致性能较低。这部分分析过于深入JMM底层原理，不适合初级甚至中级程序员学习。synchronized是通过访问锁对象的monitor和mark word实现同步的\nMark Word\nHotspot虚拟机的对象头主要包括两部分数据：Mark Word（标记字段）、Klass Pointer（类型指针）。其中Klass Point是是对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例，Mark Word用于存储对象自身的运行时数据，它是synchronized实现轻量级锁和偏向锁的关键。\rMark Word用于存储对象自身的运行时数据，如**哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程 ID、偏向时间戳**等等。Java对象头一般占有两个机器码（在32位虚拟机中，1个机器码等于4字节，也就是32bit），但是如果对象是数组类型，则需要三个机器码，因为JVM虚拟机可以通过Java对象的元数据信息确定Java对象的大小，但是无法从数组的元数据来确认数组的大小，所以用一块来记录数组长度。下图是Java对象头的存储结构（32位虚拟机）：\r对象头信息是与对象自身定义的数据无关的额外存储成本，但是考虑到虚拟机的空间效率，Mark Word被设计成一个非固定的数据结构以便在极小的空间内存存储尽量多的数据，它会根据对象的状态复用自己的存储空间，也就是说，Mark Word会随着程序的运行发生变化，变化状态如下（32位虚拟机）：\rmonitor\n什么是Monitor？我们可以把它理解为一个同步工具，也可以描述为一种同步机制，它通常被描述为一个对象。与一切皆对象一样，所有的Java对象是天生的Monitor，每一个Java对象都有成为Monitor的潜质，因为在Java的设计中 ，每一个Java对象都带了一把看不见的锁，它叫做内部锁或者Monitor锁。\nMonitor 是线程私有的数据结构，每一个线程都有一个可用monitor record列表，同时还有一个全局的可用列表。每一个被锁住的对象都会和一个monitor关联（对象头的MarkWord中的LockWord指向monitor的起始地址），同时monitor中有一个Owner字段存放拥有该锁的线程的唯一标识，表示该锁被这个线程占用。其结构如下：\nOwner：初始时为NULL表示当前没有任何线程拥有该monitor record，当线程成功拥有该锁后保存线程唯一标识，当锁被释放时又设置为NULL； EntryQ:关联一个系统互斥锁（semaphore），阻塞所有试图锁住monitor record失败的线程。 RcThis:表示blocked或waiting在该monitor record上的所有线程的个数。 Nest:用来实现重入锁的计数。 HashCode:保存从对象头拷贝过来的HashCode值（可能还包含GC age）。 Candidate:用来避免不必要的阻塞或等待线程唤醒，因为每一次只有一个线程能够成功拥有锁，如果每次前一个释放锁的线程唤醒所有正在阻塞或等待的线程，会引起不必要的上下文切换（从阻塞到就绪然后因为竞争锁失败又被阻塞）从而导致性能严重下降。Candidate只有两种可能的值0表示没有需要唤醒的线程1表示要唤醒一个继任线程来竞争锁。 8.3.3 synchronized和volatile比较 a）volatile不需要加锁，比synchronized更轻便，不会阻塞线程\nb）synchronized既能保证可见性，又能保证原子性，而volatile只能保证可见性，无法保证原子性\n​\t与锁相比，Volatile 变量是一种非常简单但同时又非常脆弱的同步机制，它在某些情况下将提供优于锁的性能和伸缩性。如果严格遵循 volatile 的使用条件（对变量的操作是原子性的 ） 在某些情况下可以使用 volatile 代替 synchronized 来优化代码提升效率。\n8.3.4 锁优化 ​\tsynchronized是重量级锁，效率不高。但在jdk 1.6中对synchronize的实现进行了各种优化，使得它显得不是那么重了。jdk1.6对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。\n​\t锁主要存在四中状态，依次是：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，他们会随着竞争的激烈而逐渐升级。\n==注意锁可以升级不可降级，这种策略是为了提高获得锁和释放锁的效率。==\n1 偏向锁 ​\t步骤如下：\n一个线程去争用时，如果没有其他线程争用，则会尝试CAS去修改mark word中一个标记为偏向(mark word单独有一个bit表示是否可偏向，记录锁的位置依然为01)，这个CAS动作同时会修改mark word部分bit以保留线程的ID值。 当线程不断发生重入时，只需要判定头部的线程ID是否是当前线程，若是，则无需任何操作。 ==如果同一个对象存在另一个线程发起了访问请求==，则首先会判定该对象是否已经被锁定了。如果已经被锁定，则会将锁修改为轻量级锁(00),也就是锁粒度上升了；而如果没有锁定，则会将对象的==是否可偏向的位置设置为不可偏向==。 ​ 偏向锁是Java 6之后加入的新锁，它是一种针对加锁操作的优化手段，经过研究发现，在大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，因此为了减少同一线程获取锁(会涉及到一些CAS操作,耗时)的代价而引入偏向锁。偏向锁的核心思想是，如果一个线程获得了锁，那么锁就进入偏向模式，此时Mark Word 的结构也变为偏向锁结构，当这个线程再次请求锁时，无需再做任何同步操作，即获取锁的过程，这样就省去了大量有关锁申请的操作，从而也就提供程序的性能。所以，对于没有锁竞争的场合，偏向锁有很好的优化效果，毕竟极有可能连续多次是同一个线程申请相同的锁。但是对于锁竞争比较激烈的场合，偏向锁就失效了，因为这样场合极有可能每次申请锁的线程都是不相同的，因此这种场合下不应该使用偏向锁，否则会得不偿失，需要注意的是，偏向锁失败后，并不会立即膨胀为重量级锁，而是先升级为轻量级锁。下面我们接着了解轻量级锁。\n2 轻量级锁 ​\tsynchronized会在对象的头部打标记，这个加锁的动作是必须要做的，悲观锁通常还会做许多其他的指令动作，轻量级锁希望通过CAS实现，它认为通过CAS尝试修改对象头部的mark区域的内容就可以达到目的，由于mark区域的宽度通常是4~8字节，也就是相当于一个int或者long的宽度，适合于CAS操作。\n轻量级锁通常会做一下4个步骤：\n==在栈中分配一块空间用来做一份对象头部mark word的拷贝==，在mark word中将对象锁的二进制位设置为“未锁定”(在32位的JVM中通常有2位用于存储锁标记，未锁定的标记为01)，这个动作是==方便等到释放锁的时候将这份数据拷贝到对象头部。== 通过CAS尝试将头部的二进制位修改为“线程私有栈中对mark区域拷贝存放的地址”，如果成功，则会将最后2位设置为00，代表已经被轻量级锁锁住了。 如果没有成功，则判定对象头部是否已经指向了当前线程所在的栈当中，如果成立则代表当前线程已经是拥有着，可以继续执行。 如果不是拥有着，则说明有多个线程在争用，那么此时会将==锁升级为悲观锁，线程进入BLOCKED状态。== ​ JVM发现在轻量级锁里面多次“重入”和“释放”时，需要做的判断和拷贝动作还是很多，而在某些应用程序中，锁就是被某一个线程一直使用，为了进一步减小锁的开销，JVM中出现了偏向锁，偏向锁希望记录的是一个线程ID，它比轻量级锁更加轻量，当再次重入判定时，首先判定对象头部的线程ID是不是当前线程，若是则表示当前线程已经是对象锁的OWNER，无须做其他任何动作。\n3 自旋锁 ​\t轻量级锁失败后，虚拟机为了避免线程真实地在操作系统层面挂起，还会进行一项称为自旋锁的优化手段。这是基于 在大多数情况下 对象锁的锁状态只会持续很短一段时间，为了这一段很短的时间频繁地阻塞和唤醒线程是非常不值得的。所以引入自旋锁。\n​\t所谓==自旋锁，就是让该线程等待一段时间，不会被立即挂起，看持有锁的线程是否会很快释放锁。怎么等待呢？执行一段无意义的循环即可（自旋）。==\n​\t==自旋等待不能替代阻塞，虽然它可以避免线程切换带来的开销==，但是它占用了处理器的时间。==如果持有锁的线程很快就释放了锁，那么自旋的效率就非常好，反之，自旋的线程就会白白消耗掉处理的资源，它不会做任何有意义的工作，典型的占着茅坑不拉屎，这样反而会带来性能上的浪费==。所以说，自旋等待的时间（自旋的次数）必须要有一个限度，如果自旋超过了定义的时间仍然没有获取到锁，则应该被挂起。\n​\t自旋锁在JDK 1.4.2中引入，默认关闭，但是可以使用-XX:+UseSpinning开开启，在JDK1.6中默认开启。同时自旋的默认次数为10次，可以通过参数-XX:PreBlockSpin来调整；\n​\t如果通过参数-XX:preBlockSpin来调整自旋锁的自旋次数，会带来诸多不便。假如我将参数调整为10，但是系统很多线程都是等你刚刚退出的时候就释放了锁（假如你多自旋一两次就可以获取锁），你是不是很尴尬。于是JDK1.6引入自适应的自旋锁，让虚拟机会变得越来越聪明。\n4 适应自旋锁 ​\tJDK 1.6引入了更加聪明的自旋锁，即自适应自旋锁。所谓自适应就意味着自旋的次数不再是固定的，==它是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。它怎么做呢？线程如果自旋成功了，那么下次自旋的次数会更加多，因为虚拟机认为既然上次成功了，那么此次自旋也很有可能会再次成功，那么它就会允许自旋等待持续的次数更多==。反之，如果对于某个锁，很少有自旋能够成功的，那么在以后要或者这个锁的时候自旋的次数会减少甚至省略掉自旋过程，以免浪费处理器资源。\n有了自适应自旋锁，随着程序运行和性能监控信息的不断完善，虚拟机对程序锁的状况预测会越来越准确，虚拟机会变得越来越聪明。\n5 锁消除 ​\t为了保证数据的完整性，我们在进行操作时需要对这部分操作进行同步控制，但是在有些情况下，==JVM检测到不可能存在共享数据竞争，这是JVM会对这些同步锁进行锁消除。锁消除的依据是逃逸分析的数据支持。==\n​\t如果不存在竞争，为什么还需要加锁呢？所以锁消除可以节省毫无意义的请求锁的时间。==变量是否逃逸，对于虚拟机来说需要使用数据流分析来确定==，但是对于我们程序员来说这还不清楚么？我们会在明明知道不存在数据竞争的代码块前加上同步吗？但是有时候程序并不是我们所想的那样？我们虽然没有显示使用锁，但是我们在使用一些JDK的内置API时，如StringBuffer、Vector、HashTable等，这个时候会存在隐形的加锁操作。比如StringBuffer的append()方法，Vector的add()方法：\npublic void test(){\rVector\u0026lt;Integer\u0026gt; vector = new Vector\u0026lt;Integer\u0026gt;();\rfor(int i = 0 ; i \u0026lt; 10 ; i++){\rvector.add(i);\r}\rSystem.out.println(vector);\r}\r​\t在运行这段代码时，JVM可以明显检测到变量vector没有逃逸出方法vectorTest()之外，所以JVM可以大胆地将vector内部的加锁操作消除。\n6 锁粗化 ​\t在使用同步锁的时候，需要让同步块的作用范围尽可能小，仅在共享数据的实际作用域中才进行同步，这样做的目的是为了使需要同步的操作量尽可能缩小，如果存在锁竞争，那么等待锁的线程也能尽快拿到锁。\n​\t在大多数的情况下，上述观点是正确的。但是如果一系列的连续加锁解锁操作，可能会导致不必要的性能损耗，所以引入锁粗化的概念。\n​\t锁粗话概念比较好理解，就是将多个连续的加锁、解锁操作连接在一起，扩展成一个范围更大的锁。如上面实例：vector每次add的时候都需要加锁操作，JVM检测到对同一个对象（vector）连续加锁、解锁操作，会合并一个更大范围的加锁、解锁操作，即加锁解锁操作会移到for循环之外。\nVector\u0026lt;Integer\u0026gt; vector = new Vector\u0026lt;Integer\u0026gt;();\rpublic void test(){\r//加锁\rfor(int i = 0 ; i \u0026lt; 10 ; i++){\rvector.add(i);//1.加锁 2.解锁\r}\r//解锁\rSystem.out.println(vector);\r}\r7 重量锁 ​\tjava的线程是映射到操作系统原生线程之上的，如果要阻塞或唤醒一个线程就需要操作系统介入，需要在用户态与核心态之间切换，这种切换会消耗大量的系统资源，因为用户态与内核态都有各自专用的内存空间，专用的寄存器等，用户态切换至内核态需要传递给许多变量、参数给内核，内核也需要保护好用户态在切换时的一些寄存器值、变量等，以便内核态调用结束后切换回用户态继续工作。\n​ 因为需要限制不同的程序之间的访问能力, 防止他们获取别的程序的内存数据, 或者获取外围设备的数据, 并发送到网络, CPU划分出两个权限等级 :用户态 和 内核态\n内核态：CPU可以访问内存所有数据, 包括外围设备, 例如硬盘, 网卡. CPU也可以将自己从一个程序切换到另一个程序 用户态：只能受限的访问内存, 且不允许访问外围设备. 占用CPU的能力被剥夺, CPU资源可以被其他程序获取 ​ 所有用户程序都是运行在用户态的, 但是有时候程序确实需要做一些内核态的事情, 例如从硬盘读取数据, 或者从键盘获取输入等.，而唯一可以做这些事情的就是操作系统, 所以此时程序就需要先操作系统请求以程序的名义来执行这些操作（比如java的I/O操作底层都是通过native方法来调用操作系统）。这时需要一个这样的机制: 用户态程序切换到内核态, 但是不能控制在内核态中执行的指令。这种机制叫系统调用, 在CPU中的实现称之为陷阱指令(Trap Instruction)。\n总结 线程和进程区别\n进程独享内存空间，线程之间是共享堆内存空间，独享栈内存空间 进程是程序的一次运行，一个进程可能包含很多线程，线程是一次任务执行的最小单元。 多线程创建\n继承Thread类，重写run方法 实现runable接口，实现run方法 匿名内部类实现runable接口 多线程创建方式注意的点\n实现runable接口可以让多个线程同时执行一个任务，继承Thread类方式不行 线程创建后要通过start方法运行，通过本地native方法调用底层c同名方法创建线程执行，不要通过run方法，因为run方法相当于在主线程运行类的run方法而已，不会创建线程执行 线程安全（多线程执行同一段代码，如果发生和预期结果不一致的情况就是线程不安全的）\n由于不满足原子性，有序性，可见性导致线程安全问题 解决方案：加锁解决 死锁：线程之间互相等待对方释放锁产生死锁。解决方案：不要在同步中嵌套同步 线程同步方式Synchorized（Synchorized都是对对象加锁）\n同步代码块加锁 sync\u0026hellip;(obj)-\u0026gt;对obj对象加锁 同步方法 sync fun01-\u0026gt;对this对象加锁 静态同步方法 static sync fun02-\u0026gt;对当前类的class加锁 线程状态\nnew（新建线程未执行） runable（可运行）：由ready（就绪）和running（运行中）两个状态组成,状态的切换是由cpu调度切换的，cpu调度到该线程，就是running，没调度到就是ready waiting：由当前获取到锁的线程，锁对象.wait()方法引起的，只能由同一个对象的notify（）/nodifyAll()方法唤醒 timed_waiting: sleep（时间），wait（时间），到达时间后自动唤醒 blocked：线程获取不到锁就进入阻塞状态 TERMINATED：线程终止 wait和sleep区别\nwait是属于Object类的，sleep是属于Thread类的 wait会让持有锁的线程释放锁，并进入waiting状态，sleep是不会释放锁的，必须有等待时间，时间过后继续执行 线程停止\n通过自定义标识：缺点是线程sleep情况下不能被立刻停止 通过线程自带标识： 当前线程正常执行：Thread.currentThread().isInterrupt()判断为true退出 当前线程sleep：会抛出异常，捕获后退出 线程优先级\n从1-10，表示的是cpu调度到该线程的概率，值越大优先级越高，越有可能被调度到，默认值是5，在宏观角度来看就是占用cpu时间片的多少 join：在线程b运行的时候执行线程a.join,a先执行执行完了执行b yield：将当前运行线程running-\u0026gt;ready，但是可能没效果，该线程可能被再次调度 保证线程安全的三个特性\n原子性：线程执行一段代码的过程中不能被其他的线程干扰对结果产生影响。 可见性：线程修改了变量之后能够立刻被其他线程看见 有序性：由jvm指令重排序导致的，jvm指令重排能够保证在单线程执行下结果一致，多线程不能保证。 jvm内存结构：jvm运行时的区域划分\njava对象模型：对象在jvm内存中的存储\n内存模型：jmm不是真实存在的是虚拟的，它的语法集映射在java语言中就是volidate，synchronized\nsynchronized\n保证原子性：因为被synchronized包裹的代码，只能被一个线程执行，所以不会被其他线程干扰 保证可见性：在加锁时，会清空本地内存变量，解锁的时候会把修改的本地内存变量刷到主内存里面去 保证有序性：因为被synchronized包裹的代码，只能被一个线程执行,jvm指令重排的结果对单线程没有影响，就算发生了指令重排对synchronized包裹的代码也不会有影响 加锁：对象的moniterEnter和moniterExit加锁和解锁 锁优化\n自旋锁：循环 适应自旋锁：循环，成功，给锁多循环几次的机会，失败则减少机会，如果多次失败就直接进入重量级锁 锁消除：如果jvm检测到锁没有竞争就把锁消除了 锁粗化：把一个个小的锁换成一个大锁 偏向锁：在markWord中保存threadid，发生竞争就升级 轻量级锁：通过cas实现 重量级锁：加锁解锁会导致线程从用户态到核心态的切换，消耗比较大的资源。 i++为什么不是线程安全的\n把变量读到cpu缓存 操作缓存中的值++ 写回主内存 由于上述三个步骤不是原子性的，所以会导致线程安全问题 ","date":"2020-06-23T10:53:40+08:00","permalink":"https://mikeLing-qx.github.io/p/juc_01/","title":"JUC_01"},{"content":"1. 简介 Spring 是一个轻量级的 Java 开发框架。Spring 的核心是控制反转(IOC)和面向切面编程(AOP)。\nSpring 主要有如下优点：\n1.解耦 2.支持面向切面编程 3.便于集成其他框架 2. IOC 控制反转 ​\tIOC，全称 Inversion of Control，意思是控制反转。它是 Spring 框架中的一种思想。\n​\t控制反转就是将对象的控制权从程序中的代码转移到了 Spring 的工厂，通过 Spring 的工厂完成对象的创建以及赋值。\n​\t之前是我们自己 new 对象、给对象中的成员变量赋值。现在是让 Spring 来帮助我们创建对象、给成员变量赋值。\n3. Spring核心内容描述 配置文件 ​\tSpring 的配置文件可以放到项目中的任意一个地方，也可以随意命名，但是建\t议使用：applicationContext.xml。\n​\t你可以将这个配置文件看成一个装有一堆 bean 标签的容器\nbean 标签 Spring 工厂创建的对象，叫做 bean，所以一个 bean 标签代表一个对象。\n\u0026lt;bean id=\u0026quot;userService\u0026quot; class=\u0026quot;com.xxl.service.impl.UserServiceImpl\u0026quot;/\u0026gt;\rbean 标签中必须要有 class 属性，它的值是一个类的全限定名（包名+类名）除了 class 属性，bean 标签还可以设置 id 、name 、scope属性\nname: 相当于这个 bean 的别名，可以配置多个\n\u0026lt;bean id=\u0026quot;user\u0026quot; name=\u0026quot;aa,bb,cc\u0026quot; class=\u0026quot;com.xxl.model.User\u0026quot;/\u0026gt;\rscope: 可以控制简单对象的创建次数\nsingleton: 每次只会创建唯一⼀个简单对象，默认值 prototype 每⼀次都会创建新的对象 \u0026lt;bean id=\u0026quot;user\u0026quot; class=\u0026quot;com.xxl.model.User\u0026quot; scope=\u0026quot;singleton\u0026quot;/\u0026gt;\rApplicationContext ApplicationContext 是 Spring 的工厂，主要用来创建对象\nSpring 通过读取配置文件创建工厂。\n工厂常用的方法\n// 1、获取工厂\rApplicationContext act = new ClassPathXmlApplicationContext(\u0026quot;/applicationContext.xml\u0026quot;);\r(1). 根据id 获取对象\nUserService userService = (UserService)act.getBean(\u0026quot;userService\u0026quot;);\r(2). 根据id 和类名获取对象\nUserService userService = (UserService)act.getBean(\u0026quot;userService\u0026quot;,UserService.class);\r(3). 只根据类名获取对象\nUserService userService = (UserService)act.getBean(UserService.class);\r(4). 获取配置文件中所有 bean 标签的 id 值\nString[] beanDefinitionNames = act.getBeanDefinitionNames();\rfor (String beanDefinitionName : beanDefinitionNames) {\rSystem.out.println(beanDefinitionName);\r}\r(5). 判断是否存在指定id 或者name 的bean\nact.containsBean(\u0026quot;userService\u0026quot;)\r(6). 判断是否存在指定id 的bean\nact.containsBeanDefinition(\u0026quot;userService\u0026quot;)\rSpring 是如何创建对象的? ​\t工厂和反射\nSpring 配置文件中 bean 标签的 id 和类的全限定名一一对应，所以 Spring 工厂的 getBean 方法其实就是先根据 bean 的 id ==获取该类的全限定名==，然后再利用==反射==根据类的全限定名创建对象并返回。\n4. DI 依赖注入 DI 全称 Dependency Injection，意思是依赖注入，它是 IOC 的具体实现。\n依赖就是说我需要你，比如 Service 层依赖 Dao 层，注入就是赋值。\nSpring 的依赖注入包含两种方式：\nset 注入：Spring 调用 Set 方法通过配置文件为成员变量赋值。\n1. set 注入 Set 注入就是在 property 标签中为属性赋值\n(1) 创建对象\npublic class User {\rprivate String name;\rprivate int age;\rpublic String getName() {\rreturn name;\r}\rpublic void setName(String name) {\rthis.name = name;\r}\rpublic int getAge() {\rreturn age;\r}\rpublic void setAge(int age) {\rthis.age = age;\r}\r}\r(2) 修改配置文件\n\u0026lt;bean id=\u0026quot;user\u0026quot; class=\u0026quot;com.xxl.model.User\u0026quot;\u0026gt;\r\u0026lt;property name=\u0026quot;name\u0026quot; value=\u0026quot;mike\u0026quot; /\u0026gt;\r\u0026lt;property name=\u0026quot;age\u0026quot; value=\u0026quot;18\u0026quot; /\u0026gt;\r\u0026lt;/bean\u0026gt;\r2. 构造注入 Spring 调用构造方法通过配置文件为成员变量赋值。\n为类添加构造方法\npublic class User {\rprivate String name;\rprivate int age;\rpublic User(String name, int age) {\rthis.name = name;\rthis.age = age;\r}\rpublic String getName() {\rreturn name;\r}\rpublic void setName(String name) {\rthis.name = name;\r}\rpublic int getAge() {\rreturn age;\r}\rpublic void setAge(int age) {\rthis.age = age;\r}\r}\rbean 标签中使用 constructor-arg\n\u0026lt;bean class=\u0026quot;com.xxl.model.User\u0026quot;\u0026gt;\r\u0026lt;constructor-arg value=\u0026quot;张三\u0026quot;/\u0026gt;\r\u0026lt;constructor-arg value=\u0026quot;18\u0026quot;/\u0026gt;\r\u0026lt;/bean\u0026gt;\r5. Bean 的生命周期 1. 创建阶段 singleton 模式, 创建 Spring 工厂的同时创建所有单例对象 public class User {\rString name;\rint age;\rpublic User() {\rSystem.out.println(\u0026quot;调用User的构造方法\u0026quot;);\r}\rpublic String getName() {\rreturn name;\r}\rpublic void setName(String name) {\rthis.name = name;\r}\rpublic int getAge() {\rreturn age;\r}\rpublic void setAge(int age) {\rthis.age = age;\r}\r@Override\rpublic String toString() {\rreturn \u0026quot;User{\u0026quot; +\r\u0026quot;name='\u0026quot; + name + ''' +\r\u0026quot;, age=\u0026quot; + age +\r'}';\r}\r}\rspring 配置文件注册bean\n\u0026lt;bean id=\u0026quot;user\u0026quot; class=\u0026quot;com.xxl.model.User\u0026quot;\u0026gt;\r\u0026lt;property name=\u0026quot;name\u0026quot; value=\u0026quot;知否君\u0026quot;/\u0026gt;\r\u0026lt;property name=\u0026quot;age\u0026quot; value=\u0026quot;23\u0026quot;/\u0026gt;\r\u0026lt;/bean\u0026gt;\r测试\n@Test\rpublic void testSpring(){\rApplicationContext act = new ClassPathXmlApplicationContext(\u0026quot;/applicationContext.xml\u0026quot;);\r}\r获取的时候才创建, 只需要在 bean 标签上面添加如下属性 \u0026mdash;\u0026gt; lazy-init=\u0026ldquo;true\u0026rdquo;\n\u0026lt;bean id=\u0026quot;user\u0026quot; class=\u0026quot;com.xxl.model.User\u0026quot; lazy-init=\u0026quot;true\u0026quot;\u0026gt;\r\u0026lt;property name=\u0026quot;name\u0026quot; value=\u0026quot;知否君\u0026quot;/\u0026gt;\r\u0026lt;property name=\u0026quot;age\u0026quot; value=\u0026quot;23\u0026quot;/\u0026gt;\r\u0026lt;/bean\u0026gt;\r2. 初始化阶段 ==初始化方法修改了注入的值，所以初始化方法一定在注入之后执行==\n​\tspring 中 bean 的初始化操作指的是在==创建对象的时候完成一些附加的功能==。bean 的初始化操作有两种实现方式：\n1. 实现 InitializingBean 接口 public class 类名 implements InitializingBean {\rpublic void afterPropertiesSet(){\r// 初始化方法操作\r}\r}\r例如\npublic class User implements InitializingBean {\rString name;\rint age;\rpublic String getName() {\rreturn name;\r}\rpublic void setName(String name) {\rthis.name = name;\r}\rpublic int getAge() {\rreturn age;\r}\rpublic void setAge(int age) {\rthis.age = age;\r}\r@Override\rpublic String toString() {\rreturn \u0026quot;User{\u0026quot; +\r\u0026quot;name='\u0026quot; + name + ''' +\r\u0026quot;, age=\u0026quot; + age +\r'}';\r}\r// 初始化操作\r@Override\rpublic void afterPropertiesSet(){\rthis.name = \u0026quot;张无忌\u0026quot;;\rthis.age = 30;\r}\r}\r@Test\rpublic void testSpring(){\rApplicationContext act = new ClassPathXmlApplicationContext(\u0026quot;/applicationContext.xml\u0026quot;);\rObject user = act.getBean(\u0026quot;user\u0026quot;);\rSystem.out.println(user);\r}\r2. 通过创建普通方法完成初始化 User 类中创建一个方法\n// 初始化方法\rpublic void initMethod() {\rthis.name = \u0026quot;张无忌\u0026quot;;\r}\r在配置文件中配置 init-method 属性\n\u0026lt;bean id=\u0026quot;user\u0026quot; class=\u0026quot;com.xxl.model.User\u0026quot; init-method=\u0026quot;initMethod\u0026quot; \u0026gt;\r\u0026lt;property name=\u0026quot;name\u0026quot; value=\u0026quot;知否君\u0026quot;/\u0026gt;\r\u0026lt;property name=\u0026quot;age\u0026quot; value=\u0026quot;23\u0026quot;/\u0026gt;\r\u0026lt;/bean\u0026gt;\r测试\n@Test\rpublic void testSpring(){\rApplicationContext act = new ClassPathXmlApplicationContext(\u0026quot;/applicationContext.xml\u0026quot;);\rObject user = act.getBean(\u0026quot;user\u0026quot;);\rSystem.out.println(user);\r}\r3. 销毁阶段 Spring 销毁对象前，会调用对象的销毁方法，完成销毁操作。\nSpring 什么时候销毁所创建的对象？当 Spring 工厂关闭时，Spring 工厂会调用我们自定义的销毁方法。\n两种方式:\n1. DisposableBean接口 public class 类名 implements DisposableBean {\r// 销毁操作\r@Override\rpublic void destroy(){\r// 销毁操作业务\r}\r}\r2. 创建普通方法 // 销毁方法\rpublic void destroyMethod() {\r// 销毁操作业务\r}\r配置文件中配置 destroy-method 属性\n\u0026lt;bean id=\u0026quot;user\u0026quot; class=\u0026quot;com.xxl.model.User\u0026quot; destroy-method=\u0026quot;destroyMethod\u0026quot;\u0026gt;\r\u0026lt;property name=\u0026quot;name\u0026quot; value=\u0026quot;知否君\u0026quot;/\u0026gt;\r\u0026lt;property name=\u0026quot;age\u0026quot; value=\u0026quot;23\u0026quot;/\u0026gt;\r\u0026lt;/bean\u0026gt;\r6. Bean 的后置处理 后置处理的流程\n1.实现 BeanPostProcessor 接口 public class BeanProcessor implements BeanPostProcessor {\r@Override\rpublic Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException {\rSystem.out.println(\u0026quot;后置bean：before 方法\u0026quot;);\rreturn bean;\r}\r@Override\rpublic Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException {\rSystem.out.println(\u0026quot;后置bean：after 方法\u0026quot;);\rif (bean instanceof User) {\rUser user = (User) bean;\ruser.setName(\u0026quot;亚里士多德\u0026quot;);\rreturn user;\r}\rreturn bean;\r}\r}\r2. 配置文件添加bean \u0026lt;bean id=\u0026quot;beanProcessor\u0026quot; class=\u0026quot;com.xxl.config.BeanProcessor\u0026quot;/\u0026gt;\r7. Spring Aop Spring 的动态代理，通过代理类为原始类增加一些 额外功能\r​\tAOP思想的实现一般都是基于代理模式，在JAVA中一般采用JDK动态代理模式，但是我们都知道，JDK动态代理模式只能代理接口，如果要代理类那么就不行了。因此，Spring AOP 会这样子来进行切换，==因为Spring AOP 同时支持 CGLIB、ASPECTJ、JDK动态代理，当你的真实对象有实现接口时，Spring AOP会默认采用JDK动态代理，否则采用cglib代理==。\n如果目标对象的实现类实现了接口，Spring AOP 将会采用 JDK 动态代理来生成 AOP 代理类； 如果目标对象的实现类没有实现接口，Spring AOP 将会采用 CGLIB 来生成 AOP 代理类——不过这个选择过程对开发者完全透明、开发者也无需关心。 1. Aop基本概念 1. advice 通知 Before 在方法被调用之前调用 After 在方法完成后调用通知，无论方法是否执行成功 After-returning ==在方法成功执行之后调用通知== After-throwing 在方法抛出异常后调用通知 Around 方法调用之==前后调用之后执行自定义的行为== 2. 切点 Pointcut ​\t切点在Spring AOP中确实是==对应系统中的方法==。但是这个方法是定义在切面中的方法，一般和通知一起使用，一起组成了切面。\n3. 连接点 JoinPoint 比如: 方法调用、方法执行、字段设置/获取、异常处理执行、类初始化、甚至是 for 循环中的某个点\n==程序执行过程中的任何时点都可以作为作为织入点, 而所有这些执行时点都是 Joint point==\nSpring AOP 目前==仅支持方法执行 (method execution)==\n4. 切面 Aspect 切面就是切点和通知的集合, 一般单独作为一个类; 通知和切点共同定义了关于切面的全部内容，它是什么时候，在何时和何处完成功能。\n5. 引入 introduction 引用允许我们向现有的类添加新的方法或者属性\n6. 织入 weaving 2. 使用 Spring 2.0 之后，Spring AOP有了两种配置方式。\nschema-based：Spring 2.0 以后使用 XML 的方式来配置，使用 命名空间 \u0026lt;aop /\u0026gt; @AspectJ 配置：Spring 2.0 以后提供的注解方式。这里虽然叫做 @AspectJ，但是这个和 AspectJ 其实没啥关系。 1. EnableAspectJAutoProxy 开启配置之后，所有在容器中，被@AspectJ注解的 bean 都会被 Spring 当做是 AOP 配置类，称为一个 Aspect。\n注意: AspectJ 注解只能作用于Spring Bean 上面\n2. 配置Pointcut 在Spring 中, Pointcut 是用来匹配Spring 容器中所有满足指定条件的bean的方法\n例如\nexecution：匹配方法名\n// 表示的是匹配名为testExecution的方法，*代表任意返回值，(..)表示零个或多个任意参数\r@Pointcut(\u0026quot;execution(* testExecution(..))\u0026quot;)\rpublic void anyTestMethod() {}\rwithin\n// \u0026quot;..\u0026quot; 代表包及其子包\r@Pointcut(\u0026quot;within(ric.study.demo.aop.svc..*)\u0026quot;)\rpublic void inSvcLayer() {}\rannotation：方法上具有特定的注解\n// 指定注解\r@Pointcut(\u0026quot;@annotation(ric.study.demo.aop.HaveAop)\u0026quot;)\rpublic void withAnnotation() {}\rbean(idOrNameOfBean)：匹配 bean 的名字\n// controller 层\r@Pointcut(\u0026quot;bean(testController)\u0026quot;)\rpublic void inControllerLayer() {}\r关于 Pointcut 的配置, 开发企业级应用，Spring 建议你使用 SystemArchitecture这种切面配置方式，即将一些公共的PointCut 配置全部写在这个一个类里面维护\n@Aspect\rpublic class SystemArchitecture {\r/**\r* A join point is in the web layer if the method is defined\r* in a type in the com.xyz.someapp.web package or any sub-package\r* under that.\r*/\r@Pointcut(\u0026quot;within(com.xyz.someapp.web..*)\u0026quot;)\rpublic void inWebLayer() {}\r@Pointcut(\u0026quot;execution(* com.xyz.someapp.dao.*.*(..))\u0026quot;)\rpublic void dataAccessOperation() {}\r}\r3. 配置 Advice 实际开发过程当中，Aspect 类应该遵守单一职责原则，不要把所有的Advice配置全部写在一个Aspect类里面。\n@Aspect\r@Component\rpublic class GlobalAopAdvice {\r@Before(\u0026quot;ric.study.demo.aop.SystemArchitecture.dataAccessOperation()\u0026quot;)\rpublic void doAccessCheck() {\r// ... 实现代码\r}\r// 实际使用过程当中 可以像这样把Advice 和 Pointcut 合在一起，直接在Advice上面定义切入点\r@Before(\u0026quot;execution(* ric.study.demo.dao.*.*(..))\u0026quot;)\rpublic void doAccessCheck() {\r// ... 实现代码\r}\r// 在方法\r@AfterReturning(\u0026quot;ric.study.demo.aop.SystemArchitecture.dataAccessOperation()\u0026quot;)\rpublic void doAccessCheck() {\r// ... 实现代码\r}\r// returnVal 就是相应方法的返回值\r@AfterReturning(\rpointcut=\u0026quot;ric.study.demo.aop.SystemArchitecture.dataAccessOperation()\u0026quot;,\rreturning=\u0026quot;returnVal\u0026quot;)\rpublic void doAccessCheck(Object returnVal) {\r// ... 实现代码\r}\r// 异常返回的时候\r@AfterThrowing(\u0026quot;ric.study.demo.aop.SystemArchitecture.dataAccessOperation()\u0026quot;)\rpublic void doRecoveryActions() {\r// ... 实现代码\r}\r// 注意理解它和 @AfterReturning 之间的区别，这里会拦截正常返回和异常的情况\r@After(\u0026quot;ric.study.demo.aop.SystemArchitecture.dataAccessOperation()\u0026quot;)\rpublic void doReleaseLock() {\r// 通常就像 finally 块一样使用，用来释放资源。\r// 无论正常返回还是异常退出，都会被拦截到\r}\r// 这种最灵活，既能做 @Before 的事情，也可以做 @AfterReturning 的事情\r@Around(\u0026quot;ric.study.demo.aop.SystemArchitecture.businessService()\u0026quot;)\rpublic Object doBasicProfiling(ProceedingJoinPoint pjp) throws Throwable {\r// target 方法执行前... 实现代码\rObject retVal = pjp.proceed();\r// target 方法执行后... 实现代码\rreturn retVal;\r}\r}\r@Before的时候，去获取方法的入参，比如进行一些日志的记录\n可以通过 org.aspectj.lang.JoinPoint来实现。上文中的ProceedingJoinPoint就是其子类\n@Before(\u0026quot;...\u0026quot;)\rpublic void logArgs(JoinPoint joinPoint) {\rSystem.out.println(\u0026quot;方法执行前，打印入参：\u0026quot; + Arrays.toString(joinPoint.getArgs()));\r}\r方法返回参数打印\n@AfterReturning( pointcut=\u0026quot;...\u0026quot;, returning=\u0026quot;returnVal\u0026quot;)\rpublic void logReturnVal(Object returnVal) {\rSystem.out.println(\u0026quot;方法执行后，打印返参：\u0026quot; + returnVal));\r}\r","date":"2020-06-16T19:21:43+08:00","permalink":"https://mikeLing-qx.github.io/p/spring%E5%9F%BA%E7%A1%80/","title":"Spring基础"},{"content":"1. 函数式接口 函数接口是==只有一个抽象方法的接口==，用作 Lambda 表达式的类型。使用@FunctionalInterface注解修饰的类，编译器会检测该类是否只有一个抽象方法或接口，否则，会报错。可以有多个默认方法，静态方法。\n1.1 常用的函数式接口 public class Test {\rpublic static void main(String[] args) {\rPredicate\u0026lt;Integer\u0026gt; predicate = x -\u0026gt; x \u0026gt; 185;\rStudent student = new Student(\u0026quot;9龙\u0026quot;, 23, 175);\rSystem.out.println(\r\u0026quot;9龙的身高高于185吗？：\u0026quot; + predicate.test(student.getStature()));\rConsumer\u0026lt;String\u0026gt; consumer = System.out::println;\rconsumer.accept(\u0026quot;命运由我不由天\u0026quot;);\rFunction\u0026lt;Student, String\u0026gt; function = Student::getName;\rString name = function.apply(student);\rSystem.out.println(name);\rSupplier\u0026lt;Integer\u0026gt; supplier = () -\u0026gt; Integer.valueOf(BigDecimal.TEN.toString());\rSystem.out.println(supplier.get());\rUnaryOperator\u0026lt;Boolean\u0026gt; unaryOperator = uglily -\u0026gt; !uglily;\rBoolean apply2 = unaryOperator.apply(true);\rSystem.out.println(apply2);\rBinaryOperator\u0026lt;Integer\u0026gt; operator = (x, y) -\u0026gt; x * y;\rInteger integer = operator.apply(2, 3);\rSystem.out.println(integer);\rtest(() -\u0026gt; \u0026quot;我是一个演示的函数式接口\u0026quot;);\r}\r/**\r* 演示自定义函数式接口使用\r*\r* @param worker\r*/\rpublic static void test(Worker worker) {\rString work = worker.work();\rSystem.out.println(work);\r}\rpublic interface Worker {\rString work();\r}\r}\r//9龙的身高高于185吗？：false\r//命运由我不由天\r//9龙\r//10\r//false\r//6\r//我是一个演示的函数式接口\rStudent::getName例子中这种编写lambda表达式的方式称为方法引用。格式为ClassNmae::methodName\r1.2 惰性求值, 及早求值 惰性求值：只描述Stream，操作的结果也是Stream，这样的操作称为惰性求值。惰性求值可以像建造者模式一样链式使用，最后再使用及早求值得到最终结果。\n及早求值：得到最终的结果而不是Stream，这样的操作称为及早求值。\n2. 常用流 2.0 创建流 单列集合: 集合对象.stream();\n数组: Arrays.stream(arr) 和 Stream.of(arr)\n双列集合: 转换成单列集合后再创建\n==peek 中间操作符, 可以用来调试==\n2.1 collect(Collectors.toList()) 将流转换为list。还有toSet()，toMap()等。及早求值。\npublic class TestCase {\rpublic static void main(String[] args) {\rList\u0026lt;Student\u0026gt; studentList = Stream.of(new Student(\u0026quot;路飞\u0026quot;, 22, 175),\rnew Student(\u0026quot;红发\u0026quot;, 40, 180),\rnew Student(\u0026quot;白胡子\u0026quot;, 50, 185)).collect(Collectors.toList());\rSystem.out.println(studentList);\r}\r}\r//输出结果\r//[Student{name='路飞', age=22, stature=175, specialities=null}, //Student{name='红发', age=40, stature=180, specialities=null}, //Student{name='白胡子', age=50, stature=185, specialities=null}]\r2.2 filter 起过滤筛选的作用。==符合条件(为true)才能保存在流当中, 内部就是Predicate接口==。惰性求值。\npublic class TestCase {\rpublic static void main(String[] args) {\rList\u0026lt;Student\u0026gt; students = new ArrayList\u0026lt;\u0026gt;(3);\rstudents.add(new Student(\u0026quot;路飞\u0026quot;, 22, 175));\rstudents.add(new Student(\u0026quot;红发\u0026quot;, 40, 180));\rstudents.add(new Student(\u0026quot;白胡子\u0026quot;, 50, 185));\rList\u0026lt;Student\u0026gt; list = students.stream()\r.filter(stu -\u0026gt; stu.getStature() \u0026lt; 180)\r.collect(Collectors.toList());\rSystem.out.println(list);\r}\r}\r//输出结果\r//[Student{name='路飞', age=22, stature=175, specialities=null}]\r2.3 map ==内部就是Function接口.== 惰性求值\npublic class TestCase {\rpublic static void main(String[] args) {\rList\u0026lt;Student\u0026gt; students = new ArrayList\u0026lt;\u0026gt;(3);\rstudents.add(new Student(\u0026quot;路飞\u0026quot;, 22, 175));\rstudents.add(new Student(\u0026quot;红发\u0026quot;, 40, 180));\rstudents.add(new Student(\u0026quot;白胡子\u0026quot;, 50, 185));\rList\u0026lt;String\u0026gt; names = students.stream().map(student -\u0026gt; student.getName())\r.collect(Collectors.toList());\rSystem.out.println(names);\r}\r}\r//输出结果\r//[路飞, 红发, 白胡子]\r2.4 flatMap ==将多个Stream 合并成一个 Stream==, 惰性求值\npublic class TestCase {\rpublic static void main(String[] args) {\rList\u0026lt;Student\u0026gt; students = new ArrayList\u0026lt;\u0026gt;(3);\rstudents.add(new Student(\u0026quot;路飞\u0026quot;, 22, 175));\rstudents.add(new Student(\u0026quot;红发\u0026quot;, 40, 180));\rstudents.add(new Student(\u0026quot;白胡子\u0026quot;, 50, 185));\rList\u0026lt;Student\u0026gt; studentList = Stream.of(students,\rasList(new Student(\u0026quot;艾斯\u0026quot;, 25, 183),\rnew Student(\u0026quot;雷利\u0026quot;, 48, 176)))\r.flatMap(students1 -\u0026gt; students1.stream()).collect(Collectors.toList());\rSystem.out.println(studentList);\r}\r}\r//输出结果\r//[Student{name='路飞', age=22, stature=175, specialities=null}, //Student{name='红发', age=40, stature=180, specialities=null}, //Student{name='白胡子', age=50, stature=185, specialities=null}, //Student{name='艾斯', age=25, stature=183, specialities=null},\r//Student{name='雷利', age=48, stature=176, specialities=null}]\r2.5 max min 集合中求最大最小值, 及早求值\npublic class TestCase {\rpublic static void main(String[] args) {\rList\u0026lt;Student\u0026gt; students = new ArrayList\u0026lt;\u0026gt;(3);\rstudents.add(new Student(\u0026quot;路飞\u0026quot;, 22, 175));\rstudents.add(new Student(\u0026quot;红发\u0026quot;, 40, 180));\rstudents.add(new Student(\u0026quot;白胡子\u0026quot;, 50, 185));\rOptional\u0026lt;Student\u0026gt; max = students.stream()\r.max(Comparator.comparing(stu -\u0026gt; stu.getAge()));\rOptional\u0026lt;Student\u0026gt; min = students.stream()\r.min(Comparator.comparing(stu -\u0026gt; stu.getAge()));\r//判断是否有值\rif (max.isPresent()) {\rSystem.out.println(max.get());\r}\rif (min.isPresent()) {\rSystem.out.println(min.get());\r}\r}\r}\r//输出结果\r//Student{name='白胡子', age=50, stature=185, specialities=null}\r//Student{name='路飞', age=22, stature=175, specialities=null}\rjmax, min 接受一个Comparator（例子中使用java8自带的静态函数，只需要传进需要比较值即可。）并且返回一个Optional对象\n2.6 count 一般结合filter 使用, 及早求值\npublic class TestCase {\rpublic static void main(String[] args) {\rList\u0026lt;Student\u0026gt; students = new ArrayList\u0026lt;\u0026gt;(3);\rstudents.add(new Student(\u0026quot;路飞\u0026quot;, 22, 175));\rstudents.add(new Student(\u0026quot;红发\u0026quot;, 40, 180));\rstudents.add(new Student(\u0026quot;白胡子\u0026quot;, 50, 185));\rlong count = students.stream().filter(s1 -\u0026gt; s1.getAge() \u0026lt; 45).count();\rSystem.out.println(\u0026quot;年龄小于45岁的人数是：\u0026quot; + count);\r}\r}\r//输出结果\r//年龄小于45岁的人数是：2\r2.7 distinct 本质是通过equals方法和hashcode 判断是否是同个对象, 中间方法\n2.8 sorted 两个重载方法\n元素对象实现comparable接口, 或者调用时传入\n2.9 limit 设置最大长度, 可以大于集合长度, 多出会被移除\n3.0 skip 跳过流中的前n个元素, 返回剩下的元素\n2.8 reduce 对流中的数据按照你指定的计算方式得到一个结果, 可以传入一个初始化值\n{@code\rU result = identity;\rfor (T element : this stream)\rresult = accumulator.apply(result, element)\rreturn result;\r}\r==实现从一组值中生成一个值==, 上述中的max, min, count 这些方法都是reduce, 及早求值\npublic class TestCase {\rpublic static void main(String[] args) {\rInteger reduce = Stream.of(1, 2, 3, 4).reduce(0, (acc, x) -\u0026gt; acc+ x);\rSystem.out.println(reduce);\r}\r}\r//输出结果\r//10\rreduce接收了一个==初始值为0的累加器==，依次取出值与累加器相加，最后累加器的值就是最终的结果\n3. 高级集合类及收集器 3.1. 转换成值 收集器: 一种通用的, 从流生成复杂值的结构, 收集器可以从 java.util.stream.Collectors 类中静态导入的\npublic class CollectorsTest {\rpublic static void main(String[] args) {\rList\u0026lt;Student\u0026gt; students1 = new ArrayList\u0026lt;\u0026gt;(3);\rstudents1.add(new Student(\u0026quot;路飞\u0026quot;, 23, 175));\rstudents1.add(new Student(\u0026quot;红发\u0026quot;, 40, 180));\rstudents1.add(new Student(\u0026quot;白胡子\u0026quot;, 50, 185));\rOutstandingClass ostClass1 = new OutstandingClass(\u0026quot;一班\u0026quot;, students1);\r//复制students1，并移除一个学生\rList\u0026lt;Student\u0026gt; students2 = new ArrayList\u0026lt;\u0026gt;(students1);\rstudents2.remove(1);\rOutstandingClass ostClass2 = new OutstandingClass(\u0026quot;二班\u0026quot;, students2);\r//将ostClass1、ostClass2转换为Stream\rStream\u0026lt;OutstandingClass\u0026gt; classStream = Stream.of(ostClass1, ostClass2);\rOutstandingClass outstandingClass = biggestGroup(classStream);\rSystem.out.println(\u0026quot;人数最多的班级是：\u0026quot; + outstandingClass.getName());\rSystem.out.println(\u0026quot;一班平均年龄是：\u0026quot; + averageNumberOfStudent(students1));\r}\r/**\r* 获取人数最多的班级\r*/\rprivate static OutstandingClass biggestGroup(Stream\u0026lt;OutstandingClass\u0026gt; outstandingClasses) {\rreturn outstandingClasses.collect(\rmaxBy(comparing(ostClass -\u0026gt; ostClass.getStudents().size())))\r.orElseGet(OutstandingClass::new);\r}\r/**\r* 计算平均年龄\r*/\rprivate static double averageNumberOfStudent(List\u0026lt;Student\u0026gt; students) {\rreturn students.stream().collect(averagingInt(Student::getAge));\r}\r}\r//输出结果\r//人数最多的班级是：一班\r//一班平均年龄是：37.666666666666664\r3.2. 转换成块 partitioningBy 将流分解成两个集合, Collectors.partitioningBy 接受一个==Predicate 函数式接口==\n分成两块 一个 true 一块, false 一块\npublic class PartitioningByTest {\rpublic static void main(String[] args) {\r//省略List\u0026lt;student\u0026gt; students的初始化\rMap\u0026lt;Boolean, List\u0026lt;Student\u0026gt;\u0026gt; listMap = students.stream().collect(\rCollectors.partitioningBy(student -\u0026gt; student.getSpecialities().\rcontains(SpecialityEnum.SING)));\r}\r}\r3.2 数据分组 groupingBy ==Collectors.groupingBy== 接受一个Function 做转换; 与sql 中的 group by 操作是一样的\npublic class GroupingByTest {\rpublic static void main(String[] args) {\r//省略List\u0026lt;student\u0026gt; students的初始化\rMap\u0026lt;SpecialityEnum, List\u0026lt;Student\u0026gt;\u0026gt; listMap = students.stream().collect(\rCollectors.groupingBy(student -\u0026gt; student.getSpecialities().get(0)));\r}\r}\r// 按照字符串长度进行分组 符合条件的元素将组成一个 List 映射到以条件长度为key 的 Map\u0026lt;Integer, List\u0026lt;String\u0026gt;\u0026gt; 中\rservers.stream().collect(Collectors.groupingBy(String::length))\r// 上面的写法等同于\rSupplier\u0026lt;Map\u0026lt;Integer,List\u0026lt;String\u0026gt;\u0026gt;\u0026gt; mapSupplier = HashMap::new;\rMap\u0026lt;Integer,List\u0026lt;String\u0026gt;\u0026gt; collect = servers.stream().collect(Collectors.groupingBy(String::length, mapSupplier, Collectors.toSet()));\r3.4 字符串拼接 joining 通常只能创建一个StringBuilder，循环拼接。使用Stream，使用Collectors.joining()简单容易\npublic class JoiningTest {\rpublic static void main(String[] args) {\rList\u0026lt;Student\u0026gt; students = new ArrayList\u0026lt;\u0026gt;(3);\rstudents.add(new Student(\u0026quot;路飞\u0026quot;, 22, 175));\rstudents.add(new Student(\u0026quot;红发\u0026quot;, 40, 180));\rstudents.add(new Student(\u0026quot;白胡子\u0026quot;, 50, 185));\rString names = students.stream()\r.map(Student::getName).collect(Collectors.joining(\u0026quot;,\u0026quot;,\u0026quot;[\u0026quot;,\u0026quot;]\u0026quot;));\rSystem.out.println(names);\r}\r}\r//输出结果\r//[路飞,红发,白胡子]\r3.5 collectingAndThen 该方法先执行了一个归纳操作，然后再对归纳的结果进行 Function 函数处理输出一个新的结果\nservers.stream.collect(Collectors.collectingAndThen(Collectors.joining(\u0026quot;,\u0026quot;), String::toUpperCase));\r","date":"2020-04-03T09:21:43+08:00","permalink":"https://mikeLing-qx.github.io/p/lamda/","title":"Lamda"},{"content":"1. Spring boot 问题0: 什么是springboot 的自动配置\n它是指基于你引入的依赖的jar包, 对spring boot 应用进行自动配置 它为springboot ==\u0026ldquo;开箱即用\u0026rdquo; 提供了基础支撑== 问题1: 为什么导入dependency时不需要指定版本?\n父依赖启动器中进行了统一的版本管理\n问题2: 项目运行依赖的JAR包是哪里来的\n基于maven的依赖传递特性, 导入starter 之后, 依赖传递了很多jar包\n问题3: springboot 如何完成的自动配置? 自动过程中都会将那些组件添加到容器中?\nSpringApplication.run() 会完成bean对象的创建\n自动配置的原理: @SpringBootApplication 注解\n启动流程简化\n2. spring boot 加载配置类的方式 ==@Import 注解, 它提供了一种显示地从其它地方加载配置类的方式, 避免了效率较差的组件扫描 (Component Scan)==\n实例2: 通过 @Import 导入接口ImportSelector 实现类\n核心逻辑是\n从classpath 中读取到所有Jar 包中的配置文件 META-INF/spring.factories. 然后根据指定的 key 从配置文件中解析出对应的value 值\n3. SPI 和 spring factories的对比 4. spring conditional 常用 5. EnableConfigurationProperties 如果一个配置类只配置@ConfigurationProperties注解，而没有使用@Component，那么在IOC容器中是获取不到properties 配置文件转化的bean。说白了 @EnableConfigurationProperties 相当于把使用 @ConfigurationProperties 的类进行了一次注入。\n2. EnableAutoConfig 注解\nSpring 中Enable 开头的注解\n作用流程\nSpringApplication.run 在启动过程中对核心启动类进行解析, 并且解析类上的注解\nSpringBootApplication 这个组合注解\n@SpringBootConfiguration // 标明该类为配置类\r@EnableAutoConfiguration // 启动自动配置功能\r@ComponentScan EnableAutoConfiguration 用@Import向容器中导入一个AutoConfigurationImportSelector 组件类\n这个组件的process 方法, 会在springApplication.run 启动过程中会被调用, 它会加载 所有jar包下 的META-inf/ spring.factories 文件, 在加载文件之后\n找到EnableAutoConfiguration 根据这个接口的类全路径为key\n找到所有可以进行自动配置工厂类的类全路径\n​\t再根据@Conditional这个注解进行过滤, 最终把满足自动配置的工厂类的全路径进行了返回. 存入到了一个list 集合中\nSPI 就是一种服务发现\n从run方法 构建springApplication 对象, 调用run 方法\n进行一些赋值操作, 监听器\n==SPI配置文件中先加载的为准, 后加载的不会覆盖后加载的==\n流程总结\n3. 自定义starter 结构\nmaven 中的可选依赖: 主要作用是阻断了依赖传递\n","date":"2020-03-18T19:21:43+08:00","permalink":"https://mikeLing-qx.github.io/p/springboot/","title":"SpringBoot"},{"content":"1. 概述 Stream的操作符大体上分为两种：中间操作符和终止操作符\n1.1 中间操作符 对于数据流来说，中间操作符在执行制定处理程序后，数据流依然可以传递给下一级的操作符。\n中间操作符包含8种(排除了parallel,sequential,这两个操作并不涉及到对数据流的加工操作)：\nmap(mapToInt,mapToLong,mapToDouble) 转换操作符，把比如A-\u0026gt;B，这里默认提供了转int，long，double的操作符。 flatmap(flatmapToInt,flatmapToLong,flatmapToDouble) 拍平操作比如把 int[]{2,3,4} 拍平 变成 2，3，4 也就是从原来的一个数据变成了3个数据，这里默认提供了拍平成int,long,double的操作符。 limit 限流操作，比如数据流中有10个 我只要出前3个就可以使用。 distint 去重操作，对重复元素去重，底层使用了equals方法。 filter 过滤操作，把不想要的数据过滤。 peek 挑出操作，如果想对数据进行某些操作，如：读取、编辑修改等。 skip 跳过操作，跳过某些元素。 sorted(unordered) 排序操作，对元素排序，前提是实现Comparable接口，当然也可以自定义比较器。 1.2 终止操作符 数据经过中间加工操作，就轮到终止操作符上场了；终止操作符就是用来对数据进行收集或者消费的，数据到了终止操作这里就不会向下流动了，终止操作符只能使用一次。\ncollect 收集操作，将所有数据收集起来，这个操作非常重要，官方的提供的Collectors 提供了非常多收集器，可以说Stream 的核心在于Collectors。 count 统计操作，统计最终的数据个数。 findFirst、findAny 查找操作，查找第一个、查找任何一个 返回的类型为Optional。 noneMatch、allMatch、anyMatch 匹配操作，数据流中是否存在符合条件的元素 返回值为bool 值。 min、max 最值操作，需要自定义比较器，返回数据流中最大最小的值。 reduce 规约操作，将整个数据流的值规约为一个值，count、min、max底层就是使用reduce。 forEach、forEachOrdered 遍历操作，这里就是对最终的数据进行消费了。 toArray 数组操作，将数据流的元素转换成数组。 这里只介绍了Stream，并没有涉及到IntStream、LongStream、DoubleStream，这三个流实现了一些特有的操作符，我将在后续文章中介绍到。\n说了这么多，只介绍这些操作符还远远不够；俗话说，实践出真知。那么，Let‘s go。\n2. 代码演练 2.1 map Stream 的一系列操作必须要使用终止操作，否者整个数据流是不会流动起来的，即处理操作不会执行。\n==map==，可以看到 map 操作符要求输入一个Function的函数是接口实例，功能是将T类型转换成R类型的 map操作将原来的单词 转换成了每个字符串的长度，利用了String自身的length()方法，该方法返回类型为int。这里我直接使用了lambda表达式，关于lambda表达式 还请读者们自行了解吧\npublic class Main { public static void main(String[] args) { Stream.of(\u0026quot;apple\u0026quot;,\u0026quot;banana\u0026quot;,\u0026quot;orange\u0026quot;,\u0026quot;waltermaleon\u0026quot;,\u0026quot;grape\u0026quot;) .map(e-\u0026gt;e.length()) //转成单词的长度 int .forEach(e-\u0026gt;System.out.println(e)); //输出 } } 当然也可以这样，这里使用了成员函数引用，为了便于读者们理解，后续的例子中将使用lambda表达式而非函数引用。\npublic class Main { public static void main(String[] args) { Stream.of(\u0026quot;apple\u0026quot;,\u0026quot;banana\u0026quot;,\u0026quot;orange\u0026quot;,\u0026quot;waltermaleon\u0026quot;,\u0026quot;grape\u0026quot;) .map(String::length) //转成单词的长度 int .forEach(System.out::println); } } 结果\nmapToInt 将数据流中得元素转成Int，这限定了转换的类型Int，最终产生的流为IntStream，及结果只能转化成int。\nmapTolong\npublic class Main { public static void main(String[] args) { Stream.of(\u0026quot;apple\u0026quot;, \u0026quot;banana\u0026quot;, \u0026quot;orange\u0026quot;, \u0026quot;waltermaleon\u0026quot;, \u0026quot;grape\u0026quot;) .mapToLong(e -\u0026gt; e.length()) //转成long ,本质上是int 但是存在类型自动转换 .forEach(e -\u0026gt; System.out.println(e)); } } mapToDouble\npublic class Main { public static void main(String[] args) { Stream.of(\u0026quot;apple\u0026quot;, \u0026quot;banana\u0026quot;, \u0026quot;orange\u0026quot;, \u0026quot;waltermaleon\u0026quot;, \u0026quot;grape\u0026quot;) .mapToDouble(e -\u0026gt; e.length()) //转成Double ，自动类型转换成Double .forEach(e -\u0026gt; System.out.println(e)); } } ==flatmap== 作用就是将元素拍平拍扁 ，将拍扁的元素重新组成Stream，并将这些Stream 串行合并成一条Stream ; flatmapToInt、flatmapToLong、flatmapToDouble 跟flatMap 都类似的，只是类型被限定了，这里就不在举例子了。 public class Main { public static void main(String[] args) { Stream.of(\u0026quot;a-b-c-d\u0026quot;,\u0026quot;e-f-i-g-h\u0026quot;) .flatMap(e-\u0026gt;Stream.of(e.split(\u0026quot;-\u0026quot;))) .forEach(e-\u0026gt;System.out.println(e)); } } ==limit==限制元素的个数，只需传入 long 类型 表示限制的最大数 public class Main { public static void main(String[] args) { Stream.of(1,2,3,4,5,6) .limit(3) //限制三个 .forEach(e-\u0026gt;System.out.println(e)); //将输出 前三个 1，2，3 } } ==distinct== 去重 public class Main { public static void main(String[] args) { Stream.of(1,2,3,1,2,5,6,7,8,0,0,1,2,3,1) .distinct() //去重 .forEach(e-\u0026gt;System.out.println(e)); } } ==filter== 对某些元素进行过滤，不符合筛选条件的将无法进入流的下游 public class Main { public static void main(String[] args) { Stream.of(1,2,3,1,2,5,6,7,8,0,0,1,2,3,1) .filter(e-\u0026gt;e\u0026gt;=5) //过滤小于5的 .forEach(e-\u0026gt;System.out.println(e)); } } ==peek==挑选 ，将元素挑选出来，可以理解为提前消费 public class Main { public static void main(String[] args) { User w = new User(\u0026quot;w\u0026quot;,10); User x = new User(\u0026quot;x\u0026quot;,11); User y = new User(\u0026quot;y\u0026quot;,12); Stream.of(w,x,y) .peek(e-\u0026gt;{e.setName(e.getAge()+e.getName());}) //重新设置名字 变成 年龄+名字 .forEach(e-\u0026gt;System.out.println(e.toString())); } static class User { private String name; private int age; public User(String name, int age) { this.name = name; this.age = age; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } @Override public String toString() { return \u0026quot;User{\u0026quot; + \u0026quot;name='\u0026quot; + name + '\\'' + \u0026quot;, age=\u0026quot; + age + '}'; } } } ==skip== 跳过 元素 public class Main { public static void main(String[] args) { Stream.of(1,2,3,4,5,6,7,8,9) .skip(4) //跳过前四个 .forEach(e-\u0026gt;System.out.println(e)); //输出的结果应该只有5，6，7，8，9 } } ==sorted==排序 底层依赖Comparable 实现，也可以提供自定义比较器 ublic class Main { public static void main(String[] args) { User x = new User(\u0026quot;x\u0026quot;,11); User y = new User(\u0026quot;y\u0026quot;,12); User w = new User(\u0026quot;w\u0026quot;,10); Stream.of(w,x,y) .sorted((e1,e2)-\u0026gt;e1.age\u0026gt;e2.age?1:e1.age==e2.age?0:-1) .forEach(e-\u0026gt;System.out.println(e.toString())); } static class User { private String name; private int age; public User(String name, int age) { this.name = name; this.age = age; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } @Override public String toString() { return \u0026quot;User{\u0026quot; + \u0026quot;name='\u0026quot; + name + '\\'' + \u0026quot;, age=\u0026quot; + age + '}'; } } } ==collect== 收集，使用系统提供的收集器可以将最终的数据流收集到List，Set，Map等容器中。 不是说终止操作符只能使用一次吗，为什么这里调用了forEach 呢？forEach不仅仅是是Stream 中得操作符还是各种集合中得一个语法糖\npublic class Main { public static void main(String[] args) { Stream.of(\u0026quot;apple\u0026quot;, \u0026quot;banana\u0026quot;, \u0026quot;orange\u0026quot;, \u0026quot;waltermaleon\u0026quot;, \u0026quot;grape\u0026quot;) .collect(Collectors.toSet()) //set 容器 .forEach(e -\u0026gt; System.out.println(e)); } } ==count== 统计数据流中的元素个数，返回的是long 类型\n==noneMatch== 数据流中得没有一个元素与条件匹配的\nallMatch和anyMatch 一个是全匹配，一个是任意匹配 和noneMatch 类似，这里就不在举例了。\nmin 最小的一个，传入比较器，也可能没有(如果数据流为空)\npublic class Main { public static void main(String[] args) { Optional\u0026lt;Integer\u0026gt; integerOptional = Stream.of(0,9,8,4,5,6,-1) .min((e1,e2)-\u0026gt;e1.compareTo(e2)); integerOptional.ifPresent(e-\u0026gt;System.out.println(e)); } max 元素中最大的，需要传入比较器，也可能没有（流为Empty时） public class Main { public static void main(String[] args) { Optional\u0026lt;Integer\u0026gt; integerOptional = Stream.of(0,9,8,4,5,6,-1) .max((e1,e2)-\u0026gt;e1.compareTo(e2)); integerOptional.ifPresent(e-\u0026gt;System.out.println(e)); } } reduce 是一个规约操作，所有的元素归约成一个，比如对所有元素求和，乘啊等 public class Main { public static void main(String[] args) { int sum = Stream.of(0,9,8,4,5,6,-1) .reduce(0,(e1,e2)-\u0026gt;e1+e2); System.out.println(sum); } } // filter 去重\nList\u0026lt;BizList\u0026gt; distinctBizLists = bizList.stream().filter(distinctByKey(BizList::getItem)).collect(Collectors.toList()); public static \u0026lt;T\u0026gt; Predicate\u0026lt;T\u0026gt; distinctByKey(Function\u0026lt;? super T, ?\u0026gt; keyExtractor) { Set\u0026lt;Object\u0026gt; seen = ConcurrentHashMap.newKeySet(); return t -\u0026gt; seen.add(keyExtractor.apply(t)); } ","date":"2020-03-03T12:21:43+08:00","permalink":"https://mikeLing-qx.github.io/p/stream/","title":"Stream"},{"content":"69道Spring面试题和答案\nSpring 概述 1. 什么是spring? Spring 是个java企业级应用的开源开发框架。Spring主要用来开发Java应用，但是有些扩展是针对构建J2EE平台的web应用。Spring 框架目标是简化Java企业级应用开发，并通过POJO为基础的编程模型促进良好的编程习惯。\n2. 使用Spring框架的好处是什么？ **轻量：**Spring 是轻量的，基本的版本大约2MB。\n**控制反转：**Spring通过控制反转实现了松散耦合，对象们给出它们的依赖，而不是创建或查找依赖的对象们。\n**面向切面的编程(AOP)：**Spring支持面向切面的编程，并且把应用业务逻辑和系统服务分开。\n**容器：**Spring 包含并管理应用中对象的生命周期和配置。\nMVC框架：Spring的WEB框架是个精心设计的框架，是Web框架的一个很好的替代品。\n**事务管理：**Spring 提供一个持续的事务管理接口，可以扩展到上至本地事务下至全局事务（JTA）。\n**异常处理：**Spring 提供方便的API把具体技术相关的异常（比如由JDBC，Hibernate or JDO抛出的）转化为一致的unchecked 异常。\n3. Spring由哪些模块组成? 以下是Spring 框架的基本模块：\nCore module\nBean module\nContext module\nExpression Language module\nJDBC module\nORM module\nOXM module\nJava Messaging Service(JMS) module\nTransaction module\nWeb module\nWeb-Servlet module\nWeb-Struts module\nWeb-Portlet module\n4. 核心容器（应用上下文) 模块。 这是基本的Spring模块，提供spring 框架的基础功能，BeanFactory 是 任何以spring为基础的应用的核心。Spring 框架建立在此模块之上，它使Spring成为一个容器。\n5. BeanFactory – BeanFactory 实现举例。 Bean 工厂是工厂模式的一个实现，提供了控制反转功能，用来把应用的配置和依赖从正真的应用代码中分离。\n最常用的BeanFactory 实现是XmlBeanFactory 类。\n6. XMLBeanFactory 最常用的就是org.springframework.beans.factory.xml.XmlBeanFactory ，它根据XML文件中的定义加载beans。该容器从XML 文件读取配置元数据并用它去创建一个完全配置的系统或应用。\n7. 解释AOP模块 AOP模块用于发给我们的Spring应用做面向切面的开发， 很多支持由AOP联盟提供，这样就确保了Spring和其他AOP框架的共通性。这个模块将元数据编程引入Spring。\n8. 解释JDBC抽象和DAO模块。 通过使用JDBC抽象和DAO模块，保证数据库代码的简洁，并能避免数据库资源错误关闭导致的问题，它在各种不同的数据库的错误信息之上，提供了一个统一的异常访问层。它还利用Spring的AOP 模块给Spring应用中的对象提供事务管理服务。\n9. 解释对象/关系映射集成模块。 Spring 通过提供ORM模块，支持我们在直接JDBC之上使用一个对象/关系映射映射(ORM)工具，Spring 支持集成主流的ORM框架，如Hiberate,JDO和 iBATIS SQL Maps。Spring的事务管理同样支持以上所有ORM框架及JDBC。\n10. 解释WEB 模块。 Spring的WEB模块是构建在application context 模块基础之上，提供一个适合web应用的上下文。这个模块也包括支持多种面向web的任务，如透明地处理多个文件上传请求和程序级请求参数的绑定到你的业务对象。它也有对Jakarta Struts的支持。\n12. Spring配置文件 Spring配置文件是个XML 文件，这个文件包含了类信息，描述了如何配置它们，以及如何相互调用。\n13. 什么是Spring IOC 容器？ Spring IOC 负责创建对象，管理对象（通过依赖注入（DI），装配对象，配置对象，并且管理这些对象的整个生命周期。\n14. IOC的优点是什么？ IOC 或 依赖注入把应用的代码量降到最低。它使应用容易测试，单元测试不再需要单例和JNDI查找机制。最小的代价和最小的侵入性使松散耦合得以实现。IOC容器支持加载服务时的饿汉式初始化和懒加载。\n15. ApplicationContext通常的实现是什么? **FileSystemXmlApplicationContext ：**此容器从一个XML文件中加载beans的定义，XML Bean 配置文件的全路径名必须提供给它的构造函数。\n**ClassPathXmlApplicationContext：**此容器也从一个XML文件中加载beans的定义，这里，你需要正确设置classpath因为这个容器将在classpath里找bean配置。\n**WebXmlApplicationContext：**此容器加载一个XML文件，此文件定义了一个WEB应用的所有bean。\n16. Bean 工厂和 Application contexts 有什么区别？ Application contexts提供一种方法处理文本消息，一个通常的做法是加载文件资源（比如镜像），它们可以向注册为监听器的bean发布事件。另外，在容器或容器内的对象上执行的那些不得不由bean工厂以程序化方式处理的操作，可以在Application contexts中以声明的方式处理。Application contexts实现了MessageSource接口，该接口的实现以可插拔的方式提供获取本地化消息的方法。\n17. 一个Spring的应用看起来象什么？ 一个定义了一些功能的接口。\n这实现包括属性，它的Setter ， getter 方法和函数等。\nSpring AOP。\nSpring 的XML 配置文件。\n使用以上功能的客户端程序。\n依赖注入 18. 什么是Spring的依赖注入？ 依赖注入，是IOC的一个方面，是个通常的概念，它有多种解释。这概念是说你不用创建对象，而只需要描述它如何被创建。你不在代码里直接组装你的组件和服务，但是要在配置文件里描述哪些组件需要哪些服务，之后一个容器（IOC容器）负责把他们组装起来。\n19. 有哪些不同类型的IOC（依赖注入）方式？ **构造器依赖注入：**构造器依赖注入通过容器触发一个类的构造器来实现的，该类有一系列参数，每个参数代表一个对其他类的依赖。\n**Setter方法注入：**Setter方法注入是容器通过调用无参构造器或无参static工厂 方法实例化bean之后，调用该bean的setter方法，即实现了基于setter的依赖注入。\n20. 哪种依赖注入方式你建议使用，构造器注入，还是 Setter方法注入？ 你两种依赖方式都可以使用，构造器注入和Setter方法注入。最好的解决方案是用构造器参数实现强制依赖，setter方法实现可选依赖。\nSpring Beans 21.什么是Spring beans? Spring beans 是那些形成Spring应用的主干的java对象。它们被Spring IOC容器初始化，装配，和管理。这些beans通过容器中配置的元数据创建。比如，以XML文件中 的形式定义。\nSpring 框架定义的beans都是单件beans。在bean tag中有个属性”singleton”，如果它被赋为TRUE，bean 就是单件，否则就是一个 prototype bean。默认是TRUE，所以所有在Spring框架中的beans 缺省都是单件。\n22. 一个 Spring Bean 定义 包含什么？ 一个Spring Bean 的定义包含容器必知的所有配置元数据，包括如何创建一个bean，它的生命周期详情及它的依赖。\n23. 如何给Spring 容器提供配置元数据? 这里有三种重要的方法给Spring 容器提供配置元数据。\nXML配置文件。\n基于注解的配置。\n基于java的配置。\n24. 你怎样定义类的作用域? 当定义一个 在Spring里，我们还能给这个bean声明一个作用域。它可以通过bean 定义中的scope属性来定义。如，当Spring要在需要的时候每次生产一个新的bean实例，bean的scope属性被指定为prototype。另一方面，一个bean每次使用的时候必须返回同一个实例，这个bean的scope 属性 必须设为 singleton。\n25. 解释Spring支持的几种bean的作用域。 Spring框架支持以下五种bean的作用域：\nsingleton : bean在每个Spring ioc 容器中只有一个实例。\nprototype：一个bean的定义可以有多个实例。\nrequest：每次http请求都会创建一个bean，该作用域仅在基于web的Spring ApplicationContext情形下有效。\nsession：在一个HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。\nglobal-session：在一个全局的HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。\n缺省的Spring bean 的作用域是Singleton.\n26. Spring框架中的单例bean是线程安全的吗? 不，Spring框架中的单例bean不是线程安全的。\n27. 解释Spring框架中bean的生命周期。 Spring容器 从XML 文件中读取bean的定义，并实例化bean。\nSpring根据bean的定义填充所有的属性。\n如果bean实现了BeanNameAware 接口，Spring 传递bean 的ID 到 setBeanName方法。\n如果Bean 实现了 BeanFactoryAware 接口， Spring传递beanfactory 给setBeanFactory 方法。\n如果有任何与bean相关联的BeanPostProcessors，Spring会在postProcesserBeforeInitialization()方法内调用它们。\n如果bean实现IntializingBean了，调用它的afterPropertySet方法，如果bean声明了初始化方法，调用此初始化方法。\n如果有BeanPostProcessors 和bean 关联，这些bean的postProcessAfterInitialization() 方法将被调用。\n如果bean实现了 DisposableBean，它将调用destroy()方法。\n28. 哪些是重要的bean生命周期方法？ 你能重载它们吗？ 有两个重要的bean 生命周期方法，第一个是setup ， 它是在容器加载bean的时候被调用。第二个方法是 teardown 它是在容器卸载类的时候被调用。\nThe bean 标签有两个重要的属性（init-method和destroy-method）。用它们你可以自己定制初始化和注销方法。它们也有相应的注解（@PostConstruct和@PreDestroy）。\n29. 什么是Spring的内部bean？ 当一个bean仅被用作另一个bean的属性时，它能被声明为一个内部bean，为了定义inner bean，在Spring 的 基于XML的 配置元数据中，可以在 或 元素内使用 元素，内部bean通常是匿名的，它们的Scope一般是prototype。\n30. 在 Spring中如何注入一个java集合？ Spring提供以下几种集合的配置元素：\n类型用于注入一列值，允许有相同的值。\n类型用于注入一组值，不允许有相同的值。\n类型用于注入一组键值对，键和值都可以为任意类型。\n类型用于注入一组键值对，键和值都只能为String类型。\n31. 什么是bean装配? 装配，或bean 装配是指在Spring 容器中把bean组装到一起，前提是容器需要知道bean的依赖关系，如何通过依赖注入来把它们装配到一起。\n32. 什么是bean的自动装配？ Spring 容器能够自动装配相互合作的bean，这意味着容器不需要和配置，能通过Bean工厂自动处理bean之间的协作。\n33. 解释不同方式的自动装配 。 有五种自动装配的方式，可以用来指导Spring容器用自动装配方式来进行依赖注入。\nno：默认的方式是不进行自动装配，通过显式设置ref 属性来进行装配。\n**byName：**通过参数名 自动装配，Spring容器在配置文件中发现bean的autowire属性被设置成byname，之后容器试图匹配、装配和该bean的属性具有相同名字的bean。\n**byType:：**通过参数类型自动装配，Spring容器在配置文件中发现bean的autowire属性被设置成byType，之后容器试图匹配、装配和该bean的属性具有相同类型的bean。如果有多个bean符合条件，则抛出错误。\nconstructor：这个方式类似于byType， 但是要提供给构造器参数，如果没有确定的带参数的构造器参数类型，将会抛出异常。\n**autodetect：**首先尝试使用constructor来自动装配，如果无法工作，则使用byType方式。\n34.自动装配有哪些局限性 ? 自动装配的局限性是：\n重写： 你仍需用 和 配置来定义依赖，意味着总要重写自动装配。\n基本数据类型：你不能自动装配简单的属性，如基本数据类型，String字符串，和类。\n**模糊特性：**自动装配不如显式装配精确，如果有可能，建议使用显式装配。\n35. 你可以在Spring中注入一个null 和一个空字符串吗？ 可以。\nSpring注解 36. 什么是基于Java的Spring注解配置? 给一些注解的例子. 基于Java的配置，允许你在少量的Java注解的帮助下，进行你的大部分Spring配置而非通过XML文件。\n以@Configuration 注解为例，它用来标记类可以当做一个bean的定义，被Spring IOC容器使用。另一个例子是@Bean注解，它表示此方法将要返回一个对象，作为一个bean注册进Spring应用上下文。\n37. 什么是基于注解的容器配置? 相对于XML文件，注解型的配置依赖于通过字节码元数据装配组件，而非尖括号的声明。\n开发者通过在相应的类，方法或属性上使用注解的方式，直接组件类中进行配置，而不是使用xml表述bean的装配关系。\n38. 怎样开启注解装配？ 注解装配在默认情况下是不开启的，为了使用注解装配，我们必须在Spring配置文件中配置 context:annotation-config/元素。\n39. @Required 注解 这个注解表明bean的属性必须在配置的时候设置，通过一个bean定义的显式的属性值或通过自动装配，若@Required注解的bean属性未被设置，容器将抛出BeanInitializationException。\n40. @Autowired 注解 @Autowired 注解提供了更细粒度的控制，包括在何处以及如何完成自动装配。它的用法和@Required一样，修饰setter方法、构造器、属性或者具有任意名称和/或多个参数的PN方法。\n41. @Qualifier 注解 当有多个相同类型的bean却只有一个需要自动装配时，将@Qualifier 注解和@Autowire 注解结合使用以消除这种混淆，指定需要装配的确切的bean。\nSpring数据访问 42.在Spring框架中如何更有效地使用JDBC? 使用SpringJDBC 框架，资源管理和错误处理的代价都会被减轻。所以开发者只需写statements 和 queries从数据存取数据，JDBC也可以在Spring框架提供的模板类的帮助下更有效地被使用，这个模板叫JdbcTemplate （例子见这里here）\n43. JdbcTemplate JdbcTemplate 类提供了很多便利的方法解决诸如把数据库数据转变成基本数据类型或对象，执行写好的或可调用的数据库操作语句，提供自定义的数据错误处理。\n44. Spring对DAO的支持 Spring对数据访问对象（DAO）的支持旨在简化它和数据访问技术如JDBC，Hibernate or JDO 结合使用。这使我们可以方便切换持久层。编码时也不用担心会捕获每种技术特有的异常。\n45. 使用Spring通过什么方式访问Hibernate? 在Spring中有两种方式访问Hibernate：\n控制反转 Hibernate Template和 Callback。\n继承 HibernateDAOSupport提供一个AOP 拦截器。\n46. Spring支持的ORM Spring支持以下ORM：\nHibernate\niBatis\nJPA (Java Persistence API)\nTopLink\nJDO (Java Data Objects)\nOJB\n47.如何通过HibernateDaoSupport将Spring和Hibernate结合起来？ 用Spring的 SessionFactory 调用 LocalSessionFactory。集成过程分三步：\n配置the Hibernate SessionFactory。\n继承HibernateDaoSupport实现一个DAO。\n在AOP支持的事务中装配。\n48. Spring支持的事务管理类型 Spring支持两种类型的事务管理：\n编程式事务管理：这意味你通过编程的方式管理事务，给你带来极大的灵活性，但是难维护。\n**声明式事务管理：**这意味着你可以将业务代码和事务管理分离，你只需用注解和XML配置来管理事务。\n49. Spring框架的事务管理有哪些优点？ 它为不同的事务API 如 JTA，JDBC，Hibernate，JPA 和JDO，提供一个不变的编程模式。\n它为编程式事务管理提供了一套简单的API而不是一些复杂的事务API如\n它支持声明式事务管理。\n它和Spring各种数据访问抽象层很好得集成。\n50. 你更倾向用那种事务管理类型？ 大多数Spring框架的用户选择声明式事务管理，因为它对应用代码的影响最小，因此更符合一个无侵入的轻量级容器的思想。声明式事务管理要优于编程式事务管理，虽然比编程式事务管理（这种方式允许你通过代码控制事务）少了一点灵活性。\nSpring面向切面编程（AOP） 51. 解释AOP 面向切面的编程，或AOP， 是一种编程技术，允许程序模块化横向切割关注点，或横切典型的责任划分，如日志和事务管理。\n52. Aspect 切面 AOP核心就是切面，它将多个类的通用行为封装成可重用的模块，该模块含有一组API提供横切功能。比如，一个日志模块可以被称作日志的AOP切面。根据需求的不同，一个应用程序可以有若干切面。在Spring AOP中，切面通过带有@Aspect注解的类实现。\n52. 在Spring AOP 中，关注点和横切关注的区别是什么？ 关注点是应用中一个模块的行为，一个关注点可能会被定义成一个我们想实现的一个功能。 横切关注点是一个关注点，此关注点是整个应用都会使用的功能，并影响整个应用，比如日志，安全和数据传输，几乎应用的每个模块都需要的功能。因此这些都属于横切关注点。\n54. 连接点 连接点代表一个应用程序的某个位置，在这个位置我们可以插入一个AOP切面，它实际上是个应用程序执行Spring AOP的位置。\n55. 通知 通知是个在方法执行前或执行后要做的动作，实际上是程序执行时要通过SpringAOP框架触发的代码段。\nSpring切面可以应用五种类型的通知：\nbefore：前置通知，在一个方法执行前被调用。\nafter: 在方法执行之后调用的通知，无论方法执行是否成功。\nafter-returning: 仅当方法成功完成后执行的通知。\nafter-throwing: 在方法抛出异常退出时执行的通知。\naround: 在方法执行之前和之后调用的通知。\n56. 切点 切入点是一个或一组连接点，通知将在这些位置执行。可以通过表达式或匹配的方式指明切入点。\n57. 什么是引入? 引入允许我们在已存在的类中增加新的方法和属性。\n58. 什么是目标对象? 被一个或者多个切面所通知的对象。它通常是一个代理对象。也指被通知（advised）对象。\n59. 什么是代理? 代理是通知目标对象后创建的对象。从客户端的角度看，代理对象和目标对象是一样的。\n60. 有几种不同类型的自动代理？ BeanNameAutoProxyCreator\nDefaultAdvisorAutoProxyCreator\nMetadata autoproxying\n61. 什么是织入。什么是织入应用的不同点？ 织入是将切面和到其他应用类型或对象连接或创建一个被通知对象的过程。\n织入可以在编译时，加载时，或运行时完成。\n62. 解释基于XML Schema方式的切面实现。 在这种情况下，切面由常规类以及基于XML的配置实现。\n63. 解释基于注解的切面实现 在这种情况下(基于@AspectJ的实现)，涉及到的切面声明的风格与带有java5标注的普通java类一致。\nSpring 的MVC 64. 什么是Spring的MVC框架？ Spring 配备构建Web 应用的全功能MVC框架。Spring可以很便捷地和其他MVC框架集成，如Struts，Spring 的MVC框架用控制反转把业务对象和控制逻辑清晰地隔离。它也允许以声明的方式把请求参数和业务对象绑定。\n65. DispatcherServlet Spring的MVC框架是围绕DispatcherServlet来设计的，它用来处理所有的HTTP请求和响应。\n66. WebApplicationContext WebApplicationContext 继承了ApplicationContext 并增加了一些WEB应用必备的特有功能，它不同于一般的ApplicationContext ，因为它能处理主题，并找到被关联的servlet。\n67. 什么是Spring MVC框架的控制器？ 控制器提供一个访问应用程序的行为，此行为通常通过服务接口实现。控制器解析用户输入并将其转换为一个由视图呈现给用户的模型。Spring用一个非常抽象的方式实现了一个控制层，允许用户创建多种用途的控制器。\n68. @Controller 注解 该注解表明该类扮演控制器的角色，Spring不需要你继承任何其他控制器基类或引用Servlet API。\n69. @RequestMapping 注解 该注解是用来映射一个URL到一个类或一个特定的方处理法上。\n","date":"2020-01-26T15:09:30+08:00","permalink":"https://mikeLing-qx.github.io/p/spring_%E9%9D%A2%E8%AF%95%E9%A2%98/","title":"Spring_面试题"}]